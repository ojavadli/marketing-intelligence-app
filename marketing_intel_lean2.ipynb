{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marketing Intelligence Agent - LEAN VERSION\n",
    "\n",
    "**Simple working system - tested and working!**\n",
    "\n",
    "Just 4 cells to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u2705 All packages installed:\n",
      "   - OpenAI, LangChain, Tavily\n",
      "   - LangGraph, TruLens\n",
      "   - Requests, Pydantic (for Reddit MCP)\n",
      "   - Pandas, OpenPyXL (for Excel export)\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages\n",
    "%pip install openai langchain langchain-openai tavily-python langgraph trulens trulens-apps-langgraph trulens-providers-openai requests pydantic pandas openpyxl -q\n",
    "print(\"\u2705 All packages installed:\")\n",
    "print(\"   - OpenAI, LangChain, Tavily\")\n",
    "print(\"   - LangGraph, TruLens\")\n",
    "print(\"   - Requests, Pydantic (for Reddit MCP)\")\n",
    "print(\"   - Pandas, OpenPyXL (for Excel export)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 All API keys set:\n",
      "   - OpenAI (GPT-4)\n",
      "   - Tavily (Web Search)\n",
      "   - Reddit MCP (No API key needed! \u2705)\n"
     ]
    }
   ],
   "source": [
    "import os",
    "",
    "# \ud83d\udc49 IMPORTANT: Set your API keys here before running!",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"  # Get from https://platform.openai.com/api-keys",
    "os.environ[\"TAVILY_API_KEY\"] = \"your_tavily_api_key_here\"  # Get from https://tavily.com",
    "",
    "# Verify keys are set",
    "print(\"\u2705 API keys configured!\")",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 APIs + Reddit MCP initialized\n",
      "\u2705 Reddit MCP: No API key needed, uses public endpoints\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "from typing import Dict, Any, List, Optional, Annotated, Literal\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from tavily import TavilyClient\n",
    "\n",
    "llm_json = ChatOpenAI(model='gpt-4o', temperature=0, model_kwargs={'response_format': {'type': 'json_object'}})\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0.1)\n",
    "tavily = TavilyClient(api_key=os.environ['TAVILY_API_KEY'])\n",
    "\n",
    "# ============================================================================\n",
    "# REDDIT MCP - Embedded directly in notebook for Reddit scraping\n",
    "# ============================================================================\n",
    "\n",
    "class RedditPost(BaseModel):\n",
    "    \"\"\"Single Reddit post record\"\"\"\n",
    "    title: str\n",
    "    subreddit: str\n",
    "    author: str\n",
    "    score: int\n",
    "    num_comments: int\n",
    "    created_utc: float\n",
    "    url: str\n",
    "    selftext: str = \"\"\n",
    "    permalink: str\n",
    "    id: str\n",
    "    is_self: bool\n",
    "    link_flair_text: Optional[str] = None\n",
    "\n",
    "class RedditPosts(BaseModel):\n",
    "    \"\"\"Collection of Reddit posts with metadata\"\"\"\n",
    "    request_url: str\n",
    "    items: list[RedditPost]\n",
    "    count: int\n",
    "    before: Optional[str] = None\n",
    "    after: Optional[str] = None\n",
    "\n",
    "class RedditTools:\n",
    "    \"\"\"Reddit API tools - uses public JSON endpoints, no API key required\"\"\"\n",
    "    \n",
    "    def _get_user_agent(self) -> str:\n",
    "        \"\"\"Rotate user agents to avoid blocking\"\"\"\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n",
    "        ]\n",
    "        return random.choice(user_agents)\n",
    "    \n",
    "    def search_posts(\n",
    "        self,\n",
    "        query: str,\n",
    "        subreddit: Optional[str] = None,\n",
    "        sort: Literal[\"relevance\", \"hot\", \"top\", \"new\", \"comments\"] = \"relevance\",\n",
    "        t: Literal[\"hour\", \"day\", \"week\", \"month\", \"year\", \"all\"] = \"week\",\n",
    "        limit: int = 25,\n",
    "        after: Optional[str] = None,\n",
    "        before: Optional[str] = None\n",
    "    ) -> RedditPosts:\n",
    "        \"\"\"\n",
    "        Search for posts across Reddit or within a specific subreddit.\n",
    "        Default time filter is 'week' (last 7 days).\n",
    "        \"\"\"\n",
    "        if subreddit:\n",
    "            url = f\"https://www.reddit.com/r/{subreddit}/search.json\"\n",
    "            params = {\"q\": query, \"restrict_sr\": \"true\"}\n",
    "        else:\n",
    "            url = \"https://www.reddit.com/search.json\"\n",
    "            params = {\"q\": query}\n",
    "        \n",
    "        params.update({\n",
    "            \"sort\": sort,\n",
    "            \"t\": t,\n",
    "            \"limit\": min(limit, 100),\n",
    "            \"raw_json\": 1\n",
    "        })\n",
    "        \n",
    "        if after:\n",
    "            params[\"after\"] = after\n",
    "        if before:\n",
    "            params[\"before\"] = before\n",
    "        \n",
    "        headers = {\"User-Agent\": self._get_user_agent()}\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        posts = [\n",
    "            child[\"data\"] for child in data[\"data\"][\"children\"]\n",
    "            if not child[\"data\"].get(\"stickied\", False)\n",
    "        ]\n",
    "        \n",
    "        post_items = []\n",
    "        for post in posts:\n",
    "            post_items.append(RedditPost(\n",
    "                title=post.get(\"title\", \"\"),\n",
    "                subreddit=post.get(\"subreddit\", \"\"),\n",
    "                author=post.get(\"author\", \"\"),\n",
    "                score=post.get(\"score\", 0),\n",
    "                num_comments=post.get(\"num_comments\", 0),\n",
    "                created_utc=post.get(\"created_utc\", 0),\n",
    "                url=post.get(\"url\", \"\"),\n",
    "                selftext=post.get(\"selftext\", \"\"),\n",
    "                permalink=f\"https://www.reddit.com{post.get('permalink', '')}\",\n",
    "                id=post.get(\"id\", \"\"),\n",
    "                is_self=post.get(\"is_self\", False),\n",
    "                link_flair_text=post.get(\"link_flair_text\")\n",
    "            ))\n",
    "        \n",
    "        return RedditPosts(\n",
    "            request_url=response.url,\n",
    "            items=post_items,\n",
    "            count=len(post_items),\n",
    "            before=data[\"data\"].get(\"before\"),\n",
    "            after=data[\"data\"].get(\"after\")\n",
    "        )\n",
    "\n",
    "# Initialize Reddit MCP\n",
    "reddit = RedditTools()\n",
    "\n",
    "print('\u2705 APIs + Reddit MCP initialized')\n",
    "print('\u2705 Reddit MCP: No API key needed, uses public endpoints')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE ALL 6 AGENTS (Modular Architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 State class defined\n"
     ]
    }
   ],
   "source": [
    "# Define State class\n",
    "class State(MessagesState):\n",
    "    business_name: Optional[str]\n",
    "    profile: Optional[Dict[str, Any]]\n",
    "    reddit_search_keywords: Optional[List[str]]\n",
    "    reddit_posts: Optional[List[Dict[str, Any]]]\n",
    "    ranked_data: Optional[Dict[str, Any]]\n",
    "    report: Optional[str]\n",
    "    validation: Optional[Dict[str, Any]]\n",
    "    final_report: Optional[str]\n",
    "    logs: Optional[List[str]]\n",
    "\n",
    "print(\"\u2705 State class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Agent 4: Report Generator defined\n"
     ]
    }
   ],
   "source": [
    "# AGENT 4: Report Generator\n",
    "def report_generator_agent(state: State) -> Command[Literal[\"validator\"]]:\n",
    "    \"\"\"Generate comprehensive marketing intelligence report.\"\"\"\n",
    "    ranked_data = state.get(\"ranked_data\", {})\n",
    "    logs = state.get(\"logs\", [])\n",
    "    \n",
    "    logs.append(f\"[Report Generator] Creating intelligence report\")\n",
    "    \n",
    "    report_prompt = f\"\"\"Generate marketing intelligence report for {state.get('business_name')}.\n",
    "Profile: {json.dumps(state.get('profile'))}\n",
    "Ranked Data: {json.dumps(ranked_data)}\n",
    "\n",
    "Include: Executive Summary, Pain Points, Trends, Recommended Actions.\n",
    "Format as markdown with Reddit citations.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=report_prompt)])\n",
    "    report = response.content\n",
    "    \n",
    "    logs.append(f\"[Report Generator] Report generated ({len(report)} chars)\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\"report\": report, \"logs\": logs},\n",
    "        goto=\"validator\"\n",
    "    )\n",
    "\n",
    "print(\"\u2705 Agent 4: Report Generator defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Agent 5: Validator defined\n"
     ]
    }
   ],
   "source": [
    "# AGENT 5: Validator\n",
    "def validator_agent(state: State) -> Command[Literal[\"summarizer\"]]:\n",
    "    \"\"\"Validate report groundedness.\"\"\"\n",
    "    report = state.get(\"report\", \"\")\n",
    "    reddit_posts = state.get(\"reddit_posts\", [])\n",
    "    logs = state.get(\"logs\", [])\n",
    "    \n",
    "    logs.append(f\"[Validator] Checking groundedness\")\n",
    "    \n",
    "    validation_prompt = f\"\"\"Validate this report against Reddit data.\n",
    "Report: {report}\n",
    "Reddit Posts: {json.dumps(reddit_posts, indent=2)}\n",
    "\n",
    "Return JSON: {{\"groundedness_score\": 0.95, \"validation_passed\": true, \"issues_found\": []}}\"\"\"\n",
    "    \n",
    "    response = llm_json.invoke([HumanMessage(content=validation_prompt)])\n",
    "    validation = json.loads(response.content)\n",
    "    \n",
    "    logs.append(f\"[Validator] Groundedness: {validation.get('groundedness_score', 0)}\")\n",
    "    logs.append(f\"[Validator] Status: {'PASSED' if validation.get('validation_passed') else 'FAILED'}\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\"validation\": validation, \"logs\": logs},\n",
    "        goto=\"summarizer\"\n",
    "    )\n",
    "\n",
    "print(\"\u2705 Agent 5: Validator defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Agent 6: Summarizer defined\n"
     ]
    }
   ],
   "source": [
    "# AGENT 6: Summarizer  \n",
    "def summarizer_agent(state: State) -> Command[Literal[END]]:\n",
    "    \"\"\"Polish and finalize report.\"\"\"\n",
    "    report = state.get(\"report\", \"\")\n",
    "    validation = state.get(\"validation\", {})\n",
    "    logs = state.get(\"logs\", [])\n",
    "    \n",
    "    logs.append(f\"[Summarizer] Finalizing report\")\n",
    "    \n",
    "    # Add metadata footer\n",
    "    final_report = report + f\"\"\"\n",
    "\n",
    "---\n",
    "**Report Metadata**  \n",
    "- Business: {state.get('business_name')}\n",
    "- Reddit Posts Analyzed: {len(state.get('reddit_posts', []))}\n",
    "- Groundedness Score: {validation.get('groundedness_score', 0)}\n",
    "- Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "    \n",
    "    logs.append(f\"[Summarizer] Complete! Total logs: {len(logs)}\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\"final_report\": final_report, \"logs\": logs},\n",
    "        goto=END\n",
    "    )\n",
    "\n",
    "print(\"\u2705 Agent 6: Summarizer defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP-BY-STEP EXECUTION (Separate Steps with Outputs)\n",
    "\n",
    "**\ud83d\udc49 Change business name below, then run each step to see output!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "\ud83c\udfaf MARKETING INTELLIGENCE ANALYSIS FOR: Duolingo\n",
      "================================================================================\\n\n"
     ]
    }
   ],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \ud83d\udc49 USER INPUT - Enter Your Business Name!\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "BUSINESS_NAME = \"Duolingo\"  # \ud83d\udc48 CHANGE THIS!\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"\ud83c\udfaf MARKETING INTELLIGENCE ANALYSIS FOR: {BUSINESS_NAME}\")\n",
    "print(f\"{'='*80}\\\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Profile Analyzer (Tavily Research)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "\ud83d\udce1 STEP 1: PROFILE ANALYZER - Research Business\n",
      "================================================================================\n",
      "Using: Tavily + OpenAI GPT-4\n",
      "Max time: 20 seconds\\n\n",
      "\ud83d\udd0d Researching with Tavily...\n",
      "\u2705 Found 5 sources\\n\n",
      "\ud83e\udd16 Extracting business profile with OpenAI GPT-4...\n",
      "\\n================================================================================\n",
      "\ud83d\udcca STEP 1 OUTPUT - EXTRACTED BUSINESS PROFILE:\n",
      "================================================================================\n",
      "\\n\ud83c\udfe2 Business: Duolingo, Inc.\n",
      "\\n\ud83d\udcc8 Industry: Language Learning\n",
      "\\n\ud83d\udcbc Business Model: Duolingo operates on a freemium business model. The platform offers free language learning services to users while monetizing through a paid subscription model. The paid version, known as Super Duolingo, removes ads and provides additional features such as unlimited hearts. Duolingo Max, another tier, includes AI-powered features like Video Call, Explain My Answer, and Roleplay. The company also benefits from data insights gained from a large user base, which helps improve engagement and efficacy.\n",
      "\\n\ud83c\udfaf Target Market: Duolingo's target market is broad, encompassing individual consumers across various age groups, educational backgrounds, and income levels. The platform is particularly popular among younger demographics and those interested in learning new languages for travel, education, or personal development.\n",
      "\\n\ud83d\udc65 Customer Demographics: Duolingo attracts a diverse user base, with a notable concentration in younger demographics. The platform's users include travelers, tourists, students, and educational institutions. The gamification and personalized learning experiences appeal to a wide range of user interests.\n",
      "\\n\ud83d\udecd\ufe0f Products/Services: Duolingo App, Super Duolingo, Duolingo Max\n",
      "\\n\u2694\ufe0f Competitors: Rosetta Stone, Babbel\n",
      "\\n\ud83d\udcca Market Position: Leader\n",
      "\\n================================================================================\n",
      "\u2705 STEP 1 COMPLETE - Profile extracted for Step 2!\n",
      "================================================================================\\n\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Profile Analyzer - Research and EXTRACT business profile\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udce1 STEP 1: PROFILE ANALYZER - Research Business\")\n",
    "print(\"=\"*80)\n",
    "print(\"Using: Tavily + OpenAI GPT-4\")\n",
    "print(\"Max time: 20 seconds\\\\n\")\n",
    "\n",
    "# Research with Tavily\n",
    "print(\"\ud83d\udd0d Researching with Tavily...\")\n",
    "search_results = tavily.search(f\"{BUSINESS_NAME} company industry business model target market customer demographics\", max_results=5, search_depth=\"advanced\")\n",
    "\n",
    "print(f\"\u2705 Found {len(search_results.get('results', []))} sources\\\\n\")\n",
    "\n",
    "# Extract complete profile with OpenAI\n",
    "print(\"\ud83e\udd16 Extracting business profile with OpenAI GPT-4...\")\n",
    "extract_prompt = f\"\"\"Analyze {BUSINESS_NAME} and extract complete business profile.\n",
    "\n",
    "Research Data: {json.dumps(search_results, indent=2)}\n",
    "\n",
    "Extract and return JSON:\n",
    "{{\n",
    "  \"business_name\": \"official company name\",\n",
    "  \"industry\": \"specific industry sector\",\n",
    "  \"business_model\": \"how they make money\",\n",
    "  \"target_market\": \"who are their customers\",\n",
    "  \"customer_demographics\": \"age, income, interests of customers\",\n",
    "  \"products_services\": [\"product1\", \"product2\"],\n",
    "  \"competitors\": [\"competitor1\", \"competitor2\"],\n",
    "  \"market_position\": \"leader/challenger/niche\"\n",
    "}}\n",
    "\n",
    "Be specific and detailed based on research data.\"\"\"\n",
    "\n",
    "response = llm_json.invoke([HumanMessage(content=extract_prompt)])\n",
    "business_profile = json.loads(response.content)\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"\ud83d\udcca STEP 1 OUTPUT - EXTRACTED BUSINESS PROFILE:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\\\n\ud83c\udfe2 Business: {business_profile.get('business_name', 'N/A')}\")\n",
    "print(f\"\\\\n\ud83d\udcc8 Industry: {business_profile.get('industry', 'N/A')}\")\n",
    "print(f\"\\\\n\ud83d\udcbc Business Model: {business_profile.get('business_model', 'N/A')}\")\n",
    "print(f\"\\\\n\ud83c\udfaf Target Market: {business_profile.get('target_market', 'N/A')}\")\n",
    "print(f\"\\\\n\ud83d\udc65 Customer Demographics: {business_profile.get('customer_demographics', 'N/A')}\")\n",
    "print(f\"\\\\n\ud83d\udecd\ufe0f Products/Services: {', '.join(business_profile.get('products_services', [])[:3])}\")\n",
    "print(f\"\\\\n\u2694\ufe0f Competitors: {', '.join(business_profile.get('competitors', [])[:3])}\")\n",
    "print(f\"\\\\n\ud83d\udcca Market Position: {business_profile.get('market_position', 'N/A')}\")\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"\u2705 STEP 1 COMPLETE - Profile extracted for Step 2!\")\n",
    "print(f\"{'='*80}\\\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Keyword Generator (OpenAI Creates Reddit Search Strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "\ud83e\udd16 STEP 2: KEYWORD GENERATOR - Create Reddit Search Strategy\n",
      "================================================================================\n",
      "Using: OpenAI GPT-4\\n\n",
      "\\n================================================================================\n",
      "\ud83d\udcca STEP 2 OUTPUT - Generated Reddit Search Strategy:\n",
      "================================================================================\n",
      "\\n\ud83d\udcc8 Industry: Language Learning\n",
      "\\n\ud83c\udfaf Target Audience: Individual consumers across various age groups, particularly younger demographics interested in learning new languages for travel, education, or personal development.\n",
      "\\n\ud83d\udd0d SEARCH KEYWORDS (48 generated):\n",
      "================================================================================\n",
      "\\n\ud83d\udccc Company-Specific (20):\n",
      "   1. \\\"Duolingo complaints\\\"\n",
      "   2. \\\"Duolingo issues\\\"\n",
      "   3. \\\"Duolingo problems\\\"\n",
      "   4. \\\"Duolingo quality\\\"\n",
      "   5. \\\"Duolingo pricing\\\"\n",
      "   6. \\\"Duolingo vs Rosetta Stone\\\"\n",
      "   7. \\\"Duolingo vs Babbel\\\"\n",
      "   8. \\\"Duolingo alternatives\\\"\n",
      "   9. \\\"Duolingo reviews\\\"\n",
      "   10. \\\"Super Duolingo complaints\\\"\n",
      "   11. \\\"Super Duolingo issues\\\"\n",
      "   12. \\\"Super Duolingo problems\\\"\n",
      "   13. \\\"Super Duolingo quality\\\"\n",
      "   14. \\\"Super Duolingo pricing\\\"\n",
      "   15. \\\"Super Duolingo vs competitors\\\"\n",
      "   16. \\\"Super Duolingo alternatives\\\"\n",
      "   17. \\\"Super Duolingo reviews\\\"\n",
      "   18. \\\"Duolingo Max complaints\\\"\n",
      "   19. \\\"Duolingo Max issues\\\"\n",
      "   20. \\\"Duolingo Max reviews\\\"\n",
      "\\n\ud83c\udf10 Industry-Wide (28):\n",
      "   1. \\\"language learning market trends\\\"\n",
      "   2. \\\"language learning technology changes\\\"\n",
      "   3. \\\"language learning customer pain points\\\"\n",
      "   4. \\\"language learning innovations\\\"\n",
      "   5. \\\"language learning future trends\\\"\n",
      "   6. \\\"language learning challenges\\\"\n",
      "   7. \\\"language learning opportunities\\\"\n",
      "   8. \\\"language learning digital transformation\\\"\n",
      "   9. \\\"language learning personalization\\\"\n",
      "   10. \\\"language learning gamification\\\"\n",
      "   11. \\\"language learning engagement strategies\\\"\n",
      "   12. \\\"language learning mobile apps\\\"\n",
      "   13. \\\"language learning AI integration\\\"\n",
      "   14. \\\"language learning best practices\\\"\n",
      "   15. \\\"language learning app issues\\\"\n",
      "   16. \\\"language learning app comparisons\\\"\n",
      "   17. \\\"language learning app recommendations\\\"\n",
      "   18. \\\"language learning app reviews\\\"\n",
      "   19. \\\"language learning app features\\\"\n",
      "   20. \\\"language learning app effectiveness\\\"\n",
      "   21. \\\"language learning app user experience\\\"\n",
      "   22. \\\"language learning app pricing\\\"\n",
      "   23. \\\"language learning app alternatives\\\"\n",
      "   24. \\\"language learning app innovations\\\"\n",
      "   25. \\\"language learning app trends\\\"\n",
      "   26. \\\"language learning app challenges\\\"\n",
      "   27. \\\"language learning app opportunities\\\"\n",
      "   28. \\\"language learning app personalization\\\"\n",
      "\\n\ud83d\udcf1 Target Subreddits (10):\n",
      "   1. r/languagelearning\n",
      "   2. r/duolingo\n",
      "   3. r/learnlanguages\n",
      "   4. r/language_exchange\n",
      "   5. r/polyglot\n",
      "   6. r/linguistics\n",
      "   7. r/education\n",
      "   8. r/edtech\n",
      "   9. r/technology\n",
      "   10. r/study\n",
      "\\n================================================================================\n",
      "\u2705 STEP 2 COMPLETE - 48 keywords ready for Reddit search!\n",
      "================================================================================\\n\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Generate Reddit search keywords with OpenAI\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"\ud83e\udd16 STEP 2: KEYWORD GENERATOR - Create Reddit Search Strategy\")\n",
    "print(\"=\"*80)\n",
    "print(\"Using: OpenAI GPT-4\\\\n\")\n",
    "\n",
    "profile_prompt = f'''Generate COMPREHENSIVE Reddit search strategy for {BUSINESS_NAME}.\n",
    "\n",
    "Business Profile: {json.dumps(business_profile, indent=2)}\n",
    "\n",
    "You MUST generate EXACTLY 50 SPECIFIC search keywords. Create many variations to maximize Reddit coverage.\n",
    "\n",
    "Categories (generate variations for each):\n",
    "1. Company-specific (20 keywords): complaints, issues, problems, quality, pricing, vs competitors, alternatives, reviews\n",
    "2. Industry trends (15 keywords): market trends, technology changes, customer pain points (NO company name)\n",
    "3. Product category (15 keywords): product type issues, best practices, comparisons (NO company name)\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"industry\": \"...\",\n",
    "  \"target_audience\": \"...\",\n",
    "  \"reddit_search_keywords\": [\"kw1\", \"kw2\", \"kw3\", ... \"kw50\"],\n",
    "  \"target_subreddits\": [\"r/sub1\", \"r/sub2\", ...]\n",
    "}}\n",
    "\n",
    "Generate ALL 50 keywords. Be creative with variations.'''\n",
    "\n",
    "response = llm_json.invoke([HumanMessage(content=profile_prompt)])\n",
    "profile = json.loads(response.content)\n",
    "\n",
    "keywords = profile.get('reddit_search_keywords', [])\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"\ud83d\udcca STEP 2 OUTPUT - Generated Reddit Search Strategy:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\\\n\ud83d\udcc8 Industry: {profile.get('industry', 'N/A')}\")\n",
    "print(f\"\\\\n\ud83c\udfaf Target Audience: {profile.get('target_audience', 'N/A')}\")\n",
    "\n",
    "print(f\"\\\\n\ud83d\udd0d SEARCH KEYWORDS ({len(keywords)} generated):\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Categorize and display keywords\n",
    "company_kw = [k for k in keywords if BUSINESS_NAME.lower() in k.lower()]\n",
    "industry_kw = [k for k in keywords if BUSINESS_NAME.lower() not in k.lower()]\n",
    "\n",
    "if company_kw:\n",
    "    print(f\"\\\\n\ud83d\udccc Company-Specific ({len(company_kw)}):\")\n",
    "    for i, kw in enumerate(company_kw, 1):\n",
    "        print(f\"   {i}. \\\\\\\"{kw}\\\\\\\"\")\n",
    "\n",
    "if industry_kw:\n",
    "    print(f\"\\\\n\ud83c\udf10 Industry-Wide ({len(industry_kw)}):\")\n",
    "    for i, kw in enumerate(industry_kw, 1):\n",
    "        print(f\"   {i}. \\\\\\\"{kw}\\\\\\\"\")\n",
    "\n",
    "print(f\"\\\\n\ud83d\udcf1 Target Subreddits ({len(profile.get('target_subreddits', []))}):\")\n",
    "for i, sub in enumerate(profile.get('target_subreddits', []), 1):\n",
    "    print(f\"   {i}. {sub}\")\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"\u2705 STEP 2 COMPLETE - {len(keywords)} keywords ready for Reddit search!\")\n",
    "print(f\"{'='*80}\\\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# STEP 3: TREND EXTRACTOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\ud83d\udcf1 STEP 3: TREND SCRAPER - Reddit MCP\n",
      "================================================================================\n",
      "Using: Reddit MCP (Public API, No Key Needed!)\n",
      "Time filter: WEEK (last 7 days only)\n",
      "\u23f1\ufe0f  HARD TIME LIMIT: 30 seconds (no more, no less!)\n",
      "\n",
      "\ud83d\udd0d Maximizing scraping in 30 seconds...\n",
      "\n",
      "   [1] 'Duolingo complaints...' (30.0s left) \u2705 +1\n",
      "   [2] 'Duolingo issues...' (29.8s left) \u2705 +10\n",
      "   [3] 'Duolingo problems...' (29.3s left) \u2705 +10\n",
      "   [4] 'Duolingo quality...' (28.9s left) \u2705 +6\n",
      "   [5] 'Duolingo pricing...' (28.4s left) \u2705 +3\n",
      "   [6] 'Duolingo vs Rosetta Stone...' (28.0s left) \u2705 +2\n",
      "   [7] 'Duolingo vs Babbel...' (27.7s left) \u2705 +7\n",
      "   [8] 'Duolingo alternatives...' (27.3s left) \u2705 +9\n",
      "   [9] 'Duolingo reviews...' (26.9s left) \u2705 +10\n",
      "   [10] 'Super Duolingo complaints...' (26.4s left) \u2705 +5\n",
      "   [11] 'Super Duolingo issues...' (26.0s left) \u2705 +9\n",
      "   [12] 'Super Duolingo problems...' (25.4s left) \u2705 +7\n",
      "   [13] 'Super Duolingo quality...' (24.9s left) \u2705 +8\n",
      "   [14] 'Super Duolingo pricing...' (24.4s left) \u2705 +1\n",
      "   [15] 'Super Duolingo vs competitors...' (24.1s left) \u2705 +10\n",
      "   [16] 'Super Duolingo alternatives...' (23.6s left) \u2705 +9\n",
      "   [17] 'Super Duolingo reviews...' (23.1s left) \u2705 +7\n",
      "   [18] 'Duolingo Max complaints...' (22.6s left) \u2705 +4\n",
      "   [19] 'Duolingo Max issues...' (22.1s left) \u2705 +10\n",
      "   [20] 'Duolingo Max reviews...' (21.5s left) \u2705 +8\n",
      "   [21] 'language learning market trends...' (21.0s left) \u2705 +10\n",
      "   [22] 'language learning technology changes...' (20.4s left) \u2705 +10\n",
      "   [23] 'language learning customer pain points...' (19.8s left) \u2705 +10\n",
      "   [24] 'language learning innovations...' (19.3s left) \u2705 +8\n",
      "   [25] 'language learning future trends...' (18.8s left) \u2705 +10\n",
      "   [26] 'language learning challenges...' (18.1s left) \u2705 +10\n",
      "   [27] 'language learning opportunities...' (17.6s left) \u2705 +8\n",
      "   [28] 'language learning digital transformation...' (17.2s left) \u2705 +8\n",
      "   [29] 'language learning personalization...' (16.4s left) \u2705 +10\n",
      "   [30] 'language learning gamification...' (16.0s left) \u2705 +5\n",
      "   [31] 'language learning engagement strategies...' (15.6s left) \u2705 +7\n",
      "   [32] 'language learning mobile apps...' (15.1s left) \u2705 +5\n",
      "   [33] 'language learning AI integration...' (14.5s left) \u2705 +5\n",
      "   [34] 'language learning best practices...' (14.0s left) \u2705 +9\n",
      "   [35] 'language learning app issues...' (13.5s left) \u2705 +8\n",
      "   [36] 'language learning app comparisons...' (13.0s left) \u2705 +8\n",
      "   [37] 'language learning app recommendations...' (12.5s left) \u2705 +7\n",
      "   [38] 'language learning app reviews...' (12.1s left) \u2705 +8\n",
      "   [39] 'language learning app features...' (11.5s left) \u2705 +6\n",
      "   [40] 'language learning app effectiveness...' (11.0s left) \u2705 +5\n",
      "   [41] 'language learning app user experience...' (10.6s left) \u2705 +6\n",
      "   [42] 'language learning app pricing...' (10.2s left) \u2705 +5\n",
      "   [43] 'language learning app alternatives...' (9.7s left) \u2705 +9\n",
      "   [44] 'language learning app innovations...' (9.0s left) \u2705 +6\n",
      "   [45] 'language learning app trends...' (8.5s left) \u2705 +9\n",
      "   [46] 'language learning app challenges...' (7.9s left) \u2705 +9\n",
      "   [47] 'language learning app opportunities...' (7.5s left) \u2705 +9\n",
      "   [48] 'language learning app personalization...' (6.9s left) \u2705 +4\n",
      "   [49] 'Duolingo complaints...' (6.4s left) \u2705 +1\n",
      "   [50] 'Duolingo issues...' (6.2s left) \u2705 +10\n",
      "   [51] 'Duolingo problems...' (5.7s left) \u2705 +10\n",
      "   [52] 'Duolingo quality...' (5.3s left) \u2705 +6\n",
      "   [53] 'Duolingo pricing...' (4.7s left) \u2705 +3\n",
      "   [54] 'Duolingo vs Rosetta Stone...' (4.4s left) \u2705 +2\n",
      "   [55] 'Duolingo vs Babbel...' (4.1s left) \u2705 +7\n",
      "   [56] 'Duolingo alternatives...' (3.7s left) \u2705 +9\n",
      "   [57] 'Duolingo reviews...' (3.3s left) \u2705 +10\n",
      "   [58] 'Super Duolingo complaints...' (2.8s left) \u2705 +5\n",
      "   [59] 'Super Duolingo issues...' (2.4s left) \u2705 +9\n",
      "   [60] 'Super Duolingo problems...' (1.9s left) \u2705 +7\n",
      "   [61] 'Super Duolingo quality...' (1.4s left) \u2705 +8\n",
      "   [62] 'Super Duolingo pricing...' (0.7s left) \u2705 +1\n",
      "   [63] 'Super Duolingo vs competitors...' (0.4s left) \u2705 +10\n",
      "   [64] 'Super Duolingo alternatives...' (0.0s left) \u2705 +9\n",
      "\n",
      "\ud83e\uddf9 Deduplicating...\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcca STEP 3 RESULTS:\n",
      "================================================================================\n",
      "\u2705 Scraped: 235 high-quality posts\n",
      "\u23f1\ufe0f  Time: 30.43 seconds (limit: 30s)\n",
      "\ud83d\udd0d Keywords searched: 64\n",
      "\ud83d\udcc5 Timeframe: Last 7 days (1 week)\n",
      "\ud83c\udfaf Min engagement: 5+ comments per post\n",
      "\n",
      "\ud83d\udcc8 Engagement Stats:\n",
      "   Total upvotes: 150,137\n",
      "   Total comments: 31,583\n",
      "   Avg upvotes/post: 638\n",
      "   Avg comments/post: 134\n",
      "\n",
      "\ud83d\udccc Top 5 Posts by Engagement:\n",
      "\n",
      "   1. \"Those who have visited other countries, what was the biggest...\"\n",
      "      r/AskTheWorld\n",
      "      5,609\u2b06\ufe0f  6,644\ud83d\udcac  Engagement: 18,897\n",
      "\n",
      "   2. \"Rockstar employee anonymously shares account of the company'...\"\n",
      "      r/gaming\n",
      "      11,849\u2b06\ufe0f  740\ud83d\udcac  Engagement: 13,329\n",
      "\n",
      "   3. \"My brother has convinced my mom that I, an openly gay man, a...\"\n",
      "      r/BestofRedditorUpdates\n",
      "      8,154\u2b06\ufe0f  608\ud83d\udcac  Engagement: 9,370\n",
      "\n",
      "   4. \"The top 5 ways Nico Harrison ruined the Mavericks...\"\n",
      "      r/nba\n",
      "      7,875\u2b06\ufe0f  725\ud83d\udcac  Engagement: 9,325\n",
      "\n",
      "   5. \"I've been \"Mute\" for eight years...\"\n",
      "      r/TrueOffMyChest\n",
      "      7,528\u2b06\ufe0f  595\ud83d\udcac  Engagement: 8,718\n",
      "\n",
      "\ud83d\udcc2 Subreddit Coverage (180 unique):\n",
      "   r/duolingo: 14 posts\n",
      "   r/BestofRedditorUpdates: 8 posts\n",
      "   r/languagelearning: 8 posts\n",
      "   r/AITAH: 3 posts\n",
      "   r/godot: 3 posts\n",
      "\n",
      "================================================================================\n",
      "\u2705 STEP 3 COMPLETE - Data ready for Ranking Agent!\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udd04 Next: Ranking Agent will analyze these 235 posts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: REDDIT MCP SCRAPER - 30 SECOND HARD LIMIT\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcf1 STEP 3: TREND SCRAPER - Reddit MCP\")\n",
    "print(\"=\"*80)\n",
    "print(\"Using: Reddit MCP (Public API, No Key Needed!)\")\n",
    "print(\"Time filter: WEEK (last 7 days only)\")\n",
    "print(\"\u23f1\ufe0f  HARD TIME LIMIT: 30 seconds (no more, no less!)\\n\")\n",
    "\n",
    "TIME_LIMIT = 30  # Hard 30-second limit\n",
    "reddit_posts = []\n",
    "keywords = profile.get('reddit_search_keywords', [])  # Use ALL keywords\n",
    "start_time = time.time()\n",
    "keywords_searched = 0\n",
    "\n",
    "print(f\"\ud83d\udd0d Maximizing scraping in {TIME_LIMIT} seconds...\\n\")\n",
    "\n",
    "# Keep looping through keywords until we hit 30 seconds\n",
    "keyword_index = 0\n",
    "while True:\n",
    "    # Check time BEFORE each operation\n",
    "    elapsed = time.time() - start_time\n",
    "    if elapsed >= TIME_LIMIT:\n",
    "        print(f\"\\n\u23f1\ufe0f  30 seconds reached - stopping scraping\")\n",
    "        break\n",
    "    \n",
    "    # Cycle through keywords (loop back to start if we run out)\n",
    "    keyword = keywords[keyword_index % len(keywords)]\n",
    "    keyword_index += 1\n",
    "    keywords_searched += 1\n",
    "    \n",
    "    remaining = TIME_LIMIT - elapsed\n",
    "    print(f\"   [{keywords_searched}] '{keyword[:40]}...' ({remaining:.1f}s left) \", end=\"\", flush=True)\n",
    "    \n",
    "    try:\n",
    "        # Search Reddit using MCP (1-week filter)\n",
    "        results = reddit.search_posts(\n",
    "            query=keyword,\n",
    "            t=\"week\",  # Last 7 days ONLY\n",
    "            limit=10\n",
    "        )\n",
    "        \n",
    "        # Process posts quickly - add all with 5+ comments\n",
    "        new_posts = 0\n",
    "        for post in results.items:\n",
    "            if post.num_comments >= 5:  # High engagement filter\n",
    "                reddit_posts.append({\n",
    "                    \"title\": post.title,\n",
    "                    \"subreddit\": post.subreddit,\n",
    "                    \"author\": post.author,\n",
    "                    \"score\": post.score,\n",
    "                    \"num_upvotes\": post.score,\n",
    "                    \"num_comments\": post.num_comments,\n",
    "                    \"created_utc\": post.created_utc,\n",
    "                    \"url\": post.url,\n",
    "                    \"selftext\": post.selftext[:1000] if post.selftext else \"\",\n",
    "                    \"permalink\": post.permalink,\n",
    "                    \"id\": post.id,\n",
    "                    \"link_flair_text\": post.link_flair_text\n",
    "                })\n",
    "                new_posts += 1\n",
    "        \n",
    "        print(f\"\u2705 +{new_posts}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f error\")\n",
    "    \n",
    "    # Check if we've exceeded time limit (safety check)\n",
    "    if time.time() - start_time >= TIME_LIMIT:\n",
    "        break\n",
    "\n",
    "# Final elapsed time\n",
    "final_elapsed = time.time() - start_time\n",
    "\n",
    "# Remove duplicates by post ID\n",
    "print(f\"\\n\ud83e\uddf9 Deduplicating...\")\n",
    "seen_ids = set()\n",
    "unique_posts = []\n",
    "for post in reddit_posts:\n",
    "    if post['id'] not in seen_ids:\n",
    "        seen_ids.add(post['id'])\n",
    "        unique_posts.append(post)\n",
    "\n",
    "reddit_posts = unique_posts\n",
    "\n",
    "# Sort by engagement (score + 2*comments) to prioritize discussion\n",
    "reddit_posts.sort(key=lambda p: p['num_upvotes'] + (2 * p['num_comments']), reverse=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"\ud83d\udcca STEP 3 RESULTS:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\u2705 Scraped: {len(reddit_posts)} high-quality posts\")\n",
    "print(f\"\u23f1\ufe0f  Time: {final_elapsed:.2f} seconds (limit: {TIME_LIMIT}s)\")\n",
    "print(f\"\ud83d\udd0d Keywords searched: {keywords_searched}\")\n",
    "print(f\"\ud83d\udcc5 Timeframe: Last 7 days (1 week)\")\n",
    "print(f\"\ud83c\udfaf Min engagement: 5+ comments per post\")\n",
    "\n",
    "if len(reddit_posts) > 0:\n",
    "    total_upvotes = sum(p['num_upvotes'] for p in reddit_posts)\n",
    "    total_comments = sum(p['num_comments'] for p in reddit_posts)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc8 Engagement Stats:\")\n",
    "    print(f\"   Total upvotes: {total_upvotes:,}\")\n",
    "    print(f\"   Total comments: {total_comments:,}\")\n",
    "    print(f\"   Avg upvotes/post: {total_upvotes//len(reddit_posts):,}\")\n",
    "    print(f\"   Avg comments/post: {total_comments//len(reddit_posts):,}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udccc Top 5 Posts by Engagement:\")\n",
    "    for i, post in enumerate(reddit_posts[:5], 1):\n",
    "        engagement = post['num_upvotes'] + (2 * post['num_comments'])\n",
    "        print(f\"\\n   {i}. \\\"{post['title'][:60]}...\\\"\")\n",
    "        print(f\"      r/{post['subreddit']}\")\n",
    "        print(f\"      {post['num_upvotes']:,}\u2b06\ufe0f  {post['num_comments']:,}\ud83d\udcac  Engagement: {engagement:,}\")\n",
    "    \n",
    "    # Count subreddits represented\n",
    "    subreddit_counts = {}\n",
    "    for post in reddit_posts:\n",
    "        sub = post['subreddit']\n",
    "        subreddit_counts[sub] = subreddit_counts.get(sub, 0) + 1\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc2 Subreddit Coverage ({len(subreddit_counts)} unique):\")\n",
    "    for sub, count in sorted(subreddit_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"   r/{sub}: {count} posts\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"\u2705 STEP 3 COMPLETE - Data ready for Ranking Agent!\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n\u26a0\ufe0f  WARNING: No posts found in {TIME_LIMIT} seconds!\")\n",
    "    print(f\"   Try broader keywords or check subreddit names\\n\")\n",
    "\n",
    "print(f\"\ud83d\udd04 Next: Ranking Agent will analyze these {len(reddit_posts)} posts\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3B: Export URLs to Excel for Manual Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\ud83d\udcca STEP 3B: EXPORT TO EXCEL FOR VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udce5 Exporting 235 posts to Excel...\n",
      "\n",
      "\u2705 Excel file created: Duolingo_Reddit_URLs.xlsx\n",
      "\n",
      "\ud83d\udccb File contents:\n",
      "   Rows: 235\n",
      "   Columns: 13 (Row, Title, Subreddit, Full_URL, Upvotes, Comments,\n",
      "            Engagement_Score, Posted_Date, Days_Ago, Author, Post_ID,\n",
      "            Has_Text, Text_Preview)\n",
      "\n",
      "\ud83d\udce5 Download 'Duolingo_Reddit_URLs.xlsx' to verify URLs manually!\n",
      "   All 235 Reddit permalinks are in the 'Full_URL' column\n",
      "\n",
      "================================================================================\n",
      "\u2705 STEP 3B COMPLETE - Excel file ready for download!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Export all Reddit posts to Excel for manual verification\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca STEP 3B: EXPORT TO EXCEL FOR VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(reddit_posts) > 0:\n",
    "    print(f\"\\n\ud83d\udce5 Exporting {len(reddit_posts)} posts to Excel...\\n\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime as dt\n",
    "    \n",
    "    # Prepare data for Excel\n",
    "    excel_data = []\n",
    "    for i, post in enumerate(reddit_posts, 1):\n",
    "        post_date = dt.fromtimestamp(post['created_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        days_ago = (dt.now() - dt.fromtimestamp(post['created_utc'])).days\n",
    "        \n",
    "        excel_data.append({\n",
    "            'Row': i,\n",
    "            'Title': post['title'],\n",
    "            'Subreddit': f\"r/{post['subreddit']}\",\n",
    "            'Full_URL': post['permalink'],\n",
    "            'Upvotes': post['num_upvotes'],\n",
    "            'Comments': post['num_comments'],\n",
    "            'Engagement_Score': post['num_upvotes'] + (2 * post['num_comments']),\n",
    "            'Posted_Date': post_date,\n",
    "            'Days_Ago': days_ago,\n",
    "            'Author': f\"u/{post['author']}\",\n",
    "            'Post_ID': post['id'],\n",
    "            'Has_Text': 'Yes' if post.get('selftext') else 'No',\n",
    "            'Text_Preview': post.get('selftext', '')[:200]\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(excel_data)\n",
    "    \n",
    "    # Save to Excel\n",
    "    excel_filename = f\"{BUSINESS_NAME.replace(' ', '_')}_Reddit_URLs.xlsx\"\n",
    "    df.to_excel(excel_filename, index=False, sheet_name='Reddit Posts', engine='openpyxl')\n",
    "    \n",
    "    print(f\"\u2705 Excel file created: {excel_filename}\")\n",
    "    print(f\"\\n\ud83d\udccb File contents:\")\n",
    "    print(f\"   Rows: {len(reddit_posts)}\")\n",
    "    print(f\"   Columns: 13 (Row, Title, Subreddit, Full_URL, Upvotes, Comments,\")\n",
    "    print(f\"            Engagement_Score, Posted_Date, Days_Ago, Author, Post_ID,\")\n",
    "    print(f\"            Has_Text, Text_Preview)\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udce5 Download '{excel_filename}' to verify URLs manually!\")\n",
    "    print(f\"   All {len(reddit_posts)} Reddit permalinks are in the 'Full_URL' column\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"\u2705 STEP 3B COMPLETE - Excel file ready for download!\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f No posts to export (Step 3 returned 0 posts)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Ranking Agent (OpenAI Ranks Insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\ud83d\udcca STEP 4: RANKING AGENT\n",
      "================================================================================\n",
      "\ud83d\udcca Analyzing 235 posts (max 15s)...\n",
      "\n",
      "\u26a0\ufe0f Timeout after 15s - using basic analysis\n",
      "\n",
      "\u2705 STEP 4 DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: RANKING AGENT - Extract SPECIFIC, DETAILED insights (MAX 15s)\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca STEP 4: RANKING AGENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not reddit_posts:\n",
    "    print(\"\u26a0\ufe0f No posts to rank\")\n",
    "    ranked_data = {}\n",
    "else:\n",
    "    print(f\"\ud83d\udcca Analyzing {len(reddit_posts)} posts (max 15s)...\\n\")\n",
    "    \n",
    "    start_step4 = time.time()\n",
    "    \n",
    "    # Include post IDs for citation tracking\n",
    "    posts_for_analysis = []\n",
    "    for idx, post in enumerate(reddit_posts[:100], 1):\n",
    "        posts_for_analysis.append({\n",
    "            \"post_id\": idx,\n",
    "            \"title\": post.get('title', '')[:300],\n",
    "            \"subreddit\": post.get('subreddit', ''),\n",
    "            \"url\": post.get('url', ''),\n",
    "            \"upvotes\": post.get('num_upvotes', 0),\n",
    "            \"comments\": post.get('num_comments', 0)\n",
    "        })\n",
    "    \n",
    "    ranking_prompt = f\"\"\"Analyze {len(posts_for_analysis)} Reddit posts for {BUSINESS_NAME}.\n",
    "\n",
    "Business: {BUSINESS_NAME}\n",
    "Industry: {profile.get('industry', 'N/A')}\n",
    "Target Market: {profile.get('target_market', 'N/A')[:200]}\n",
    "\n",
    "Reddit Posts:\n",
    "{json.dumps(posts_for_analysis, indent=2)}\n",
    "\n",
    "Extract JSON with SPECIFIC, DETAILED insights:\n",
    "{{\n",
    "  \"total_posts_analyzed\": {len(reddit_posts)},\n",
    "  \"ranked_posts\": [\n",
    "    {{\"post_id\": 1, \"title\": \"...\", \"subreddit\": \"...\", \"relevance_score\": 0.95, \"key_insight\": \"specific insight\"}},\n",
    "    ... (top 10)\n",
    "  ],\n",
    "  \"pain_points\": [\n",
    "    {{\n",
    "      \"pain\": \"HIGHLY SPECIFIC pain point with numbers/details (e.g., 'Users losing 3-5 hours daily due to energy system')\",\n",
    "      \"supporting_posts\": [1, 3, 5],\n",
    "      \"severity\": \"high/medium/low\"\n",
    "    }},\n",
    "    ... (5-10 pain points, each with SPECIFIC details and post citations)\n",
    "  ],\n",
    "  \"overall_trends\": [\n",
    "    {{\n",
    "      \"trend\": \"SPECIFIC trend with timeframe and context (e.g., 'Over past 7 days, 15+ posts discussing migration to LibreLingo after $3 price increase')\",\n",
    "      \"supporting_posts\": [2, 4, 7, 9],\n",
    "      \"momentum\": \"rising/stable/declining\"\n",
    "    }},\n",
    "    ... (5-10 trends, each with SPECIFIC details, examples, and post citations)\n",
    "  ],\n",
    "  \"sentiment_summary\": \"overall sentiment with specifics\",\n",
    "  \"subreddit_breakdown\": {{\"r/sub1\": \"specific insight\", \"r/sub2\": \"specific insight\"}}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Pain points MUST be HIGHLY SPECIFIC with numbers, examples, details\n",
    "2. Trends MUST include timeframe, scale, and actionable context\n",
    "3. EVERY pain/trend MUST cite supporting_posts (list of post IDs)\n",
    "4. Include severity/momentum indicators\n",
    "5. NO generic statements - only specific, detailed insights\"\"\"\n",
    "\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(\n",
    "                lambda: json.loads(llm_json.invoke([HumanMessage(content=ranking_prompt)]).content)\n",
    "            )\n",
    "            ranked_data = future.result(timeout=15)\n",
    "        \n",
    "        step4_time = time.time() - start_step4\n",
    "        \n",
    "        print(f\"\u2705 Analysis complete ({step4_time:.1f}s):\")\n",
    "        print(f\"   Total posts: {ranked_data.get('total_posts_analyzed', 0)}\")\n",
    "        print(f\"   Top ranked: {len(ranked_data.get('ranked_posts', []))}\")\n",
    "        print(f\"   Pain points: {len(ranked_data.get('pain_points', []))}\")\n",
    "        print(f\"   Trends: {len(ranked_data.get('overall_trends', []))}\")\n",
    "        \n",
    "        # Show detailed pain points with citations\n",
    "        if ranked_data.get('pain_points'):\n",
    "            print(f\"\\n\ud83d\udccc Top Pain Points (with citations):\")\n",
    "            for idx, pain_obj in enumerate(ranked_data.get('pain_points', [])[:5], 1):\n",
    "                if isinstance(pain_obj, dict):\n",
    "                    pain_text = pain_obj.get('pain', str(pain_obj))\n",
    "                    posts = pain_obj.get('supporting_posts', [])\n",
    "                    print(f\"   {idx}. {pain_text}\")\n",
    "                    print(f\"      (Posts: {posts})\")\n",
    "                else:\n",
    "                    print(f\"   {idx}. {pain_obj}\")\n",
    "    \n",
    "    except concurrent.futures.TimeoutError:\n",
    "        print(f\"\u26a0\ufe0f Timeout after 15s - using basic analysis\")\n",
    "        ranked_data = {\n",
    "            \"total_posts_analyzed\": len(reddit_posts),\n",
    "            \"ranked_posts\": [{\"post_id\": i+1, \"title\": p.get('title', ''), \"subreddit\": p.get('subreddit', ''), \"relevance_score\": 0.8} for i, p in enumerate(reddit_posts[:10])],\n",
    "            \"pain_points\": [{\"pain\": \"Analysis timed out - rerun for insights\", \"supporting_posts\": []}],\n",
    "            \"overall_trends\": [{\"trend\": \"Analysis timed out\", \"supporting_posts\": []}],\n",
    "            \"sentiment_summary\": \"Unknown\"\n",
    "        }\n",
    "\n",
    "print(\"\\n\u2705 STEP 4 DONE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Report Generator (OpenAI Creates Final Report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\ud83d\udcdd STEP 5: REPORT GENERATOR - Create Final Intelligence Report\n",
      "================================================================================\n",
      "Using: OpenAI GPT-4 (with CITATION TRACKING)\n",
      "\n",
      "\n",
      "\u2705 Report generated (3992 characters)\n",
      "\u2705 Groundedness: 0.0\n",
      "\u2705 Citation Diversity: 0.5\n",
      "   (High diversity = good grounding)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcca STEP 5 OUTPUT - FINAL INTELLIGENCE REPORT\n",
      "================================================================================\n",
      "\n",
      "# Duolingo Marketing Intelligence Report\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "1. **User Engagement and Cultural Insights**\n",
      "   Users frequently discuss cultural experiences and language learning as a means to better understand different cultures. This aligns with Duolingo's mission to make language learning accessible and engaging.\n",
      "   - [Post #1: r/AskTheWorld](https://i.redd.it/tatc3ys2k20g1.jpeg)\n",
      "\n",
      "2. **Comparative Analysis with Competitors**\n",
      "   Discussions often compare Duolingo with other language learning ...\n",
      "\n",
      "================================================================================\n",
      "\u2705 STEP 5 DONE - Report ready for PDF/evaluation\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: REPORT GENERATOR - Create grounded report with diverse citations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcdd STEP 5: REPORT GENERATOR - Create Final Intelligence Report\")\n",
    "print(\"=\"*80)  \n",
    "print(\"Using: OpenAI GPT-4 (with CITATION TRACKING)\\n\")\n",
    "\n",
    "if len(reddit_posts) == 0:\n",
    "    print(\"\u26a0\ufe0f WARNING: No Reddit data!\")\n",
    "    final_report = f\"# Marketing Intelligence Report for {BUSINESS_NAME}\\n\\nNo Reddit data available.\"\n",
    "else:\n",
    "    # Build post lookup with IDs\n",
    "    post_lookup = {}\n",
    "    for idx, post in enumerate(reddit_posts[:100], 1):\n",
    "        post_lookup[idx] = {\n",
    "            \"id\": idx,\n",
    "            \"title\": post.get('title', ''),\n",
    "            \"subreddit\": post.get('subreddit', ''),\n",
    "            \"url\": post.get('url', ''),\n",
    "            \"upvotes\": post.get('num_upvotes', 0),\n",
    "            \"comments\": post.get('num_comments', 0)\n",
    "        }\n",
    "\n",
    "    report_prompt = f\"\"\"Generate marketing intelligence report for {BUSINESS_NAME}.\n",
    "\n",
    "Business Profile:\n",
    "{json.dumps(profile, indent=2)[:800]}\n",
    "\n",
    "Insights with Post Citations:\n",
    "{json.dumps(ranked_data, indent=2)[:3000]}\n",
    "\n",
    "Post Lookup (for citations):\n",
    "{json.dumps(post_lookup, indent=2)[:2000]}\n",
    "\n",
    "CRITICAL CITATION REQUIREMENTS:\n",
    "1. For EACH pain point: Use the supporting_posts IDs to cite specific posts\n",
    "2. For EACH trend: Use the supporting_posts IDs to cite multiple posts\n",
    "3. Format: [Post #X: r/subreddit](URL)\n",
    "4. DIVERSE citations - don't cite same post repeatedly\n",
    "5. If insight has supporting_posts [1,3,5], cite ALL of them\n",
    "6. NO claims without citations\n",
    "\n",
    "Report Structure:\n",
    "1. Executive Summary (3 insights, EACH citing different posts)\n",
    "2. Pain Points (EACH with citations from supporting_posts)\n",
    "3. Trending Topics (EACH with multiple citations from supporting_posts)\n",
    "4. Recommended Actions (based on cited insights)\n",
    "5. Top Discussions (show Post IDs, URLs, quotes)\n",
    "\n",
    "Example Pain Point Format:\n",
    "- **[Specific Pain Point with Details]**\n",
    "  Users report [specific issue with numbers/context].\n",
    "  - [Post #1: r/subreddit1](URL1)\n",
    "  - [Post #3: r/subreddit2](URL2)\n",
    "  - [Post #5: r/subreddit3](URL3)\n",
    "\n",
    "Example Trend Format:\n",
    "- **[Specific Trend with Timeframe and Scale]**\n",
    "  Over the past [timeframe], [specific observation with numbers].\n",
    "  - [Post #2: r/subreddit](URL)\n",
    "  - [Post #4: r/subreddit](URL)\n",
    "  - [Post #7: r/subreddit](URL)\n",
    "\n",
    "Format as markdown. GROUND EVERY CLAIM with DIVERSE, SPECIFIC citations.\"\"\"\n",
    "\n",
    "    report_response = llm.invoke([HumanMessage(content=report_prompt)])\n",
    "    report = report_response.content\n",
    "\n",
    "    # Validate citation diversity\n",
    "    validation_prompt = f\"\"\"Validate report citations:\n",
    "Report: {report[:2000]}\n",
    "Post Lookup: {json.dumps(post_lookup, indent=2)[:1000]}\n",
    "\n",
    "Check:\n",
    "1. Does EVERY claim have citations?\n",
    "2. Are citations DIVERSE (not same post repeatedly)?\n",
    "3. Are citations accurate (Post IDs match URLs)?\n",
    "\n",
    "Return JSON: {{\"groundedness_score\": 0.0-1.0, \"citation_diversity\": 0.0-1.0, \"validation_passed\": true/false}}\"\"\"\n",
    "\n",
    "    validation = json.loads(llm_json.invoke([HumanMessage(content=validation_prompt)]).content)\n",
    "\n",
    "    # Final report with metadata\n",
    "    final_report = report + f\"\"\"\n",
    "\n",
    "---\n",
    "**Report Metadata**\n",
    "- Business: {BUSINESS_NAME}\n",
    "- Reddit Posts Analyzed: {len(reddit_posts)}\n",
    "- Groundedness: {validation.get('groundedness_score', 0):.1f}\n",
    "- Citation Diversity: {validation.get('citation_diversity', 0):.1f}\n",
    "- Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"\\n\u2705 Report generated ({len(report)} characters)\")\n",
    "    print(f\"\u2705 Groundedness: {validation.get('groundedness_score', 0):.1f}\")\n",
    "    print(f\"\u2705 Citation Diversity: {validation.get('citation_diversity', 0):.1f}\")\n",
    "    print(\"   (High diversity = good grounding)\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca STEP 5 OUTPUT - FINAL INTELLIGENCE REPORT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(final_report[:500] + \"...\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"\u2705 STEP 5 DONE - Report ready for PDF/evaluation\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summarizer - Generate PDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\ud83d\udcc4 STEP 6: SUMMARIZER - Auto-Generate PDF Report\n",
      "================================================================================\n",
      "\n",
      "\u2705 PDF AUTO-GENERATED:\n",
      "   \ud83d\udcc4 File: Duolingo_Report.pdf\n",
      "   \ud83d\udcca Posts: 235\n",
      "   \ud83d\udcdd Length: 4144 chars\n",
      "   \ud83d\udcbe Size: 5.2 KB\n",
      "\n",
      "\u2705 PDF ready at: Duolingo_Report.pdf\n",
      "\n",
      "\u2705 STEP 6 COMPLETE!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: SUMMARIZER - Auto-Generate PDF\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcc4 STEP 6: SUMMARIZER - Auto-Generate PDF Report\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install reportlab -q\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "\n",
    "# Create PDF\n",
    "filename = f\"{BUSINESS_NAME.replace(' ', '_')}_Report.pdf\"\n",
    "doc = SimpleDocTemplate(filename, pagesize=letter)\n",
    "styles = getSampleStyleSheet()\n",
    "story = []\n",
    "\n",
    "# Title\n",
    "title = Paragraph(f\"<b>Marketing Intelligence Report: {BUSINESS_NAME}</b>\", styles['Title'])\n",
    "story.append(title)\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "# Metadata\n",
    "meta_text = f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br/>Posts Analyzed: {len(reddit_posts)}<br/>Groundedness: {validation.get('groundedness_score', 'N/A') if 'validation' in dir() else 'N/A'}\"\n",
    "meta = Paragraph(meta_text, styles['Normal'])\n",
    "story.append(meta)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Report content - split into paragraphs\n",
    "for line in final_report.split('\\n'):\n",
    "    if line.strip():\n",
    "        # Clean line\n",
    "        line = line.replace('#', '').strip()\n",
    "        if len(line) > 3:\n",
    "            try:\n",
    "                p = Paragraph(line, styles['Normal'])\n",
    "                story.append(p)\n",
    "                story.append(Spacer(1, 6))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Build PDF\n",
    "doc.build(story)\n",
    "\n",
    "print(f\"\\n\u2705 PDF AUTO-GENERATED:\")\n",
    "print(f\"   \ud83d\udcc4 File: {filename}\")\n",
    "print(f\"   \ud83d\udcca Posts: {len(reddit_posts)}\")\n",
    "print(f\"   \ud83d\udcdd Length: {len(final_report)} chars\")\n",
    "\n",
    "import os\n",
    "if os.path.exists(filename):\n",
    "    size_kb = os.path.getsize(filename) / 1024\n",
    "    print(f\"   \ud83d\udcbe Size: {size_kb:.1f} KB\")\n",
    "    print(f\"\\n\u2705 PDF ready at: {filename}\")\n",
    "else:\n",
    "    print(f\"   \u26a0\ufe0f PDF not found\")\n",
    "\n",
    "print(f\"\\n\u2705 STEP 6 COMPLETE!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: Validator - Verify report groundedness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\ud83d\udcca STEP 7: EVALUATION - 5 LLM Judges (TruLens)\n",
      "================================================================================\n",
      "\ud83d\udd27 Initializing TruLens...\n",
      "\n",
      "\ud83e\udd91 Initialized with db url sqlite:///marketing_intel_evaluation.sqlite .\n",
      "\ud83d\uded1 Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n",
      "\u2705 TruLens initialized with gpt-4o provider\n",
      "\n",
      "\ud83e\udd16 Running 5 LLM Judge Evaluations...\n",
      "\n",
      "1\ufe0f\u20e3 Evaluating User Identification Relevance...\n",
      "   Score: 1.00\n",
      "\n",
      "2\ufe0f\u20e3 Evaluating Community Relevance...\n",
      "   Score: 0.90\n",
      "\n",
      "3\ufe0f\u20e3 Evaluating Insight Extraction Quality...\n",
      "   Score: 0.00\n",
      "\n",
      "4\ufe0f\u20e3 Evaluating Trend Relevance...\n",
      "   Score: 0.00\n",
      "\n",
      "5\ufe0f\u20e3 Evaluating Groundedness...\n",
      "   Score: 0.70\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcca EVALUATION RESULTS:\n",
      "================================================================================\n",
      "\n",
      "1\ufe0f\u20e3 User Identification Relevance: 1.00\n",
      "2\ufe0f\u20e3 Community Relevance:           0.90\n",
      "3\ufe0f\u20e3 Insight Extraction Quality:    0.00\n",
      "4\ufe0f\u20e3 Trend Relevance:                0.00\n",
      "5\ufe0f\u20e3 Groundedness:                   0.70\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcc8 AVERAGE SCORE: 0.52\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udcbe Evaluation scores ready for TruLens recording (Step 8)\n",
      "\u2705 STEP 7 COMPLETE!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: EVALUATION - 5 LLM Judges with TruLens\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca STEP 7: EVALUATION - 5 LLM Judges (TruLens)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize TruLens\n",
    "from trulens.core.database.connector.default import DefaultDBConnector\n",
    "from trulens.core.session import TruSession\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "print(\"\ud83d\udd27 Initializing TruLens...\\n\")\n",
    "\n",
    "# Create TruLens session with SQLite database\n",
    "connector = DefaultDBConnector(database_url=\"sqlite:///marketing_intel_evaluation.sqlite\")\n",
    "session = TruSession(connector=connector)\n",
    "# session.reset_database()  # Don't reset - would delete data\n",
    "\n",
    "# Initialize OpenAI provider for LLM-as-judge (using TruLens provider)\n",
    "eval_provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "\n",
    "print(\"\u2705 TruLens initialized with gpt-4o provider\\n\")\n",
    "print(\"\ud83e\udd16 Running 5 LLM Judge Evaluations...\\n\")\n",
    "\n",
    "# METRIC 1: User Identification Relevance\n",
    "print(\"1\ufe0f\u20e3 Evaluating User Identification Relevance...\")\n",
    "user_id_context = f\"\"\"Business Name: {BUSINESS_NAME}\n",
    "Identified Industry: {business_profile.get('industry', 'N/A')}\n",
    "Business Model: {business_profile.get('business_model', 'N/A')}\n",
    "Target Market: {business_profile.get('target_market', 'N/A')}\n",
    "Market Position: {business_profile.get('market_position', 'N/A')}\"\"\"\n",
    "\n",
    "user_id_prompt = f\"\"\"Rate from 0 to 1 how well the profile analyzer identified the business's industry, professional activity, and market position.\n",
    "\n",
    "{user_id_context}\n",
    "\n",
    "Return only a number between 0 and 1, where:\n",
    "- 0.0-0.3: Poor identification, missing key details\n",
    "- 0.4-0.6: Adequate but incomplete\n",
    "- 0.7-0.9: Good identification with most details\n",
    "- 1.0: Excellent, comprehensive identification\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "# Real GPT-4o evaluation (no fallbacks)\n",
    "response = llm.invoke([HumanMessage(content=user_id_prompt)])\n",
    "s1 = float(response.content.strip())\n",
    "print(f\"   Score: {s1:.2f}\\n\")\n",
    "\n",
    "# METRIC 2: Community Relevance\n",
    "print(\"2\ufe0f\u20e3 Evaluating Community Relevance...\")\n",
    "community_context = f\"\"\"Target Market: {business_profile.get('target_market', 'N/A')}\n",
    "Customer Demographics: {business_profile.get('customer_demographics', 'N/A')}\n",
    "Target Subreddits: {', '.join(profile.get('target_subreddits', [])[:10])}\"\"\"\n",
    "\n",
    "community_prompt = f\"\"\"Rate from 0 to 1 how well the discovered subreddits match the target audience description.\n",
    "\n",
    "{community_context}\n",
    "\n",
    "Consider:\n",
    "- Do the subreddits align with the target market?\n",
    "- Are they relevant to the customer demographics?\n",
    "- Would these communities have meaningful discussions about this business?\n",
    "\n",
    "Return only a number between 0 and 1:\n",
    "- 0.0-0.3: Poor match, irrelevant communities\n",
    "- 0.4-0.6: Some relevance but misaligned\n",
    "- 0.7-0.9: Good match, mostly relevant\n",
    "- 1.0: Excellent match, perfectly aligned\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "# Real GPT-4o evaluation (no fallbacks)\n",
    "response = llm.invoke([HumanMessage(content=community_prompt)])\n",
    "s2 = float(response.content.strip())\n",
    "print(f\"   Score: {s2:.2f}\\n\")\n",
    "\n",
    "# METRIC 3: Insight Extraction Quality\n",
    "print(\"3\ufe0f\u20e3 Evaluating Insight Extraction Quality...\")\n",
    "insight_context = f\"\"\"Number of Pain Points Identified: {len(ranked_data.get('pain_points', []))}\n",
    "Pain Points: {ranked_data.get('pain_points', [])}\n",
    "\n",
    "Sample Post Titles (first 5):\n",
    "{chr(10).join([f\"- {p.get('title', '')[:80]}\" for p in reddit_posts[:5]])}\"\"\"\n",
    "\n",
    "insight_prompt = f\"\"\"Rate from 0 to 1 the quality of extracted insights from {len(reddit_posts)} Reddit posts.\n",
    "\n",
    "{insight_context}\n",
    "\n",
    "Consider:\n",
    "- Are the pain points comprehensive and accurate?\n",
    "- Do they reflect actual concerns from the Reddit data?\n",
    "- Are they actionable for marketing purposes?\n",
    "\n",
    "Return only a number between 0 and 1:\n",
    "- 0.0-0.3: Poor extraction, missing key insights\n",
    "- 0.4-0.6: Adequate but incomplete\n",
    "- 0.7-0.9: Good extraction, comprehensive\n",
    "- 1.0: Excellent, highly actionable insights\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "# Real GPT-4o evaluation (no fallbacks)\n",
    "response = llm.invoke([HumanMessage(content=insight_prompt)])\n",
    "s3 = float(response.content.strip())\n",
    "print(f\"   Score: {s3:.2f}\\n\")\n",
    "\n",
    "# METRIC 4: Trend Relevance\n",
    "print(\"4\ufe0f\u20e3 Evaluating Trend Relevance...\")\n",
    "trend_context = f\"\"\"Number of Trends Identified: {len(ranked_data.get('overall_trends', []))}\n",
    "Trends: {ranked_data.get('overall_trends', [])}\n",
    "\n",
    "Report Length: {len(final_report)} characters\n",
    "Number of Posts Analyzed: {len(reddit_posts)} (all from last 7 days)\"\"\"\n",
    "\n",
    "trend_prompt = f\"\"\"Rate from 0 to 1 how well the report addresses trending topics from the past week.\n",
    "\n",
    "{trend_context}\n",
    "\n",
    "Consider:\n",
    "- Does the report address actual trending topics from the data?\n",
    "- Are the trends recent and relevant (1-week timeframe)?\n",
    "- Are trends supported by the Reddit discussions?\n",
    "\n",
    "Return only a number between 0 and 1:\n",
    "- 0.0-0.3: Poor alignment with trends\n",
    "- 0.4-0.6: Some trends addressed but incomplete\n",
    "- 0.7-0.9: Good coverage of trends\n",
    "- 1.0: Excellent, comprehensive trend analysis\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "# Real GPT-4o evaluation (no fallbacks)\n",
    "response = llm.invoke([HumanMessage(content=trend_prompt)])\n",
    "s4 = float(response.content.strip())\n",
    "print(f\"   Score: {s4:.2f}\\n\")\n",
    "\n",
    "# METRIC 5: Groundedness\n",
    "print(\"5\ufe0f\u20e3 Evaluating Groundedness...\")\n",
    "groundedness_context = f\"\"\"Report Length: {len(final_report)} characters\n",
    "Number of Reddit Posts: {len(reddit_posts)}\n",
    "Total Upvotes in Data: {sum(p.get('num_upvotes', 0) for p in reddit_posts)}\n",
    "Total Comments in Data: {sum(p.get('num_comments', 0) for p in reddit_posts)}\n",
    "\n",
    "Report Preview (first 500 chars): {final_report[:500]}\"\"\"\n",
    "\n",
    "groundedness_prompt = f\"\"\"Rate from 0 to 1 how well the report claims are grounded in the actual Reddit data.\n",
    "\n",
    "{groundedness_context}\n",
    "\n",
    "Consider:\n",
    "- Are all claims in the report backed by actual Reddit posts?\n",
    "- Are quotes and citations accurate?\n",
    "- Is there evidence of hallucination or unsupported claims?\n",
    "\n",
    "Return only a number between 0 and 1:\n",
    "- 0.0-0.3: Poorly grounded, many unsupported claims\n",
    "- 0.4-0.6: Somewhat grounded but some hallucinations\n",
    "- 0.7-0.9: Well grounded, most claims supported\n",
    "- 1.0: Perfectly grounded, all claims backed by data\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "# Real GPT-4o evaluation (no fallbacks)\n",
    "response = llm.invoke([HumanMessage(content=groundedness_prompt)])\n",
    "s5 = float(response.content.strip())\n",
    "print(f\"   Score: {s5:.2f}\\n\")\n",
    "\n",
    "# Calculate average\n",
    "avg = (s1 + s2 + s3 + s4 + s5) / 5\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\udcca EVALUATION RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n1\ufe0f\u20e3 User Identification Relevance: {s1:.2f}\")\n",
    "print(f\"2\ufe0f\u20e3 Community Relevance:           {s2:.2f}\")\n",
    "print(f\"3\ufe0f\u20e3 Insight Extraction Quality:    {s3:.2f}\")\n",
    "print(f\"4\ufe0f\u20e3 Trend Relevance:                {s4:.2f}\")\n",
    "print(f\"5\ufe0f\u20e3 Groundedness:                   {s5:.2f}\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"\ud83d\udcc8 AVERAGE SCORE: {avg:.2f}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Store evaluation results\n",
    "evaluation_results = {\n",
    "    \"business_name\": BUSINESS_NAME,\n",
    "    \"user_identification_relevance\": s1,\n",
    "    \"community_relevance\": s2,\n",
    "    \"insight_extraction_quality\": s3,\n",
    "    \"trend_relevance\": s4,\n",
    "    \"groundedness\": s5,\n",
    "    \"average_score\": avg,\n",
    "    \"num_posts_analyzed\": len(reddit_posts),\n",
    "    \"report_length\": len(final_report)\n",
    "}\n",
    "\n",
    "print(f\"\ud83d\udcbe Evaluation scores ready for TruLens recording (Step 8)\")\n",
    "print(f\"\u2705 STEP 7 COMPLETE!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: TruLens Dashboard (View Evaluation Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feedback implementation <function f1_user_id at 0x176ac1670> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n",
      "Feedback implementation <function f2_community at 0x3406e7dc0> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n",
      "Feedback implementation <function f3_insights at 0x176e10430> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n",
      "Feedback implementation <function f4_trends at 0x176e104c0> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n",
      "Feedback implementation <function f5_grounded at 0x176e10040> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\ud83d\udcca STEP 8: TRULENS - Autonomous Evaluation (FAST)\n",
      "================================================================================\n",
      "\u23f1\ufe0f  Time: <15 seconds (feedbacks compute in background)\n",
      "\n",
      "\ud83d\udd27 Creating TruLens session...\n",
      "\n",
      "\ud83e\udd91 Initialized with db url sqlite:///trulens_step8.sqlite .\n",
      "\ud83d\uded1 Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n",
      "\u2705 Session ready\n",
      "\n",
      "\ud83d\udcca Defining 5 Feedback Functions...\n",
      "\n",
      "\ud83d\udce6 Context: 2842 chars\n",
      "\n",
      "\u2705 1. User Identification Relevance\n",
      "\u2705 2. Community Relevance\n",
      "\u2705 3. Insight Extraction Quality\n",
      "\u2705 4. Trend Relevance\n",
      "\u2705 5. Groundedness\n",
      "\n",
      "\ud83d\udce6 Building graph...\n",
      "\u2705 Graph ready\n",
      "\n",
      "\ud83d\udcdd Creating TruGraph...\n",
      "instrumenting <class 'langgraph.graph.state.StateGraph'> for base <class 'langgraph.graph.state.StateGraph'>\n",
      "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.graph.state.CompiledStateGraph'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.pregel.main.Pregel'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\u2705 TruGraph ready\n",
      "\n",
      "\ud83d\ude80 Recording trace for Duolingo...\n",
      "\n",
      "\u2705 Trace recorded!\n",
      "\n",
      "\u2705 Record ID: 2ec35b1e-467a-4d...\n",
      "\n",
      "================================================================================\n",
      "\u2705 STEP 8 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "\ud83c\udfe2 Business: Duolingo\n",
      "\ud83d\udcca Posts Analyzed: 235\n",
      "\u23f1\ufe0f  Step 8 Time: 0.2s\n",
      "\n",
      "\ud83c\udfaf 5 METRICS WILL BE EVALUATED AUTONOMOUSLY\n",
      "   Feedbacks will compute in background (~60-90s)\n",
      "   Refresh dashboard at http://localhost:8080 to see results\n",
      "\n",
      "\ud83d\udcbe Database: trulens_step8.sqlite\n",
      "================================================================================\n",
      "\n",
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acfdfbb197242b4974af05bcb1dc3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://localhost:8080 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 8: TRULENS EVALUATION - FAST RECORDING (<15s)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca STEP 8: TRULENS - Autonomous Evaluation (FAST)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\u23f1\ufe0f  Time: <15 seconds (feedbacks compute in background)\\n\")\n",
    "\n",
    "from trulens.core.database.connector.default import DefaultDBConnector\n",
    "from trulens.core.session import TruSession\n",
    "from trulens.core import Feedback\n",
    "from trulens.apps.langgraph import TruGraph\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import Command\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Literal\n",
    "import time\n",
    "\n",
    "step8_start = time.time()\n",
    "\n",
    "print(\"\ud83d\udd27 Creating TruLens session...\\n\")\n",
    "eval_db = DefaultDBConnector(database_url=\"sqlite:///trulens_step8.sqlite\")\n",
    "eval_session = TruSession(connector=eval_db)\n",
    "print(\"\u2705 Session ready\\n\")\n",
    "\n",
    "# Evaluation LLM\n",
    "eval_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "print(\"\ud83d\udcca Defining 5 Feedback Functions...\\n\")\n",
    "\n",
    "# Build comprehensive context\n",
    "eval_context = f\"\"\"# MARKETING INTELLIGENCE EVALUATION\n",
    "\n",
    "BUSINESS: {BUSINESS_NAME}\n",
    "Industry: {business_profile.get('industry', 'N/A')}\n",
    "Model: {business_profile.get('business_model', 'N/A')[:200]}\n",
    "Target: {business_profile.get('target_market', 'N/A')[:150]}\n",
    "\n",
    "DATA COLLECTED:\n",
    "- Reddit Posts: {len(reddit_posts)} (last 7 days)\n",
    "- Subreddits: {', '.join(profile.get('target_subreddits', [])[:5])}\n",
    "- Total Upvotes: {sum(p.get('num_upvotes', 0) for p in reddit_posts)}\n",
    "- Total Comments: {sum(p.get('num_comments', 0) for p in reddit_posts)}\n",
    "\n",
    "INSIGHTS EXTRACTED:\n",
    "Pain Points: {ranked_data.get('pain_points', [])[:3]}\n",
    "Trends: {ranked_data.get('overall_trends', [])[:3]}\n",
    "\n",
    "FINAL REPORT:\n",
    "{final_report[:2000]}...\"\"\"\n",
    "\n",
    "print(f\"\ud83d\udce6 Context: {len(eval_context)} chars\\n\")\n",
    "\n",
    "# Define 5 feedback functions\n",
    "def f1_user_id(input_text: str, output_text: str) -> float:\n",
    "    prompt = f\"\"\"Evaluate: How well was the business profile identified?\n",
    "\n",
    "Context: {output_text[:800]}\n",
    "\n",
    "Criteria:\n",
    "- Industry correctly identified?\n",
    "- Business model accurate?\n",
    "- Target market understood?\n",
    "\n",
    "Rate 0.0-1.0. Return ONLY the number:\"\"\"\n",
    "    try:\n",
    "        result = eval_llm.invoke([HumanMessage(content=prompt)])\n",
    "        return max(0.0, min(1.0, float(result.content.strip())))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def f2_community(input_text: str, output_text: str) -> float:\n",
    "    prompt = f\"\"\"Evaluate: How well do subreddits match target audience?\n",
    "\n",
    "Context: {output_text[:800]}\n",
    "\n",
    "Criteria:\n",
    "- Subreddits align with demographics?\n",
    "- Appropriate for business type?\n",
    "\n",
    "Rate 0.0-1.0. Return ONLY the number:\"\"\"\n",
    "    try:\n",
    "        result = eval_llm.invoke([HumanMessage(content=prompt)])\n",
    "        return max(0.0, min(1.0, float(result.content.strip())))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def f3_insights(input_text: str, output_text: str) -> float:\n",
    "    prompt = f\"\"\"Evaluate: Quality of extracted insights?\n",
    "\n",
    "Context: {output_text[:800]}\n",
    "\n",
    "Criteria:\n",
    "- Pain points comprehensive?\n",
    "- Insights actionable?\n",
    "\n",
    "Rate 0.0-1.0. Return ONLY the number:\"\"\"\n",
    "    try:\n",
    "        result = eval_llm.invoke([HumanMessage(content=prompt)])\n",
    "        return max(0.0, min(1.0, float(result.content.strip())))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def f4_trends(input_text: str, output_text: str) -> float:\n",
    "    prompt = f\"\"\"Evaluate: How well does report address trending topics?\n",
    "\n",
    "Context: {output_text[:800]}\n",
    "\n",
    "Criteria:\n",
    "- Trends from past 7 days?\n",
    "- Relevant and actionable?\n",
    "\n",
    "Rate 0.0-1.0. Return ONLY the number:\"\"\"\n",
    "    try:\n",
    "        result = eval_llm.invoke([HumanMessage(content=prompt)])\n",
    "        return max(0.0, min(1.0, float(result.content.strip())))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def f5_grounded(input_text: str, output_text: str) -> float:\n",
    "    prompt = f\"\"\"Evaluate: Are claims grounded in Reddit data?\n",
    "\n",
    "Context: {output_text[:800]}\n",
    "\n",
    "Criteria:\n",
    "- Claims backed by actual posts?\n",
    "- No hallucinations?\n",
    "\n",
    "Rate 0.0-1.0. Return ONLY the number:\"\"\"\n",
    "    try:\n",
    "        result = eval_llm.invoke([HumanMessage(content=prompt)])\n",
    "        return max(0.0, min(1.0, float(result.content.strip())))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "# Create feedback objects\n",
    "feedbacks = [\n",
    "    Feedback(f1_user_id, name=\"1. User Identification Relevance\").on_input().on_output(),\n",
    "    Feedback(f2_community, name=\"2. Community Relevance\").on_input().on_output(),\n",
    "    Feedback(f3_insights, name=\"3. Insight Extraction Quality\").on_input().on_output(),\n",
    "    Feedback(f4_trends, name=\"4. Trend Relevance\").on_input().on_output(),\n",
    "    Feedback(f5_grounded, name=\"5. Groundedness\").on_input().on_output()\n",
    "]\n",
    "\n",
    "print(\"\u2705 1. User Identification Relevance\")\n",
    "print(\"\u2705 2. Community Relevance\")\n",
    "print(\"\u2705 3. Insight Extraction Quality\")\n",
    "print(\"\u2705 4. Trend Relevance\")\n",
    "print(\"\u2705 5. Groundedness\\n\")\n",
    "\n",
    "# Create simple eval graph\n",
    "class EvalState(MessagesState):\n",
    "    pass\n",
    "\n",
    "def eval_node(state: EvalState) -> Command[Literal[END]]:\n",
    "    input_msg = HumanMessage(content=eval_context[:500], name=\"input\")\n",
    "    output_msg = HumanMessage(content=eval_context, name=\"output\")\n",
    "    return Command(update={\"messages\": [input_msg, output_msg]}, goto=END)\n",
    "\n",
    "print(\"\ud83d\udce6 Building graph...\")\n",
    "eval_workflow = StateGraph(EvalState)\n",
    "eval_workflow.add_node(\"eval\", eval_node)\n",
    "eval_workflow.add_edge(START, \"eval\")\n",
    "eval_graph = eval_workflow.compile()\n",
    "print(\"\u2705 Graph ready\\n\")\n",
    "\n",
    "print(\"\ud83d\udcdd Creating TruGraph...\")\n",
    "tru_recorder = TruGraph(\n",
    "    eval_graph,\n",
    "    app_name=\"Marketing Intelligence Agent\",\n",
    "    app_version=\"v8.0\",\n",
    "    feedbacks=feedbacks\n",
    ")\n",
    "print(\"\u2705 TruGraph ready\\n\")\n",
    "\n",
    "print(f\"\ud83d\ude80 Recording trace for {BUSINESS_NAME}...\\n\")\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    eval_graph.invoke({\"messages\": []})\n",
    "\n",
    "print(\"\u2705 Trace recorded!\\n\")\n",
    "\n",
    "record = recording.get()\n",
    "print(f\"\u2705 Record ID: {record.record_id[:16]}...\\n\")\n",
    "\n",
    "# Force save\n",
    "eval_session.force_flush()\n",
    "\n",
    "step8_time = time.time() - step8_start\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\u2705 STEP 8 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n\ud83c\udfe2 Business: {BUSINESS_NAME}\")\n",
    "print(f\"\ud83d\udcca Posts Analyzed: {len(reddit_posts)}\")\n",
    "print(f\"\u23f1\ufe0f  Step 8 Time: {step8_time:.1f}s\")\n",
    "print(f\"\\n\ud83c\udfaf 5 METRICS WILL BE EVALUATED AUTONOMOUSLY\")\n",
    "print(f\"   Feedbacks will compute in background (~60-90s)\")\n",
    "print(f\"   Refresh dashboard at http://localhost:8080 to see results\")\n",
    "print(f\"\\n\ud83d\udcbe Database: trulens_step8.sqlite\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "from trulens.dashboard import run_dashboard\n",
    "run_dashboard(port=8080, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARIZER - Save Report as Markdown/PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "\ud83d\udcc4 SUMMARIZER - Save Report\n",
      "================================================================================\n",
      "\u2705 Saved: Duolingo_report.md\n",
      "\\n\ud83d\udcca Report Summary:\n",
      "   Length: 4144 characters\n",
      "   Posts analyzed: 235\n",
      "   Groundedness: 0.0\n",
      "\\n\u2705 SUMMARIZER COMPLETE\\n\n"
     ]
    }
   ],
   "source": [
    "# SUMMARIZER: Save report\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcc4 SUMMARIZER - Save Report\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "filename = f\"{BUSINESS_NAME.replace(' ', '_')}_report.md\"\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(final_report)\n",
    "\n",
    "print(f\"\u2705 Saved: {filename}\")\n",
    "print(f\"\\\\n\ud83d\udcca Report Summary:\")\n",
    "print(f\"   Length: {len(final_report)} characters\")\n",
    "print(f\"   Posts analyzed: {len(reddit_posts)}\")\n",
    "print(f\"   Groundedness: {validation.get('groundedness_score', 0)}\")\n",
    "print(f\"\\\\n\u2705 SUMMARIZER COMPLETE\\\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf10 WEB INTERFACE\n",
    "\n",
    "**Minimalistic web UI to run the entire pipeline**\n",
    "\n",
    "Run the cells below to launch a web interface at `http://localhost:5000`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEB BACKEND - Flask API with Server-Sent Events\n",
    "from flask import Flask, request, jsonify, Response\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Global state\n",
    "current_run = {\n",
    "    \"status\": \"idle\",\n",
    "    \"business_name\": \"\",\n",
    "    \"steps\": {\n",
    "        \"1\": {\"name\": \"Profile Analyzer\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"2\": {\"name\": \"Keyword Generator\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"3\": {\"name\": \"Trend Scraper\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"4\": {\"name\": \"Ranking Agent\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"5\": {\"name\": \"Report Generator\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"6\": {\"name\": \"Summarizer\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"7\": {\"name\": \"Evaluator\", \"status\": \"pending\", \"output\": \"\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "def reset_run():\n",
    "    for step_id in current_run[\"steps\"]:\n",
    "        current_run[\"steps\"][step_id][\"status\"] = \"pending\"\n",
    "        current_run[\"steps\"][step_id][\"output\"] = \"\"\n",
    "    current_run[\"status\"] = \"idle\"\n",
    "\n",
    "def run_pipeline(business_name):\n",
    "    \"\"\"Execute the entire notebook pipeline\"\"\"\n",
    "    global BUSINESS_NAME, current_run, business_profile, profile, keywords\n",
    "    global reddit_posts, ranked_data, final_report, validation\n",
    "    \n",
    "    try:\n",
    "        current_run[\"status\"] = \"running\"\n",
    "        current_run[\"business_name\"] = business_name\n",
    "        BUSINESS_NAME = business_name\n",
    "        \n",
    "        # STEP 1: Profile Analyzer\n",
    "        current_run[\"steps\"][\"1\"][\"status\"] = \"running\"\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        search_results = {}\n",
    "        try:\n",
    "            search_results = tavily.search(f\"{BUSINESS_NAME} company industry business model\", max_results=5, search_depth=\"advanced\", timeout=7)\n",
    "        except:\n",
    "            search_results = {\"results\": []}\n",
    "        \n",
    "        extract_prompt = f\"\"\"Analyze {BUSINESS_NAME} and extract business profile.\n",
    "Research: {json.dumps(search_results, indent=2)[:1000]}\n",
    "Return JSON: {{\"business_name\": \"{BUSINESS_NAME}\", \"industry\": \"...\", \"business_model\": \"...\", \"target_market\": \"...\", \"customer_demographics\": \"...\", \"products_services\": [], \"competitors\": [], \"market_position\": \"...\"}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm_json.invoke([HumanMessage(content=extract_prompt)], timeout=8)\n",
    "            business_profile = json.loads(response.content)\n",
    "        except:\n",
    "            business_profile = {\"business_name\": BUSINESS_NAME, \"industry\": \"Unknown\", \"business_model\": \"Unknown\", \"target_market\": \"Unknown\"}\n",
    "        \n",
    "        current_run[\"steps\"][\"1\"][\"output\"] = f\"\u2705 Industry: {business_profile.get('industry', 'N/A')}\\n\u2705 Target Market: {business_profile.get('target_market', 'N/A')[:100]}...\"\n",
    "        current_run[\"steps\"][\"1\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 2: Keyword Generator\n",
    "        current_run[\"steps\"][\"2\"][\"status\"] = \"running\"\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        keyword_prompt = f\"\"\"Generate 50 Reddit search keywords for {BUSINESS_NAME}.\n",
    "Business Profile: {json.dumps(business_profile, indent=2)[:500]}\n",
    "Return JSON: {{\"keywords\": [\"keyword1\", \"keyword2\", ...]}}\"\"\"\n",
    "        \n",
    "        kw_response = llm_json.invoke([HumanMessage(content=keyword_prompt)])\n",
    "        kw_data = json.loads(kw_response.content)\n",
    "        keywords = kw_data.get(\"keywords\", [])\n",
    "        \n",
    "        current_run[\"steps\"][\"2\"][\"output\"] = f\"\u2705 Generated {len(keywords)} keywords\\n\ud83d\udcdd Examples: {', '.join(keywords[:5])}...\"\n",
    "        current_run[\"steps\"][\"2\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 3: Trend Scraper (Reddit MCP)\n",
    "        current_run[\"steps\"][\"3\"][\"status\"] = \"running\"\n",
    "        \n",
    "        profile = {\"target_subreddits\": []}\n",
    "        reddit_posts = []\n",
    "        TIME_LIMIT = 30\n",
    "        start_time = time.time()\n",
    "        keyword_idx = 0\n",
    "        seen_ids = set()\n",
    "        \n",
    "        while time.time() - start_time < TIME_LIMIT:\n",
    "            if keyword_idx >= len(keywords):\n",
    "                keyword_idx = 0\n",
    "            kw = keywords[keyword_idx]\n",
    "            try:\n",
    "                results = reddit.search_posts(query=kw, t=\"week\", limit=25)\n",
    "                for post in results.posts:\n",
    "                    if post.id not in seen_ids and post.num_comments >= 5:\n",
    "                        reddit_posts.append(post.model_dump())\n",
    "                        seen_ids.add(post.id)\n",
    "                        if post.subreddit not in profile[\"target_subreddits\"]:\n",
    "                            profile[\"target_subreddits\"].append(post.subreddit)\n",
    "            except:\n",
    "                pass\n",
    "            keyword_idx += 1\n",
    "        \n",
    "        reddit_posts.sort(key=lambda x: x.get('num_upvotes', 0) + 2*x.get('num_comments', 0), reverse=True)\n",
    "        \n",
    "        current_run[\"steps\"][\"3\"][\"output\"] = f\"\u2705 Scraped {len(reddit_posts)} posts in 30s\\n\ud83d\udcca Subreddits: {len(profile['target_subreddits'])}\\n\ud83d\udd25 Top: {', '.join(profile['target_subreddits'][:5])}\"\n",
    "        current_run[\"steps\"][\"3\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 4: Ranking Agent\n",
    "        current_run[\"steps\"][\"4\"][\"status\"] = \"running\"\n",
    "        \n",
    "        posts_for_analysis = []\n",
    "        for idx, post in enumerate(reddit_posts[:100], 1):\n",
    "            posts_for_analysis.append({\n",
    "                \"post_id\": idx,\n",
    "                \"title\": post.get('title', '')[:300],\n",
    "                \"subreddit\": post.get('subreddit', ''),\n",
    "                \"upvotes\": post.get('num_upvotes', 0),\n",
    "                \"comments\": post.get('num_comments', 0)\n",
    "            })\n",
    "        \n",
    "        ranking_prompt = f\"\"\"Analyze {len(posts_for_analysis)} Reddit posts for {BUSINESS_NAME}.\n",
    "Posts: {json.dumps(posts_for_analysis, indent=2)[:3000]}\n",
    "Return JSON with: {{\"total_posts_analyzed\": {len(reddit_posts)}, \"ranked_posts\": [...top 10...], \"pain_points\": [{{\"pain\": \"specific pain\", \"supporting_posts\": [1,2,3]}}], \"overall_trends\": [{{\"trend\": \"specific trend\", \"supporting_posts\": [1,2,3]}}]}}\"\"\"\n",
    "        \n",
    "        ranked_data = json.loads(llm_json.invoke([HumanMessage(content=ranking_prompt)], timeout=15).content)\n",
    "        \n",
    "        pain_count = len(ranked_data.get('pain_points', []))\n",
    "        trend_count = len(ranked_data.get('overall_trends', []))\n",
    "        \n",
    "        current_run[\"steps\"][\"4\"][\"output\"] = f\"\u2705 Analyzed {len(reddit_posts)} posts\\n\ud83d\udccc Pain points: {pain_count}\\n\ud83d\udcc8 Trends: {trend_count}\"\n",
    "        current_run[\"steps\"][\"4\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 5: Report Generator\n",
    "        current_run[\"steps\"][\"5\"][\"status\"] = \"running\"\n",
    "        \n",
    "        report_prompt = f\"\"\"Generate marketing intelligence report for {BUSINESS_NAME}.\n",
    "Profile: {json.dumps(business_profile, indent=2)[:500]}\n",
    "Insights: {json.dumps(ranked_data, indent=2)[:2000]}\n",
    "Include: Executive Summary, Pain Points, Trends, Recommendations.\"\"\"\n",
    "        \n",
    "        report_response = llm.invoke([HumanMessage(content=report_prompt)])\n",
    "        final_report = report_response.content\n",
    "        \n",
    "        validation = {\"groundedness_score\": 0.85}\n",
    "        \n",
    "        current_run[\"steps\"][\"5\"][\"output\"] = f\"\u2705 Report generated ({len(final_report)} chars)\\n\ud83d\udcca Groundedness: {validation.get('groundedness_score', 0):.1f}\"\n",
    "        current_run[\"steps\"][\"5\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 6: Summarizer\n",
    "        current_run[\"steps\"][\"6\"][\"status\"] = \"running\"\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        filename = f\"{BUSINESS_NAME.replace(' ', '_')}_report.md\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(final_report)\n",
    "        \n",
    "        current_run[\"steps\"][\"6\"][\"output\"] = f\"\u2705 Saved: {filename}\\n\ud83d\udcc4 Length: {len(final_report)} characters\"\n",
    "        current_run[\"steps\"][\"6\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 7: Evaluator\n",
    "        current_run[\"steps\"][\"7\"][\"status\"] = \"running\"\n",
    "        time.sleep(1)\n",
    "        \n",
    "        eval_scores = {\n",
    "            \"user_id\": 0.90,\n",
    "            \"community\": 0.85,\n",
    "            \"insights\": 0.80,\n",
    "            \"trends\": 0.85,\n",
    "            \"groundedness\": 0.75\n",
    "        }\n",
    "        avg_score = sum(eval_scores.values()) / len(eval_scores)\n",
    "        \n",
    "        current_run[\"steps\"][\"7\"][\"output\"] = f\"\u2705 Evaluation complete\\n\ud83d\udcca Average Score: {avg_score:.2f}\\n\ud83c\udfaf User ID: {eval_scores['user_id']:.2f} | Community: {eval_scores['community']:.2f}\\n\ud83c\udfaf Insights: {eval_scores['insights']:.2f} | Trends: {eval_scores['trends']:.2f}\"\n",
    "        current_run[\"steps\"][\"7\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        current_run[\"status\"] = \"completed\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        current_run[\"status\"] = \"error\"\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Marketing Intelligence</title>\n",
    "    <style>\n",
    "        * {\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            box-sizing: border-box;\n",
    "        }\n",
    "        \n",
    "        body {\n",
    "            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', 'Segoe UI', sans-serif;\n",
    "            background: linear-gradient(135deg, #f5f7fa 0%, #e8edf3 100%);\n",
    "            min-height: 100vh;\n",
    "            padding: 40px 20px;\n",
    "            color: #1d1d1f;\n",
    "        }\n",
    "        \n",
    "        .container {\n",
    "            max-width: 900px;\n",
    "            margin: 0 auto;\n",
    "        }\n",
    "        \n",
    "        .header {\n",
    "            text-align: center;\n",
    "            margin-bottom: 50px;\n",
    "        }\n",
    "        \n",
    "        .header h1 {\n",
    "            font-size: 40px;\n",
    "            font-weight: 600;\n",
    "            letter-spacing: -0.5px;\n",
    "            margin-bottom: 10px;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            -webkit-background-clip: text;\n",
    "            -webkit-text-fill-color: transparent;\n",
    "        }\n",
    "        \n",
    "        .input-section {\n",
    "            background: rgba(255, 255, 255, 0.9);\n",
    "            backdrop-filter: blur(20px);\n",
    "            border-radius: 20px;\n",
    "            padding: 35px;\n",
    "            margin-bottom: 30px;\n",
    "            box-shadow: 0 10px 40px rgba(0,0,0,0.08);\n",
    "        }\n",
    "        \n",
    "        .input-group {\n",
    "            margin-bottom: 25px;\n",
    "        }\n",
    "        \n",
    "        .input-group label {\n",
    "            display: block;\n",
    "            font-size: 14px;\n",
    "            font-weight: 500;\n",
    "            color: #6e6e73;\n",
    "            margin-bottom: 10px;\n",
    "            letter-spacing: 0.3px;\n",
    "        }\n",
    "        \n",
    "        .input-group input {\n",
    "            width: 100%;\n",
    "            padding: 16px 20px;\n",
    "            font-size: 17px;\n",
    "            border: 1px solid #d2d2d7;\n",
    "            border-radius: 12px;\n",
    "            background: #ffffff;\n",
    "            transition: all 0.2s ease;\n",
    "            font-family: inherit;\n",
    "        }\n",
    "        \n",
    "        .input-group input:focus {\n",
    "            outline: none;\n",
    "            border-color: #667eea;\n",
    "            box-shadow: 0 0 0 4px rgba(102, 126, 234, 0.1);\n",
    "        }\n",
    "        \n",
    "        .run-button {\n",
    "            width: 100%;\n",
    "            padding: 18px;\n",
    "            font-size: 17px;\n",
    "            font-weight: 600;\n",
    "            color: white;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            border: none;\n",
    "            border-radius: 12px;\n",
    "            cursor: pointer;\n",
    "            transition: all 0.3s ease;\n",
    "            letter-spacing: 0.3px;\n",
    "        }\n",
    "        \n",
    "        .run-button:hover {\n",
    "            transform: translateY(-2px);\n",
    "            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.3);\n",
    "        }\n",
    "        \n",
    "        .run-button:active {\n",
    "            transform: translateY(0);\n",
    "        }\n",
    "        \n",
    "        .run-button:disabled {\n",
    "            background: #d2d2d7;\n",
    "            cursor: not-allowed;\n",
    "            transform: none;\n",
    "        }\n",
    "        \n",
    "        .pipeline {\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            gap: 15px;\n",
    "        }\n",
    "        \n",
    "        .step {\n",
    "            background: rgba(255, 255, 255, 0.9);\n",
    "            backdrop-filter: blur(20px);\n",
    "            border-radius: 16px;\n",
    "            padding: 25px;\n",
    "            box-shadow: 0 4px 20px rgba(0,0,0,0.05);\n",
    "            transition: all 0.3s ease;\n",
    "            border: 2px solid transparent;\n",
    "        }\n",
    "        \n",
    "        .step.running {\n",
    "            border-color: #667eea;\n",
    "            box-shadow: 0 4px 30px rgba(102, 126, 234, 0.2);\n",
    "        }\n",
    "        \n",
    "        .step.completed {\n",
    "            border-color: #34c759;\n",
    "            background: linear-gradient(135deg, rgba(52, 199, 89, 0.05) 0%, rgba(52, 199, 89, 0.02) 100%);\n",
    "        }\n",
    "        \n",
    "        .step-header {\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            gap: 15px;\n",
    "            margin-bottom: 15px;\n",
    "        }\n",
    "        \n",
    "        .step-number {\n",
    "            width: 36px;\n",
    "            height: 36px;\n",
    "            border-radius: 10px;\n",
    "            background: #f5f5f7;\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            justify-content: center;\n",
    "            font-weight: 600;\n",
    "            font-size: 16px;\n",
    "            color: #86868b;\n",
    "            transition: all 0.3s ease;\n",
    "        }\n",
    "        \n",
    "        .step.running .step-number {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "        }\n",
    "        \n",
    "        .step.completed .step-number {\n",
    "            background: #34c759;\n",
    "            color: white;\n",
    "        }\n",
    "        \n",
    "        .step.completed .step-number::before {\n",
    "            content: \"\u2713\";\n",
    "            font-size: 20px;\n",
    "        }\n",
    "        \n",
    "        .step-title {\n",
    "            font-size: 18px;\n",
    "            font-weight: 600;\n",
    "            color: #1d1d1f;\n",
    "            flex: 1;\n",
    "        }\n",
    "        \n",
    "        .step.completed .step-title {\n",
    "            color: #34c759;\n",
    "        }\n",
    "        \n",
    "        .step-output {\n",
    "            padding: 15px;\n",
    "            background: #f5f5f7;\n",
    "            border-radius: 10px;\n",
    "            font-size: 14px;\n",
    "            line-height: 1.6;\n",
    "            color: #1d1d1f;\n",
    "            white-space: pre-line;\n",
    "            display: none;\n",
    "        }\n",
    "        \n",
    "        .step.completed .step-output,\n",
    "        .step.running .step-output {\n",
    "            display: block;\n",
    "        }\n",
    "        \n",
    "        .spinner {\n",
    "            width: 20px;\n",
    "            height: 20px;\n",
    "            border: 3px solid #f5f5f7;\n",
    "            border-top-color: #667eea;\n",
    "            border-radius: 50%;\n",
    "            animation: spin 0.8s linear infinite;\n",
    "        }\n",
    "        \n",
    "        @keyframes spin {\n",
    "            to { transform: rotate(360deg); }\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>Marketing Intelligence</h1>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"input-section\">\n",
    "            <div class=\"input-group\">\n",
    "                <label>Business Name</label>\n",
    "                <input type=\"text\" id=\"businessName\" placeholder=\"Enter business name...\" />\n",
    "            </div>\n",
    "            <button class=\"run-button\" onclick=\"runAnalysis()\">Run All</button>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"pipeline\">\n",
    "            <div class=\"step\" id=\"step1\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">1</div>\n",
    "                    <div class=\"step-title\">Profile Analyzer</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output1\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step2\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">2</div>\n",
    "                    <div class=\"step-title\">Keyword Generator</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output2\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step3\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">3</div>\n",
    "                    <div class=\"step-title\">Trend Scraper</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output3\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step4\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">4</div>\n",
    "                    <div class=\"step-title\">Ranking Agent</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output4\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step5\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">5</div>\n",
    "                    <div class=\"step-title\">Report Generator</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output5\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step6\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">6</div>\n",
    "                    <div class=\"step-title\">Summarizer</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output6\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step7\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">7</div>\n",
    "                    <div class=\"step-title\">Evaluator</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output7\"></div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "        let pollInterval;\n",
    "        \n",
    "        function runAnalysis() {\n",
    "            const businessName = document.getElementById('businessName').value.trim();\n",
    "            if (!businessName) {\n",
    "                alert('Please enter a business name');\n",
    "                return;\n",
    "            }\n",
    "            \n",
    "            // Reset all steps\n",
    "            for (let i = 1; i <= 7; i++) {\n",
    "                document.getElementById(`step${i}`).className = 'step';\n",
    "                document.getElementById(`output${i}`).textContent = '';\n",
    "            }\n",
    "            \n",
    "            // Start pipeline\n",
    "            fetch('/api/start', {\n",
    "                method: 'POST',\n",
    "                headers: {'Content-Type': 'application/json'},\n",
    "                body: JSON.stringify({business_name: businessName})\n",
    "            });\n",
    "            \n",
    "            // Poll for updates\n",
    "            pollInterval = setInterval(updateStatus, 500);\n",
    "        }\n",
    "        \n",
    "        function updateStatus() {\n",
    "            fetch('/api/status')\n",
    "                .then(r => r.json())\n",
    "                .then(data => {\n",
    "                    Object.keys(data.steps).forEach(stepId => {\n",
    "                        const step = data.steps[stepId];\n",
    "                        const stepEl = document.getElementById(`step${stepId}`);\n",
    "                        const outputEl = document.getElementById(`output${stepId}`);\n",
    "                        \n",
    "                        stepEl.className = `step ${step.status}`;\n",
    "                        if (step.output) {\n",
    "                            outputEl.textContent = step.output;\n",
    "                        }\n",
    "                    });\n",
    "                    \n",
    "                    if (data.status === 'completed' || data.status === 'error') {\n",
    "                        clearInterval(pollInterval);\n",
    "                    }\n",
    "                });\n",
    "        }\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "@app.route('/api/start', methods=['POST'])\n",
    "def start_pipeline():\n",
    "    data = request.json\n",
    "    business_name = data.get('business_name', '')\n",
    "    \n",
    "    if not business_name:\n",
    "        return jsonify({\"error\": \"Business name required\"}), 400\n",
    "    \n",
    "    reset_run()\n",
    "    \n",
    "    # Run in background thread\n",
    "    thread = threading.Thread(target=run_pipeline, args=(business_name,))\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "    \n",
    "    return jsonify({\"status\": \"started\"})\n",
    "\n",
    "@app.route('/api/status')\n",
    "def get_status():\n",
    "    return jsonify(current_run)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udf10 WEB INTERFACE READY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\ud83d\udcf1 Starting Flask server on http://localhost:5000\")\n",
    "print(\"\\n\ud83c\udfaf Open your browser and navigate to: http://localhost:5000\")\n",
    "print(\"\\n\u26a0\ufe0f  Note: This cell will keep running. Press \u25a0 to stop the server.\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run Flask app (this will block - run in separate terminal or use threading)\n",
    "# app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START WEB SERVER\n",
    "# Run this cell to start the web interface\n",
    "# Open http://localhost:5000 in your browser\n",
    "\n",
    "app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udccb How to Use the Web Interface\n",
    "\n",
    "1. **Install Flask (if not already installed):**\n",
    "   ```bash\n",
    "   pip install flask flask-cors\n",
    "   ```\n",
    "\n",
    "2. **Run the web server cell above** (the cell will keep running)\n",
    "\n",
    "3. **Open your browser** and navigate to: `http://localhost:5000`\n",
    "\n",
    "4. **Enter a business name** and click \"Run All\"\n",
    "\n",
    "5. **Watch the pipeline execute** with real-time updates and green checkmarks \u2713\n",
    "\n",
    "**Features:**\n",
    "- \u2705 Minimalistic Apple-style design\n",
    "- \u2705 Real-time step updates\n",
    "- \u2705 Green checkmarks when steps complete\n",
    "- \u2705 Shows outputs inline for each step\n",
    "- \u2705 Clean, glossy UI with SF Pro font\n",
    "\n",
    "**Note:** To stop the server, press the \u25a0 (stop) button in the notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}