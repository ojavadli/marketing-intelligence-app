{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marketing Intelligence Agent - LEAN VERSION\n",
    "\n",
    "**Simple working system - tested and working!**\n",
    "\n",
    "Just 4 cells to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ All packages installed:\n",
      "   - OpenAI, LangChain, Tavily\n",
      "   - LangGraph, TruLens\n",
      "   - Requests, Pydantic (for Reddit MCP)\n",
      "   - Pandas, OpenPyXL (for Excel export)\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages\n",
    "%pip install openai langchain langchain-openai tavily-python langgraph trulens trulens-apps-langgraph trulens-providers-openai requests pydantic pandas openpyxl -q\n",
    "print(\"‚úÖ All packages installed:\")\n",
    "print(\"   - OpenAI, LangChain, Tavily\")\n",
    "print(\"   - LangGraph, TruLens\")\n",
    "print(\"   - Requests, Pydantic (for Reddit MCP)\")\n",
    "print(\"   - Pandas, OpenPyXL (for Excel export)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set your API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "print(\"‚úÖ API keys configured!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ APIs + Reddit MCP initialized\n",
      "‚úÖ Reddit MCP: No API key needed, uses public endpoints\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "from typing import Dict, Any, List, Optional, Annotated, Literal\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from tavily import TavilyClient\n",
    "\n",
    "llm_json = ChatOpenAI(model='gpt-4o', temperature=0, model_kwargs={'response_format': {'type': 'json_object'}})\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0.1)\n",
    "tavily = TavilyClient(api_key=os.environ['TAVILY_API_KEY'])\n",
    "\n",
    "# ============================================================================\n",
    "# REDDIT MCP - Embedded directly in notebook for Reddit scraping\n",
    "# ============================================================================\n",
    "\n",
    "class RedditPost(BaseModel):\n",
    "    \"\"\"Single Reddit post record\"\"\"\n",
    "    title: str\n",
    "    subreddit: str\n",
    "    author: str\n",
    "    score: int\n",
    "    num_comments: int\n",
    "    created_utc: float\n",
    "    url: str\n",
    "    selftext: str = \"\"\n",
    "    permalink: str\n",
    "    id: str\n",
    "    is_self: bool\n",
    "    link_flair_text: Optional[str] = None\n",
    "\n",
    "class RedditPosts(BaseModel):\n",
    "    \"\"\"Collection of Reddit posts with metadata\"\"\"\n",
    "    request_url: str\n",
    "    items: list[RedditPost]\n",
    "    count: int\n",
    "    before: Optional[str] = None\n",
    "    after: Optional[str] = None\n",
    "\n",
    "class RedditTools:\n",
    "    \"\"\"Reddit API tools - uses public JSON endpoints, no API key required\"\"\"\n",
    "    \n",
    "    def _get_user_agent(self) -> str:\n",
    "        \"\"\"Rotate user agents to avoid blocking\"\"\"\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n",
    "        ]\n",
    "        return random.choice(user_agents)\n",
    "    \n",
    "    def search_posts(\n",
    "        self,\n",
    "        query: str,\n",
    "        subreddit: Optional[str] = None,\n",
    "        sort: Literal[\"relevance\", \"hot\", \"top\", \"new\", \"comments\"] = \"relevance\",\n",
    "        t: Literal[\"hour\", \"day\", \"week\", \"month\", \"year\", \"all\"] = \"week\",\n",
    "        limit: int = 25,\n",
    "        after: Optional[str] = None,\n",
    "        before: Optional[str] = None\n",
    "    ) -> RedditPosts:\n",
    "        \"\"\"\n",
    "        Search for posts across Reddit or within a specific subreddit.\n",
    "        Default time filter is 'week' (last 7 days).\n",
    "        \"\"\"\n",
    "        if subreddit:\n",
    "            url = f\"https://www.reddit.com/r/{subreddit}/search.json\"\n",
    "            params = {\"q\": query, \"restrict_sr\": \"true\"}\n",
    "        else:\n",
    "            url = \"https://www.reddit.com/search.json\"\n",
    "            params = {\"q\": query}\n",
    "        \n",
    "        params.update({\n",
    "            \"sort\": sort,\n",
    "            \"t\": t,\n",
    "            \"limit\": min(limit, 100),\n",
    "            \"raw_json\": 1\n",
    "        })\n",
    "        \n",
    "        if after:\n",
    "            params[\"after\"] = after\n",
    "        if before:\n",
    "            params[\"before\"] = before\n",
    "        \n",
    "        headers = {\"User-Agent\": self._get_user_agent()}\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        posts = [\n",
    "            child[\"data\"] for child in data[\"data\"][\"children\"]\n",
    "            if not child[\"data\"].get(\"stickied\", False)\n",
    "        ]\n",
    "        \n",
    "        post_items = []\n",
    "        for post in posts:\n",
    "            post_items.append(RedditPost(\n",
    "                title=post.get(\"title\", \"\"),\n",
    "                subreddit=post.get(\"subreddit\", \"\"),\n",
    "                author=post.get(\"author\", \"\"),\n",
    "                score=post.get(\"score\", 0),\n",
    "                num_comments=post.get(\"num_comments\", 0),\n",
    "                created_utc=post.get(\"created_utc\", 0),\n",
    "                url=post.get(\"url\", \"\"),\n",
    "                selftext=post.get(\"selftext\", \"\"),\n",
    "                permalink=f\"https://www.reddit.com{post.get('permalink', '')}\",\n",
    "                id=post.get(\"id\", \"\"),\n",
    "                is_self=post.get(\"is_self\", False),\n",
    "                link_flair_text=post.get(\"link_flair_text\")\n",
    "            ))\n",
    "        \n",
    "        return RedditPosts(\n",
    "            request_url=response.url,\n",
    "            items=post_items,\n",
    "            count=len(post_items),\n",
    "            before=data[\"data\"].get(\"before\"),\n",
    "            after=data[\"data\"].get(\"after\")\n",
    "        )\n",
    "\n",
    "# Initialize Reddit MCP\n",
    "reddit = RedditTools()\n",
    "\n",
    "print('‚úÖ APIs + Reddit MCP initialized')\n",
    "print('‚úÖ Reddit MCP: No API key needed, uses public endpoints')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE ALL 6 AGENTS (Modular Architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ State class defined\n"
     ]
    }
   ],
   "source": [
    "# Define State class\n",
    "class State(MessagesState):\n",
    "    business_name: Optional[str]\n",
    "    profile: Optional[Dict[str, Any]]\n",
    "    reddit_search_keywords: Optional[List[str]]\n",
    "    reddit_posts: Optional[List[Dict[str, Any]]]\n",
    "    ranked_data: Optional[Dict[str, Any]]\n",
    "    report: Optional[str]\n",
    "    validation: Optional[Dict[str, Any]]\n",
    "    final_report: Optional[str]\n",
    "    logs: Optional[List[str]]\n",
    "\n",
    "print(\"‚úÖ State class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent 4: Report Generator defined\n"
     ]
    }
   ],
   "source": [
    "# AGENT 4: Report Generator\n",
    "def report_generator_agent(state: State) -> Command[Literal[\"validator\"]]:\n",
    "    \"\"\"Generate comprehensive marketing intelligence report.\"\"\"\n",
    "    ranked_data = state.get(\"ranked_data\", {})\n",
    "    logs = state.get(\"logs\", [])\n",
    "    \n",
    "    logs.append(f\"[Report Generator] Creating intelligence report\")\n",
    "    \n",
    "    report_prompt = f\"\"\"Generate marketing intelligence report for {state.get('business_name')}.\n",
    "Profile: {json.dumps(state.get('profile'))}\n",
    "Ranked Data: {json.dumps(ranked_data)}\n",
    "\n",
    "Include: Executive Summary, Pain Points, Trends, Recommended Actions.\n",
    "Format as markdown with Reddit citations.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=report_prompt)])\n",
    "    report = response.content\n",
    "    \n",
    "    logs.append(f\"[Report Generator] Report generated ({len(report)} chars)\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\"report\": report, \"logs\": logs},\n",
    "        goto=\"validator\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Agent 4: Report Generator defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent 5: Validator defined\n"
     ]
    }
   ],
   "source": [
    "# AGENT 5: Validator\n",
    "def validator_agent(state: State) -> Command[Literal[\"summarizer\"]]:\n",
    "    \"\"\"Validate report groundedness.\"\"\"\n",
    "    report = state.get(\"report\", \"\")\n",
    "    reddit_posts = state.get(\"reddit_posts\", [])\n",
    "    logs = state.get(\"logs\", [])\n",
    "    \n",
    "    logs.append(f\"[Validator] Checking groundedness\")\n",
    "    \n",
    "    validation_prompt = f\"\"\"Validate this report against Reddit data.\n",
    "Report: {report}\n",
    "Reddit Posts: {json.dumps(reddit_posts, indent=2)}\n",
    "\n",
    "Return JSON: {{\"groundedness_score\": 0.95, \"validation_passed\": true, \"issues_found\": []}}\"\"\"\n",
    "    \n",
    "    response = llm_json.invoke([HumanMessage(content=validation_prompt)])\n",
    "    validation = json.loads(response.content)\n",
    "    \n",
    "    logs.append(f\"[Validator] Groundedness: {validation.get('groundedness_score', 0)}\")\n",
    "    logs.append(f\"[Validator] Status: {'PASSED' if validation.get('validation_passed') else 'FAILED'}\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\"validation\": validation, \"logs\": logs},\n",
    "        goto=\"summarizer\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Agent 5: Validator defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent 6: Summarizer defined\n"
     ]
    }
   ],
   "source": [
    "# AGENT 6: Summarizer  \n",
    "def summarizer_agent(state: State) -> Command[Literal[END]]:\n",
    "    \"\"\"Polish and finalize report.\"\"\"\n",
    "    report = state.get(\"report\", \"\")\n",
    "    validation = state.get(\"validation\", {})\n",
    "    logs = state.get(\"logs\", [])\n",
    "    \n",
    "    logs.append(f\"[Summarizer] Finalizing report\")\n",
    "    \n",
    "    # Add metadata footer\n",
    "    final_report = report + f\"\"\"\n",
    "\n",
    "---\n",
    "**Report Metadata**  \n",
    "- Business: {state.get('business_name')}\n",
    "- Reddit Posts Analyzed: {len(state.get('reddit_posts', []))}\n",
    "- Groundedness Score: {validation.get('groundedness_score', 0)}\n",
    "- Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "    \n",
    "    logs.append(f\"[Summarizer] Complete! Total logs: {len(logs)}\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\"final_report\": final_report, \"logs\": logs},\n",
    "        goto=END\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Agent 6: Summarizer defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP-BY-STEP EXECUTION (Separate Steps with Outputs)\n",
    "\n",
    "**üëâ Change business name below, then run each step to see output!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "üéØ MARKETING INTELLIGENCE ANALYSIS FOR: Amazon\n",
      "================================================================================\\n\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üëâ USER INPUT - Enter Your Business Name!\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "BUSINESS_NAME = \"Amazon\"  # üëà CHANGE THIS!\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"üéØ MARKETING INTELLIGENCE ANALYSIS FOR: {BUSINESS_NAME}\")\n",
    "print(f\"{'='*80}\\\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Profile Analyzer (Tavily Research)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "üì° STEP 1: PROFILE ANALYZER - Research Business\n",
      "================================================================================\n",
      "Using: Tavily + OpenAI GPT-4\n",
      "Max time: 20 seconds\\n\n",
      "üîç Researching with Tavily...\n",
      "‚úÖ Found 5 sources\\n\n",
      "ü§ñ Extracting business profile with OpenAI GPT-4...\n",
      "\\n================================================================================\n",
      "üìä STEP 1 OUTPUT - EXTRACTED BUSINESS PROFILE:\n",
      "================================================================================\n",
      "\\nüè¢ Business: Amazon.com, Inc.\n",
      "\\nüìà Industry: E-commerce, Cloud Computing, Digital Streaming, Artificial Intelligence\n",
      "\\nüíº Business Model: Amazon operates a diverse business model that includes e-commerce retail, third-party marketplace, subscription services (Amazon Prime), cloud computing services (Amazon Web Services), and digital content streaming. The company generates revenue through direct product sales, commissions on third-party sales, subscription fees, and service fees for AWS.\n",
      "\\nüéØ Target Market: Amazon's target market is broad and includes consumers of all ages, genders, and nationalities. The primary focus is on value-conscious consumers, millennials, urban residents, and tech-savvy individuals who prefer online shopping for convenience and fast delivery.\n",
      "\\nüë• Customer Demographics: {'age': 'Primarily 18-44 years old, with a significant portion aged 25-34', 'income': 'Varies widely, but generally includes middle to upper-middle-class consumers', 'interests': 'Tech-savvy, convenience-oriented, value-conscious, and frequent online shoppers'}\n",
      "\\nüõçÔ∏è Products/Services: E-commerce retail, Amazon Prime, Amazon Web Services (AWS)\n",
      "\\n‚öîÔ∏è Competitors: Walmart, Alibaba, Microsoft (for AWS)\n",
      "\\nüìä Market Position: Leader\n",
      "\\n================================================================================\n",
      "‚úÖ STEP 1 COMPLETE - Profile extracted for Step 2!\n",
      "================================================================================\\n\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Profile Analyzer - Research and EXTRACT business profile\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"üì° STEP 1: PROFILE ANALYZER - Research Business\")\n",
    "print(\"=\"*80)\n",
    "print(\"Using: Tavily + OpenAI GPT-4\")\n",
    "print(\"Max time: 20 seconds\\\\n\")\n",
    "\n",
    "# Research with Tavily\n",
    "print(\"üîç Researching with Tavily...\")\n",
    "search_results = tavily.search(f\"{BUSINESS_NAME} company industry business model target market customer demographics\", max_results=5, search_depth=\"advanced\")\n",
    "\n",
    "print(f\"‚úÖ Found {len(search_results.get('results', []))} sources\\\\n\")\n",
    "\n",
    "# Extract complete profile with OpenAI\n",
    "print(\"ü§ñ Extracting business profile with OpenAI GPT-4...\")\n",
    "extract_prompt = f\"\"\"Analyze {BUSINESS_NAME} and extract complete business profile.\n",
    "\n",
    "Research Data: {json.dumps(search_results, indent=2)}\n",
    "\n",
    "Extract and return JSON:\n",
    "{{\n",
    "  \"business_name\": \"official company name\",\n",
    "  \"industry\": \"specific industry sector\",\n",
    "  \"business_model\": \"how they make money\",\n",
    "  \"target_market\": \"who are their customers\",\n",
    "  \"customer_demographics\": \"age, income, interests of customers\",\n",
    "  \"products_services\": [\"product1\", \"product2\"],\n",
    "  \"competitors\": [\"competitor1\", \"competitor2\"],\n",
    "  \"market_position\": \"leader/challenger/niche\"\n",
    "}}\n",
    "\n",
    "Be specific and detailed based on research data.\"\"\"\n",
    "\n",
    "response = llm_json.invoke([HumanMessage(content=extract_prompt)])\n",
    "business_profile = json.loads(response.content)\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"üìä STEP 1 OUTPUT - EXTRACTED BUSINESS PROFILE:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\\\nüè¢ Business: {business_profile.get('business_name', 'N/A')}\")\n",
    "print(f\"\\\\nüìà Industry: {business_profile.get('industry', 'N/A')}\")\n",
    "print(f\"\\\\nüíº Business Model: {business_profile.get('business_model', 'N/A')}\")\n",
    "print(f\"\\\\nüéØ Target Market: {business_profile.get('target_market', 'N/A')}\")\n",
    "print(f\"\\\\nüë• Customer Demographics: {business_profile.get('customer_demographics', 'N/A')}\")\n",
    "print(f\"\\\\nüõçÔ∏è Products/Services: {', '.join(business_profile.get('products_services', [])[:3])}\")\n",
    "print(f\"\\\\n‚öîÔ∏è Competitors: {', '.join(business_profile.get('competitors', [])[:3])}\")\n",
    "print(f\"\\\\nüìä Market Position: {business_profile.get('market_position', 'N/A')}\")\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"‚úÖ STEP 1 COMPLETE - Profile extracted for Step 2!\")\n",
    "print(f\"{'='*80}\\\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Keyword Generator (OpenAI Creates Reddit Search Strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "ü§ñ STEP 2: KEYWORD GENERATOR - Create Reddit Search Strategy\n",
      "================================================================================\n",
      "Using: OpenAI GPT-4\\n\n",
      "\\n================================================================================\n",
      "üìä STEP 2 OUTPUT - Generated Reddit Search Strategy:\n",
      "================================================================================\n",
      "\\nüìà Industry: E-commerce, Cloud Computing, Digital Streaming, Artificial Intelligence\n",
      "\\nüéØ Target Audience: Value-conscious consumers, millennials, urban residents, and tech-savvy individuals who prefer online shopping for convenience and fast delivery.\n",
      "\\nüîç SEARCH KEYWORDS (49 generated):\n",
      "================================================================================\n",
      "\\nüìå Company-Specific (20):\n",
      "   1. \\\"Amazon complaints\\\"\n",
      "   2. \\\"Amazon issues\\\"\n",
      "   3. \\\"Amazon problems\\\"\n",
      "   4. \\\"Amazon quality concerns\\\"\n",
      "   5. \\\"Amazon pricing discussion\\\"\n",
      "   6. \\\"Amazon vs Walmart\\\"\n",
      "   7. \\\"Amazon vs Alibaba\\\"\n",
      "   8. \\\"Amazon vs Microsoft\\\"\n",
      "   9. \\\"Amazon vs Netflix\\\"\n",
      "   10. \\\"Amazon alternatives\\\"\n",
      "   11. \\\"Amazon reviews\\\"\n",
      "   12. \\\"Amazon customer service issues\\\"\n",
      "   13. \\\"Amazon delivery problems\\\"\n",
      "   14. \\\"Amazon Prime complaints\\\"\n",
      "   15. \\\"Amazon Web Services issues\\\"\n",
      "   16. \\\"Amazon Echo problems\\\"\n",
      "   17. \\\"Amazon Kindle issues\\\"\n",
      "   18. \\\"Amazon Music quality\\\"\n",
      "   19. \\\"Amazon Video streaming issues\\\"\n",
      "   20. \\\"Amazon Alexa concerns\\\"\n",
      "\\nüåê Industry-Wide (29):\n",
      "   1. \\\"E-commerce market trends\\\"\n",
      "   2. \\\"Cloud computing technology changes\\\"\n",
      "   3. \\\"Digital streaming industry trends\\\"\n",
      "   4. \\\"AI advancements in retail\\\"\n",
      "   5. \\\"Online shopping customer pain points\\\"\n",
      "   6. \\\"Sustainability in e-commerce\\\"\n",
      "   7. \\\"Future of cloud services\\\"\n",
      "   8. \\\"Digital content consumption trends\\\"\n",
      "   9. \\\"Impact of AI on shopping\\\"\n",
      "   10. \\\"Consumer behavior in e-commerce\\\"\n",
      "   11. \\\"E-commerce best practices\\\"\n",
      "   12. \\\"Cloud service provider comparisons\\\"\n",
      "   13. \\\"Streaming service quality issues\\\"\n",
      "   14. \\\"Smart speaker technology trends\\\"\n",
      "   15. \\\"E-reader market analysis\\\"\n",
      "   16. \\\"Music streaming service comparisons\\\"\n",
      "   17. \\\"Video streaming best practices\\\"\n",
      "   18. \\\"Online retail customer experience\\\"\n",
      "   19. \\\"Subscription service value\\\"\n",
      "   20. \\\"Tech-savvy consumer preferences\\\"\n",
      "   21. \\\"Urban shopping habits\\\"\n",
      "   22. \\\"E-commerce delivery innovations\\\"\n",
      "   23. \\\"Cloud computing security concerns\\\"\n",
      "   24. \\\"Digital streaming content variety\\\"\n",
      "   25. \\\"AI in customer service\\\"\n",
      "   26. \\\"Voice assistant technology issues\\\"\n",
      "   27. \\\"E-commerce platform comparisons\\\"\n",
      "   28. \\\"Cloud service cost analysis\\\"\n",
      "   29. \\\"Streaming service user experience\\\"\n",
      "\\nüì± Target Subreddits (10):\n",
      "   1. r/amazon\n",
      "   2. r/ecommerce\n",
      "   3. r/cloudcomputing\n",
      "   4. r/digitalstreaming\n",
      "   5. r/artificial\n",
      "   6. r/technology\n",
      "   7. r/shopping\n",
      "   8. r/technews\n",
      "   9. r/consumeradvice\n",
      "   10. r/streaming\n",
      "\\n================================================================================\n",
      "‚úÖ STEP 2 COMPLETE - 49 keywords ready for Reddit search!\n",
      "================================================================================\\n\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Generate Reddit search keywords with OpenAI\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ STEP 2: KEYWORD GENERATOR - Create Reddit Search Strategy\")\n",
    "print(\"=\"*80)\n",
    "print(\"Using: OpenAI GPT-4\\\\n\")\n",
    "\n",
    "profile_prompt = f'''Generate COMPREHENSIVE Reddit search strategy for {BUSINESS_NAME}.\n",
    "\n",
    "Business Profile: {json.dumps(business_profile, indent=2)}\n",
    "\n",
    "You MUST generate EXACTLY 50 SPECIFIC search keywords. Create many variations to maximize Reddit coverage.\n",
    "\n",
    "Categories (generate variations for each):\n",
    "1. Company-specific (20 keywords): complaints, issues, problems, quality, pricing, vs competitors, alternatives, reviews\n",
    "2. Industry trends (15 keywords): market trends, technology changes, customer pain points (NO company name)\n",
    "3. Product category (15 keywords): product type issues, best practices, comparisons (NO company name)\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"industry\": \"...\",\n",
    "  \"target_audience\": \"...\",\n",
    "  \"reddit_search_keywords\": [\"kw1\", \"kw2\", \"kw3\", ... \"kw50\"],\n",
    "  \"target_subreddits\": [\"r/sub1\", \"r/sub2\", ...]\n",
    "}}\n",
    "\n",
    "Generate ALL 50 keywords. Be creative with variations.'''\n",
    "\n",
    "response = llm_json.invoke([HumanMessage(content=profile_prompt)])\n",
    "profile = json.loads(response.content)\n",
    "\n",
    "keywords = profile.get('reddit_search_keywords', [])\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"üìä STEP 2 OUTPUT - Generated Reddit Search Strategy:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\\\nüìà Industry: {profile.get('industry', 'N/A')}\")\n",
    "print(f\"\\\\nüéØ Target Audience: {profile.get('target_audience', 'N/A')}\")\n",
    "\n",
    "print(f\"\\\\nüîç SEARCH KEYWORDS ({len(keywords)} generated):\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Categorize and display keywords\n",
    "company_kw = [k for k in keywords if BUSINESS_NAME.lower() in k.lower()]\n",
    "industry_kw = [k for k in keywords if BUSINESS_NAME.lower() not in k.lower()]\n",
    "\n",
    "if company_kw:\n",
    "    print(f\"\\\\nüìå Company-Specific ({len(company_kw)}):\")\n",
    "    for i, kw in enumerate(company_kw, 1):\n",
    "        print(f\"   {i}. \\\\\\\"{kw}\\\\\\\"\")\n",
    "\n",
    "if industry_kw:\n",
    "    print(f\"\\\\nüåê Industry-Wide ({len(industry_kw)}):\")\n",
    "    for i, kw in enumerate(industry_kw, 1):\n",
    "        print(f\"   {i}. \\\\\\\"{kw}\\\\\\\"\")\n",
    "\n",
    "print(f\"\\\\nüì± Target Subreddits ({len(profile.get('target_subreddits', []))}):\")\n",
    "for i, sub in enumerate(profile.get('target_subreddits', []), 1):\n",
    "    print(f\"   {i}. {sub}\")\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"‚úÖ STEP 2 COMPLETE - {len(keywords)} keywords ready for Reddit search!\")\n",
    "print(f\"{'='*80}\\\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# STEP 3: TREND EXTRACTOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì± STEP 3: TREND SCRAPER - Reddit MCP\n",
      "================================================================================\n",
      "Using: Reddit MCP (Public API, No Key Needed!)\n",
      "Time filter: WEEK (last 7 days only)\n",
      "‚è±Ô∏è  HARD TIME LIMIT: 30 seconds (no more, no less!)\n",
      "\n",
      "üîç Maximizing scraping in 30 seconds...\n",
      "\n",
      "   [1] 'Amazon complaints...' (30.0s left) ‚úÖ +10\n",
      "   [2] 'Amazon issues...' (29.5s left) ‚úÖ +10\n",
      "   [3] 'Amazon problems...' (29.0s left) ‚úÖ +8\n",
      "   [4] 'Amazon quality concerns...' (28.6s left) ‚úÖ +8\n",
      "   [5] 'Amazon pricing discussion...' (27.9s left) ‚úÖ +8\n",
      "   [6] 'Amazon vs Walmart...' (27.2s left) ‚úÖ +1\n",
      "   [7] 'Amazon vs Alibaba...' (26.8s left) ‚úÖ +10\n",
      "   [8] 'Amazon vs Microsoft...' (26.4s left) ‚úÖ +7\n",
      "   [9] 'Amazon vs Netflix...' (25.8s left) ‚úÖ +9\n",
      "   [10] 'Amazon alternatives...' (25.4s left) ‚úÖ +6\n",
      "   [11] 'Amazon reviews...' (24.8s left) ‚úÖ +10\n",
      "   [12] 'Amazon customer service issues...' (24.3s left) ‚úÖ +9\n",
      "   [13] 'Amazon delivery problems...' (23.7s left) ‚úÖ +8\n",
      "   [14] 'Amazon Prime complaints...' (23.0s left) ‚úÖ +6\n",
      "   [15] 'Amazon Web Services issues...' (22.5s left) ‚úÖ +8\n",
      "   [16] 'Amazon Echo problems...' (21.9s left) ‚úÖ +6\n",
      "   [17] 'Amazon Kindle issues...' (21.4s left) ‚úÖ +7\n",
      "   [18] 'Amazon Music quality...' (20.8s left) ‚úÖ +10\n",
      "   [19] 'Amazon Video streaming issues...' (20.2s left) ‚úÖ +5\n",
      "   [20] 'Amazon Alexa concerns...' (19.7s left) ‚úÖ +8\n",
      "   [21] 'E-commerce market trends...' (19.2s left) ‚úÖ +8\n",
      "   [22] 'Cloud computing technology changes...' (18.7s left) ‚úÖ +9\n",
      "   [23] 'Digital streaming industry trends...' (18.1s left) ‚úÖ +5\n",
      "   [24] 'AI advancements in retail...' (17.5s left) ‚úÖ +9\n",
      "   [25] 'Online shopping customer pain points...' (17.0s left) ‚úÖ +5\n",
      "   [26] 'Sustainability in e-commerce...' (16.3s left) ‚úÖ +10\n",
      "   [27] 'Future of cloud services...' (15.8s left) ‚úÖ +9\n",
      "   [28] 'Digital content consumption trends...' (15.3s left) ‚úÖ +1\n",
      "   [29] 'Impact of AI on shopping...' (14.7s left) ‚úÖ +8\n",
      "   [30] 'Consumer behavior in e-commerce...' (13.9s left) ‚úÖ +1\n",
      "   [31] 'E-commerce best practices...' (13.5s left) ‚úÖ +2\n",
      "   [32] 'Cloud service provider comparisons...' (12.8s left) ‚úÖ +4\n",
      "   [33] 'Streaming service quality issues...' (12.2s left) ‚úÖ +6\n",
      "   [34] 'Smart speaker technology trends...' (11.5s left) ‚úÖ +5\n",
      "   [35] 'E-reader market analysis...' (11.0s left) ‚úÖ +6\n",
      "   [36] 'Music streaming service comparisons...' (10.3s left) ‚úÖ +4\n",
      "   [37] 'Video streaming best practices...' (9.6s left) ‚úÖ +10\n",
      "   [38] 'Online retail customer experience...' (8.7s left) ‚úÖ +4\n",
      "   [39] 'Subscription service value...' (8.2s left) ‚úÖ +10\n",
      "   [40] 'Tech-savvy consumer preferences...' (7.1s left) ‚úÖ +6\n",
      "   [41] 'Urban shopping habits...' (6.6s left) ‚úÖ +7\n",
      "   [42] 'E-commerce delivery innovations...' (6.2s left) ‚úÖ +3\n",
      "   [43] 'Cloud computing security concerns...' (5.6s left) ‚úÖ +9\n",
      "   [44] 'Digital streaming content variety...' (5.1s left) ‚úÖ +2\n",
      "   [45] 'AI in customer service...' (4.4s left) ‚úÖ +2\n",
      "   [46] 'Voice assistant technology issues...' (3.9s left) ‚úÖ +5\n",
      "   [47] 'E-commerce platform comparisons...' (3.0s left) ‚úÖ +2\n",
      "   [48] 'Cloud service cost analysis...' (2.4s left) ‚úÖ +8\n",
      "   [49] 'Streaming service user experience...' (1.7s left) ‚úÖ +6\n",
      "   [50] 'Amazon complaints...' (1.2s left) ‚úÖ +10\n",
      "   [51] 'Amazon issues...' (0.8s left) ‚úÖ +10\n",
      "   [52] 'Amazon problems...' (0.4s left) ‚úÖ +8\n",
      "   [53] 'Amazon quality concerns...' (0.1s left) ‚úÖ +8\n",
      "\n",
      "üßπ Deduplicating...\n",
      "\n",
      "================================================================================\n",
      "üìä STEP 3 RESULTS:\n",
      "================================================================================\n",
      "‚úÖ Scraped: 279 high-quality posts\n",
      "‚è±Ô∏è  Time: 30.49 seconds (limit: 30s)\n",
      "üîç Keywords searched: 53\n",
      "üìÖ Timeframe: Last 7 days (1 week)\n",
      "üéØ Min engagement: 5+ comments per post\n",
      "\n",
      "üìà Engagement Stats:\n",
      "   Total upvotes: 53,256\n",
      "   Total comments: 16,217\n",
      "   Avg upvotes/post: 190\n",
      "   Avg comments/post: 58\n",
      "\n",
      "üìå Top 5 Posts by Engagement:\n",
      "\n",
      "   1. \"Meta just lost $200 billion in one week. Zuckerberg spent 3 ...\"\n",
      "      r/ArtificialInteligence\n",
      "      5,213‚¨ÜÔ∏è  630üí¨  Engagement: 6,473\n",
      "\n",
      "   2. \"What was the most frugal thing you did this week? I'll start...\"\n",
      "      r/Frugal\n",
      "      2,076‚¨ÜÔ∏è  976üí¨  Engagement: 4,028\n",
      "\n",
      "   3. \"Corporate Media, \"Everything good for the Working Class is b...\"\n",
      "      r/WorkReform\n",
      "      3,849‚¨ÜÔ∏è  79üí¨  Engagement: 4,007\n",
      "\n",
      "   4. \"Told my wife (F35) that she couldn‚Äôt do it without me (M34)....\"\n",
      "      r/BORUpdates\n",
      "      2,872‚¨ÜÔ∏è  452üí¨  Engagement: 3,776\n",
      "\n",
      "   5. \"Dev Talk | New Reveals...\"\n",
      "      r/ZenlessZoneZero\n",
      "      2,383‚¨ÜÔ∏è  225üí¨  Engagement: 2,833\n",
      "\n",
      "üìÇ Subreddit Coverage (242 unique):\n",
      "   r/AmazonVine: 4 posts\n",
      "   r/alexa: 4 posts\n",
      "   r/ArtificialInteligence: 3 posts\n",
      "   r/LegalAdviceUK: 3 posts\n",
      "   r/amazonprime: 3 posts\n",
      "\n",
      "================================================================================\n",
      "‚úÖ STEP 3 COMPLETE - Data ready for Ranking Agent!\n",
      "================================================================================\n",
      "\n",
      "üîÑ Next: Ranking Agent will analyze these 279 posts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: REDDIT MCP SCRAPER - 30 SECOND HARD LIMIT\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì± STEP 3: TREND SCRAPER - Reddit MCP\")\n",
    "print(\"=\"*80)\n",
    "print(\"Using: Reddit MCP (Public API, No Key Needed!)\")\n",
    "print(\"Time filter: WEEK (last 7 days only)\")\n",
    "print(\"‚è±Ô∏è  HARD TIME LIMIT: 30 seconds (no more, no less!)\\n\")\n",
    "\n",
    "TIME_LIMIT = 30  # Hard 30-second limit\n",
    "reddit_posts = []\n",
    "keywords = profile.get('reddit_search_keywords', [])  # Use ALL keywords\n",
    "start_time = time.time()\n",
    "keywords_searched = 0\n",
    "\n",
    "print(f\"üîç Maximizing scraping in {TIME_LIMIT} seconds...\\n\")\n",
    "\n",
    "# Keep looping through keywords until we hit 30 seconds\n",
    "keyword_index = 0\n",
    "while True:\n",
    "    # Check time BEFORE each operation\n",
    "    elapsed = time.time() - start_time\n",
    "    if elapsed >= TIME_LIMIT:\n",
    "        print(f\"\\n‚è±Ô∏è  30 seconds reached - stopping scraping\")\n",
    "        break\n",
    "    \n",
    "    # Cycle through keywords (loop back to start if we run out)\n",
    "    keyword = keywords[keyword_index % len(keywords)]\n",
    "    keyword_index += 1\n",
    "    keywords_searched += 1\n",
    "    \n",
    "    remaining = TIME_LIMIT - elapsed\n",
    "    print(f\"   [{keywords_searched}] '{keyword[:40]}...' ({remaining:.1f}s left) \", end=\"\", flush=True)\n",
    "    \n",
    "    try:\n",
    "        # Search Reddit using MCP (1-week filter)\n",
    "        results = reddit.search_posts(\n",
    "            query=keyword,\n",
    "            t=\"week\",  # Last 7 days ONLY\n",
    "            limit=10\n",
    "        )\n",
    "        \n",
    "        # Process posts quickly - add all with 5+ comments\n",
    "        new_posts = 0\n",
    "        for post in results.items:\n",
    "            if post.num_comments >= 5:  # High engagement filter\n",
    "                reddit_posts.append({\n",
    "                    \"title\": post.title,\n",
    "                    \"subreddit\": post.subreddit,\n",
    "                    \"author\": post.author,\n",
    "                    \"score\": post.score,\n",
    "                    \"num_upvotes\": post.score,\n",
    "                    \"num_comments\": post.num_comments,\n",
    "                    \"created_utc\": post.created_utc,\n",
    "                    \"url\": post.url,\n",
    "                    \"selftext\": post.selftext[:1000] if post.selftext else \"\",\n",
    "                    \"permalink\": post.permalink,\n",
    "                    \"id\": post.id,\n",
    "                    \"link_flair_text\": post.link_flair_text\n",
    "                })\n",
    "                new_posts += 1\n",
    "        \n",
    "        print(f\"‚úÖ +{new_posts}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è error\")\n",
    "    \n",
    "    # Check if we've exceeded time limit (safety check)\n",
    "    if time.time() - start_time >= TIME_LIMIT:\n",
    "        break\n",
    "\n",
    "# Final elapsed time\n",
    "final_elapsed = time.time() - start_time\n",
    "\n",
    "# Remove duplicates by post ID\n",
    "print(f\"\\nüßπ Deduplicating...\")\n",
    "seen_ids = set()\n",
    "unique_posts = []\n",
    "for post in reddit_posts:\n",
    "    if post['id'] not in seen_ids:\n",
    "        seen_ids.add(post['id'])\n",
    "        unique_posts.append(post)\n",
    "\n",
    "reddit_posts = unique_posts\n",
    "\n",
    "# Sort by engagement (score + 2*comments) to prioritize discussion\n",
    "reddit_posts.sort(key=lambda p: p['num_upvotes'] + (2 * p['num_comments']), reverse=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìä STEP 3 RESULTS:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚úÖ Scraped: {len(reddit_posts)} high-quality posts\")\n",
    "print(f\"‚è±Ô∏è  Time: {final_elapsed:.2f} seconds (limit: {TIME_LIMIT}s)\")\n",
    "print(f\"üîç Keywords searched: {keywords_searched}\")\n",
    "print(f\"üìÖ Timeframe: Last 7 days (1 week)\")\n",
    "print(f\"üéØ Min engagement: 5+ comments per post\")\n",
    "\n",
    "if len(reddit_posts) > 0:\n",
    "    total_upvotes = sum(p['num_upvotes'] for p in reddit_posts)\n",
    "    total_comments = sum(p['num_comments'] for p in reddit_posts)\n",
    "    \n",
    "    print(f\"\\nüìà Engagement Stats:\")\n",
    "    print(f\"   Total upvotes: {total_upvotes:,}\")\n",
    "    print(f\"   Total comments: {total_comments:,}\")\n",
    "    print(f\"   Avg upvotes/post: {total_upvotes//len(reddit_posts):,}\")\n",
    "    print(f\"   Avg comments/post: {total_comments//len(reddit_posts):,}\")\n",
    "    \n",
    "    print(f\"\\nüìå Top 5 Posts by Engagement:\")\n",
    "    for i, post in enumerate(reddit_posts[:5], 1):\n",
    "        engagement = post['num_upvotes'] + (2 * post['num_comments'])\n",
    "        print(f\"\\n   {i}. \\\"{post['title'][:60]}...\\\"\")\n",
    "        print(f\"      r/{post['subreddit']}\")\n",
    "        print(f\"      {post['num_upvotes']:,}‚¨ÜÔ∏è  {post['num_comments']:,}üí¨  Engagement: {engagement:,}\")\n",
    "    \n",
    "    # Count subreddits represented\n",
    "    subreddit_counts = {}\n",
    "    for post in reddit_posts:\n",
    "        sub = post['subreddit']\n",
    "        subreddit_counts[sub] = subreddit_counts.get(sub, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìÇ Subreddit Coverage ({len(subreddit_counts)} unique):\")\n",
    "    for sub, count in sorted(subreddit_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"   r/{sub}: {count} posts\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úÖ STEP 3 COMPLETE - Data ready for Ranking Agent!\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: No posts found in {TIME_LIMIT} seconds!\")\n",
    "    print(f\"   Try broader keywords or check subreddit names\\n\")\n",
    "\n",
    "print(f\"üîÑ Next: Ranking Agent will analyze these {len(reddit_posts)} posts\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3B: Export URLs to Excel for Manual Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä STEP 3B: EXPORT TO EXCEL FOR VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "üì• Exporting 279 posts to Excel...\n",
      "\n",
      "‚úÖ Excel file created: Amazon_Reddit_URLs.xlsx\n",
      "\n",
      "üìã File contents:\n",
      "   Rows: 279\n",
      "   Columns: 13 (Row, Title, Subreddit, Full_URL, Upvotes, Comments,\n",
      "            Engagement_Score, Posted_Date, Days_Ago, Author, Post_ID,\n",
      "            Has_Text, Text_Preview)\n",
      "\n",
      "üì• Download 'Amazon_Reddit_URLs.xlsx' to verify URLs manually!\n",
      "   All 279 Reddit permalinks are in the 'Full_URL' column\n",
      "\n",
      "================================================================================\n",
      "‚úÖ STEP 3B COMPLETE - Excel file ready for download!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Export all Reddit posts to Excel for manual verification\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä STEP 3B: EXPORT TO EXCEL FOR VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(reddit_posts) > 0:\n",
    "    print(f\"\\nüì• Exporting {len(reddit_posts)} posts to Excel...\\n\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime as dt\n",
    "    \n",
    "    # Prepare data for Excel\n",
    "    excel_data = []\n",
    "    for i, post in enumerate(reddit_posts, 1):\n",
    "        post_date = dt.fromtimestamp(post['created_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        days_ago = (dt.now() - dt.fromtimestamp(post['created_utc'])).days\n",
    "        \n",
    "        excel_data.append({\n",
    "            'Row': i,\n",
    "            'Title': post['title'],\n",
    "            'Subreddit': f\"r/{post['subreddit']}\",\n",
    "            'Full_URL': post['permalink'],\n",
    "            'Upvotes': post['num_upvotes'],\n",
    "            'Comments': post['num_comments'],\n",
    "            'Engagement_Score': post['num_upvotes'] + (2 * post['num_comments']),\n",
    "            'Posted_Date': post_date,\n",
    "            'Days_Ago': days_ago,\n",
    "            'Author': f\"u/{post['author']}\",\n",
    "            'Post_ID': post['id'],\n",
    "            'Has_Text': 'Yes' if post.get('selftext') else 'No',\n",
    "            'Text_Preview': post.get('selftext', '')[:200]\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(excel_data)\n",
    "    \n",
    "    # Save to Excel\n",
    "    excel_filename = f\"{BUSINESS_NAME.replace(' ', '_')}_Reddit_URLs.xlsx\"\n",
    "    df.to_excel(excel_filename, index=False, sheet_name='Reddit Posts', engine='openpyxl')\n",
    "    \n",
    "    print(f\"‚úÖ Excel file created: {excel_filename}\")\n",
    "    print(f\"\\nüìã File contents:\")\n",
    "    print(f\"   Rows: {len(reddit_posts)}\")\n",
    "    print(f\"   Columns: 13 (Row, Title, Subreddit, Full_URL, Upvotes, Comments,\")\n",
    "    print(f\"            Engagement_Score, Posted_Date, Days_Ago, Author, Post_ID,\")\n",
    "    print(f\"            Has_Text, Text_Preview)\")\n",
    "    \n",
    "    print(f\"\\nüì• Download '{excel_filename}' to verify URLs manually!\")\n",
    "    print(f\"   All {len(reddit_posts)} Reddit permalinks are in the 'Full_URL' column\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úÖ STEP 3B COMPLETE - Excel file ready for download!\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No posts to export (Step 3 returned 0 posts)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Ranking Agent (OpenAI Ranks Insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä STEP 4: RANKING AGENT\n",
      "================================================================================\n",
      "üìä Analyzing 279 posts (max 15s)...\n",
      "\n",
      "‚ö†Ô∏è Timeout after 15s - using basic analysis\n",
      "\n",
      "‚úÖ STEP 4 DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: RANKING AGENT - Extract SPECIFIC, DETAILED insights (MAX 15s)\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä STEP 4: RANKING AGENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not reddit_posts:\n",
    "    print(\"‚ö†Ô∏è No posts to rank\")\n",
    "    ranked_data = {}\n",
    "else:\n",
    "    print(f\"üìä Analyzing {len(reddit_posts)} posts (max 15s)...\\n\")\n",
    "    \n",
    "    start_step4 = time.time()\n",
    "    \n",
    "    # Include post IDs for citation tracking\n",
    "    posts_for_analysis = []\n",
    "    for idx, post in enumerate(reddit_posts[:100], 1):\n",
    "        posts_for_analysis.append({\n",
    "            \"post_id\": idx,\n",
    "            \"title\": post.get('title', '')[:300],\n",
    "            \"subreddit\": post.get('subreddit', ''),\n",
    "            \"url\": post.get('url', ''),\n",
    "            \"upvotes\": post.get('num_upvotes', 0),\n",
    "            \"comments\": post.get('num_comments', 0)\n",
    "        })\n",
    "    \n",
    "    ranking_prompt = f\"\"\"Analyze {len(posts_for_analysis)} Reddit posts for {BUSINESS_NAME}.\n",
    "\n",
    "Business: {BUSINESS_NAME}\n",
    "Industry: {profile.get('industry', 'N/A')}\n",
    "Target Market: {profile.get('target_market', 'N/A')[:200]}\n",
    "\n",
    "Reddit Posts:\n",
    "{json.dumps(posts_for_analysis, indent=2)}\n",
    "\n",
    "Extract JSON with SPECIFIC, DETAILED insights:\n",
    "{{\n",
    "  \"total_posts_analyzed\": {len(reddit_posts)},\n",
    "  \"ranked_posts\": [\n",
    "    {{\"post_id\": 1, \"title\": \"...\", \"subreddit\": \"...\", \"relevance_score\": 0.95, \"key_insight\": \"specific insight\"}},\n",
    "    ... (top 10)\n",
    "  ],\n",
    "  \"pain_points\": [\n",
    "    {{\n",
    "      \"pain\": \"HIGHLY SPECIFIC pain point with numbers/details (e.g., 'Users losing 3-5 hours daily due to energy system')\",\n",
    "      \"supporting_posts\": [1, 3, 5],\n",
    "      \"severity\": \"high/medium/low\"\n",
    "    }},\n",
    "    ... (5-10 pain points, each with SPECIFIC details and post citations)\n",
    "  ],\n",
    "  \"overall_trends\": [\n",
    "    {{\n",
    "      \"trend\": \"SPECIFIC trend with timeframe and context (e.g., 'Over past 7 days, 15+ posts discussing migration to LibreLingo after $3 price increase')\",\n",
    "      \"supporting_posts\": [2, 4, 7, 9],\n",
    "      \"momentum\": \"rising/stable/declining\"\n",
    "    }},\n",
    "    ... (5-10 trends, each with SPECIFIC details, examples, and post citations)\n",
    "  ],\n",
    "  \"sentiment_summary\": \"overall sentiment with specifics\",\n",
    "  \"subreddit_breakdown\": {{\"r/sub1\": \"specific insight\", \"r/sub2\": \"specific insight\"}}\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Pain points MUST be HIGHLY SPECIFIC with numbers, examples, details\n",
    "2. Trends MUST include timeframe, scale, and actionable context\n",
    "3. EVERY pain/trend MUST cite supporting_posts (list of post IDs)\n",
    "4. Include severity/momentum indicators\n",
    "5. NO generic statements - only specific, detailed insights\"\"\"\n",
    "\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(\n",
    "                lambda: json.loads(llm_json.invoke([HumanMessage(content=ranking_prompt)]).content)\n",
    "            )\n",
    "            ranked_data = future.result(timeout=15)\n",
    "        \n",
    "        step4_time = time.time() - start_step4\n",
    "        \n",
    "        print(f\"‚úÖ Analysis complete ({step4_time:.1f}s):\")\n",
    "        print(f\"   Total posts: {ranked_data.get('total_posts_analyzed', 0)}\")\n",
    "        print(f\"   Top ranked: {len(ranked_data.get('ranked_posts', []))}\")\n",
    "        print(f\"   Pain points: {len(ranked_data.get('pain_points', []))}\")\n",
    "        print(f\"   Trends: {len(ranked_data.get('overall_trends', []))}\")\n",
    "        \n",
    "        # Show detailed pain points with citations\n",
    "        if ranked_data.get('pain_points'):\n",
    "            print(f\"\\nüìå Top Pain Points (with citations):\")\n",
    "            for idx, pain_obj in enumerate(ranked_data.get('pain_points', [])[:5], 1):\n",
    "                if isinstance(pain_obj, dict):\n",
    "                    pain_text = pain_obj.get('pain', str(pain_obj))\n",
    "                    posts = pain_obj.get('supporting_posts', [])\n",
    "                    print(f\"   {idx}. {pain_text}\")\n",
    "                    print(f\"      (Posts: {posts})\")\n",
    "                else:\n",
    "                    print(f\"   {idx}. {pain_obj}\")\n",
    "    \n",
    "    except concurrent.futures.TimeoutError:\n",
    "        print(f\"‚ö†Ô∏è Timeout after 15s - using basic analysis\")\n",
    "        ranked_data = {\n",
    "            \"total_posts_analyzed\": len(reddit_posts),\n",
    "            \"ranked_posts\": [{\"post_id\": i+1, \"title\": p.get('title', ''), \"subreddit\": p.get('subreddit', ''), \"relevance_score\": 0.8} for i, p in enumerate(reddit_posts[:10])],\n",
    "            \"pain_points\": [{\"pain\": \"Analysis timed out - rerun for insights\", \"supporting_posts\": []}],\n",
    "            \"overall_trends\": [{\"trend\": \"Analysis timed out\", \"supporting_posts\": []}],\n",
    "            \"sentiment_summary\": \"Unknown\"\n",
    "        }\n",
    "\n",
    "print(\"\\n‚úÖ STEP 4 DONE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Report Generator (OpenAI Creates Final Report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìù STEP 5: REPORT GENERATOR - Create Final Intelligence Report\n",
      "================================================================================\n",
      "Using: OpenAI GPT-4 (with CITATION TRACKING)\n",
      "\n",
      "\n",
      "‚úÖ Report generated (3830 characters)\n",
      "‚úÖ Groundedness: 0.8\n",
      "‚úÖ Citation Diversity: 0.7\n",
      "   (High diversity = good grounding)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä STEP 5 OUTPUT - FINAL INTELLIGENCE REPORT\n",
      "================================================================================\n",
      "\n",
      "# Amazon Marketing Intelligence Report\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "1. **Consumer Trust Issues with Amazon Purchases**\n",
      "   There is a growing concern among consumers regarding the quality and reliability of products purchased from Amazon. This is particularly evident in the tech sector, where users have reported issues with high-value items like MacBook Pros.\n",
      "   - [Post #7: r/macbookpro](https://www.reddit.com/r/macbookpro/comments/1opktpk/warning_do_not_buy_a_macbook_pro_from_amazon/)\n",
      "\n",
      "2. **Frugality ...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ STEP 5 DONE - Report ready for PDF/evaluation\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: REPORT GENERATOR - Create grounded report with diverse citations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù STEP 5: REPORT GENERATOR - Create Final Intelligence Report\")\n",
    "print(\"=\"*80)  \n",
    "print(\"Using: OpenAI GPT-4 (with CITATION TRACKING)\\n\")\n",
    "\n",
    "if len(reddit_posts) == 0:\n",
    "    print(\"‚ö†Ô∏è WARNING: No Reddit data!\")\n",
    "    final_report = f\"# Marketing Intelligence Report for {BUSINESS_NAME}\\n\\nNo Reddit data available.\"\n",
    "else:\n",
    "    # Build post lookup with IDs\n",
    "    post_lookup = {}\n",
    "    for idx, post in enumerate(reddit_posts[:100], 1):\n",
    "        post_lookup[idx] = {\n",
    "            \"id\": idx,\n",
    "            \"title\": post.get('title', ''),\n",
    "            \"subreddit\": post.get('subreddit', ''),\n",
    "            \"url\": post.get('url', ''),\n",
    "            \"upvotes\": post.get('num_upvotes', 0),\n",
    "            \"comments\": post.get('num_comments', 0)\n",
    "        }\n",
    "\n",
    "    report_prompt = f\"\"\"Generate marketing intelligence report for {BUSINESS_NAME}.\n",
    "\n",
    "Business Profile:\n",
    "{json.dumps(profile, indent=2)[:800]}\n",
    "\n",
    "Insights with Post Citations:\n",
    "{json.dumps(ranked_data, indent=2)[:3000]}\n",
    "\n",
    "Post Lookup (for citations):\n",
    "{json.dumps(post_lookup, indent=2)[:2000]}\n",
    "\n",
    "CRITICAL CITATION REQUIREMENTS:\n",
    "1. For EACH pain point: Use the supporting_posts IDs to cite specific posts\n",
    "2. For EACH trend: Use the supporting_posts IDs to cite multiple posts\n",
    "3. Format: [Post #X: r/subreddit](URL)\n",
    "4. DIVERSE citations - don't cite same post repeatedly\n",
    "5. If insight has supporting_posts [1,3,5], cite ALL of them\n",
    "6. NO claims without citations\n",
    "\n",
    "Report Structure:\n",
    "1. Executive Summary (3 insights, EACH citing different posts)\n",
    "2. Pain Points (EACH with citations from supporting_posts)\n",
    "3. Trending Topics (EACH with multiple citations from supporting_posts)\n",
    "4. Recommended Actions (based on cited insights)\n",
    "5. Top Discussions (show Post IDs, URLs, quotes)\n",
    "\n",
    "Example Pain Point Format:\n",
    "- **[Specific Pain Point with Details]**\n",
    "  Users report [specific issue with numbers/context].\n",
    "  - [Post #1: r/subreddit1](URL1)\n",
    "  - [Post #3: r/subreddit2](URL2)\n",
    "  - [Post #5: r/subreddit3](URL3)\n",
    "\n",
    "Example Trend Format:\n",
    "- **[Specific Trend with Timeframe and Scale]**\n",
    "  Over the past [timeframe], [specific observation with numbers].\n",
    "  - [Post #2: r/subreddit](URL)\n",
    "  - [Post #4: r/subreddit](URL)\n",
    "  - [Post #7: r/subreddit](URL)\n",
    "\n",
    "Format as markdown. GROUND EVERY CLAIM with DIVERSE, SPECIFIC citations.\"\"\"\n",
    "\n",
    "    report_response = llm.invoke([HumanMessage(content=report_prompt)])\n",
    "    report = report_response.content\n",
    "\n",
    "    # Validate citation diversity\n",
    "    validation_prompt = f\"\"\"Validate report citations:\n",
    "Report: {report[:2000]}\n",
    "Post Lookup: {json.dumps(post_lookup, indent=2)[:1000]}\n",
    "\n",
    "Check:\n",
    "1. Does EVERY claim have citations?\n",
    "2. Are citations DIVERSE (not same post repeatedly)?\n",
    "3. Are citations accurate (Post IDs match URLs)?\n",
    "\n",
    "Return JSON: {{\"groundedness_score\": 0.0-1.0, \"citation_diversity\": 0.0-1.0, \"validation_passed\": true/false}}\"\"\"\n",
    "\n",
    "    validation = json.loads(llm_json.invoke([HumanMessage(content=validation_prompt)]).content)\n",
    "\n",
    "    # Final report with metadata\n",
    "    final_report = report + f\"\"\"\n",
    "\n",
    "---\n",
    "**Report Metadata**\n",
    "- Business: {BUSINESS_NAME}\n",
    "- Reddit Posts Analyzed: {len(reddit_posts)}\n",
    "- Groundedness: {validation.get('groundedness_score', 0):.1f}\n",
    "- Citation Diversity: {validation.get('citation_diversity', 0):.1f}\n",
    "- Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"\\n‚úÖ Report generated ({len(report)} characters)\")\n",
    "    print(f\"‚úÖ Groundedness: {validation.get('groundedness_score', 0):.1f}\")\n",
    "    print(f\"‚úÖ Citation Diversity: {validation.get('citation_diversity', 0):.1f}\")\n",
    "    print(\"   (High diversity = good grounding)\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä STEP 5 OUTPUT - FINAL INTELLIGENCE REPORT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(final_report[:500] + \"...\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ STEP 5 DONE - Report ready for PDF/evaluation\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summarizer - Generate PDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìÑ STEP 6: SUMMARIZER - Auto-Generate PDF Report\n",
      "================================================================================\n",
      "\n",
      "‚úÖ PDF AUTO-GENERATED:\n",
      "   üìÑ File: Amazon_Report.pdf\n",
      "   üìä Posts: 279\n",
      "   üìù Length: 3980 chars\n",
      "   üíæ Size: 5.1 KB\n",
      "\n",
      "‚úÖ PDF ready at: Amazon_Report.pdf\n",
      "\n",
      "‚úÖ STEP 6 COMPLETE!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: SUMMARIZER - Auto-Generate PDF\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÑ STEP 6: SUMMARIZER - Auto-Generate PDF Report\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install reportlab -q\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "\n",
    "# Create PDF\n",
    "filename = f\"{BUSINESS_NAME.replace(' ', '_')}_Report.pdf\"\n",
    "doc = SimpleDocTemplate(filename, pagesize=letter)\n",
    "styles = getSampleStyleSheet()\n",
    "story = []\n",
    "\n",
    "# Title\n",
    "title = Paragraph(f\"<b>Marketing Intelligence Report: {BUSINESS_NAME}</b>\", styles['Title'])\n",
    "story.append(title)\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "# Metadata\n",
    "meta_text = f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br/>Posts Analyzed: {len(reddit_posts)}<br/>Groundedness: {validation.get('groundedness_score', 'N/A') if 'validation' in dir() else 'N/A'}\"\n",
    "meta = Paragraph(meta_text, styles['Normal'])\n",
    "story.append(meta)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Report content - split into paragraphs\n",
    "for line in final_report.split('\\n'):\n",
    "    if line.strip():\n",
    "        # Clean line\n",
    "        line = line.replace('#', '').strip()\n",
    "        if len(line) > 3:\n",
    "            try:\n",
    "                p = Paragraph(line, styles['Normal'])\n",
    "                story.append(p)\n",
    "                story.append(Spacer(1, 6))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Build PDF\n",
    "doc.build(story)\n",
    "\n",
    "print(f\"\\n‚úÖ PDF AUTO-GENERATED:\")\n",
    "print(f\"   üìÑ File: {filename}\")\n",
    "print(f\"   üìä Posts: {len(reddit_posts)}\")\n",
    "print(f\"   üìù Length: {len(final_report)} chars\")\n",
    "\n",
    "import os\n",
    "if os.path.exists(filename):\n",
    "    size_kb = os.path.getsize(filename) / 1024\n",
    "    print(f\"   üíæ Size: {size_kb:.1f} KB\")\n",
    "    print(f\"\\n‚úÖ PDF ready at: {filename}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è PDF not found\")\n",
    "\n",
    "print(f\"\\n‚úÖ STEP 6 COMPLETE!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: Validator - Verify report groundedness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä STEP 7: EVALUATION - 5 LLM Judges (TruLens)\n",
      "================================================================================\n",
      "üîß Initializing TruLens...\n",
      "\n",
      "ü¶ë Initialized with db url sqlite:///marketing_intel_evaluation.sqlite .\n",
      "üõë Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n",
      "‚úÖ TruLens initialized with gpt-4o provider\n",
      "\n",
      "ü§ñ Running 5 LLM Judge Evaluations...\n",
      "\n",
      "1Ô∏è‚É£ Evaluating User Identification Relevance...\n",
      "   Score: 1.00\n",
      "\n",
      "2Ô∏è‚É£ Evaluating Community Relevance...\n",
      "   Score: 0.80\n",
      "\n",
      "3Ô∏è‚É£ Evaluating Insight Extraction Quality...\n",
      "   Score: 0.20\n",
      "\n",
      "4Ô∏è‚É£ Evaluating Trend Relevance...\n",
      "   Score: 0.00\n",
      "\n",
      "5Ô∏è‚É£ Evaluating Groundedness...\n",
      "   Score: 0.60\n",
      "\n",
      "================================================================================\n",
      "üìä EVALUATION RESULTS:\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ User Identification Relevance: 1.00\n",
      "2Ô∏è‚É£ Community Relevance:           0.80\n",
      "3Ô∏è‚É£ Insight Extraction Quality:    0.20\n",
      "4Ô∏è‚É£ Trend Relevance:                0.00\n",
      "5Ô∏è‚É£ Groundedness:                   0.60\n",
      "\n",
      "================================================================================\n",
      "üìà AVERAGE SCORE: 0.52\n",
      "================================================================================\n",
      "\n",
      "üíæ Evaluation scores ready for TruLens recording (Step 8)\n",
      "‚úÖ STEP 7 COMPLETE!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: EVALUATION - 5 LLM Judges with TruLens\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä STEP 7: EVALUATION - 5 LLM Judges (TruLens)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize TruLens\n",
    "from trulens.core.database.connector.default import DefaultDBConnector\n",
    "from trulens.core.session import TruSession\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "print(\"üîß Initializing TruLens...\\n\")\n",
    "\n",
    "# Create TruLens session with SQLite database\n",
    "connector = DefaultDBConnector(database_url=\"sqlite:///marketing_intel_evaluation.sqlite\")\n",
    "session = TruSession(connector=connector)\n",
    "# session.reset_database()  # Don't reset - would delete data\n",
    "\n",
    "# Initialize OpenAI provider for LLM-as-judge (using TruLens provider)\n",
    "eval_provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "\n",
    "print(\"‚úÖ TruLens initialized with gpt-4o provider\\n\")\n",
    "print(\"ü§ñ Running 5 LLM Judge Evaluations...\\n\")\n",
    "\n",
    "# METRIC 1: User Identification Relevance\n",
    "print(\"1Ô∏è‚É£ Evaluating User Identification Relevance...\")\n",
    "user_id_context = f\"\"\"Business Name: {BUSINESS_NAME}\n",
    "Identified Industry: {business_profile.get('industry', 'N/A')}\n",
    "Business Model: {business_profile.get('business_model', 'N/A')}\n",
    "Target Market: {business_profile.get('target_market', 'N/A')}\n",
    "Market Position: {business_profile.get('market_position', 'N/A')}\"\"\"\n",
    "\n",
    "user_id_prompt = f\"\"\"Rate from 0 to 1 how well the profile analyzer identified the business's industry, professional activity, and market position.\n",
    "\n",
    "{user_id_context}\n",
    "\n",
    "Return only a number between 0 and 1, where:\n",
    "- 0.0-0.3: Poor identification, missing key details\n",
    "- 0.4-0.6: Adequate but incomplete\n",
    "- 0.7-0.9: Good identification with most details\n",
    "- 1.0: Excellent, comprehensive identification\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "# Real GPT-4o evaluation (no fallbacks)\n",
    "response = llm.invoke([HumanMessage(content=user_id_prompt)])\n",
    "s1 = float(response.content.strip())\n",
    "print(f\"   Score: {s1:.2f}\\n\")\n",
    "\n",
    "# METRIC 2: Community Relevance\n",
    "print(\"2Ô∏è‚É£ Evaluating Community Relevance...\")\n",
    "community_context = f\"\"\"Target Market: {business_profile.get('target_market', 'N/A')}\n",
    "Customer Demographics: {business_profile.get('customer_demographics', 'N/A')}\n",
    "Target Subreddits: {', '.join(profile.get('target_subreddits', [])[:10])}\"\"\"\n",
    "\n",
    "community_prompt = f\"\"\"Rate from 0 to 1 how well the discovered subreddits match the target audience description.\n",
    "\n",
    "{community_context}\n",
    "\n",
    "Consider:\n",
    "- Do the subreddits align with the target market?\n",
    "- Are they relevant to the customer demographics?\n",
    "- Would these communities have meaningful discussions about this business?\n",
    "\n",
    "Return only a number between 0 and 1:\n",
    "- 0.0-0.3: Poor match, irrelevant communities\n",
    "- 0.4-0.6: Some relevance but misaligned\n",
    "- 0.7-0.9: Good match, mostly relevant\n",
    "- 1.0: Excellent match, perfectly aligned\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "# Real GPT-4o evaluation (no fallbacks)\n",
    "response = llm.invoke([HumanMessage(content=community_prompt)])\n",
    "s2 = float(response.content.strip())\n",
    "print(f\"   Score: {s2:.2f}\\n\")\n",
    "\n",
    "# METRIC 3: Insight Extraction Quality\n",
    "print(\"3Ô∏è‚É£ Evaluating Insight Extraction Quality...\")\n",
    "insight_context = f\"\"\"Number of Pain Points Identified: {len(ranked_data.get('pain_points', []))}\n",
    "Pain Points: {ranked_data.get('pain_points', [])}\n",
    "\n",
    "Sample Post Titles (first 5):\n",
    "{chr(10).join([f\"- {p.get('title', '')[:80]}\" for p in reddit_posts[:5]])}\"\"\"\n",
    "\n",
    "insight_prompt = f\"\"\"Rate from 0 to 1 the quality of extracted insights from {len(reddit_posts)} Reddit posts.\n",
    "\n",
    "{insight_context}\n",
    "\n",
    "Consider:\n",
    "- Are the pain points comprehensive and accurate?\n",
    "- Do they reflect actual concerns from the Reddit data?\n",
    "- Are they actionable for marketing purposes?\n",
    "\n",
    "Return only a number between 0 and 1:\n",
    "- 0.0-0.3: Poor extraction, missing key insights\n",
    "- 0.4-0.6: Adequate but incomplete\n",
    "- 0.7-0.9: Good extraction, comprehensive\n",
    "- 1.0: Excellent, highly actionable insights\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "# Real GPT-4o evaluation (no fallbacks)\n",
    "response = llm.invoke([HumanMessage(content=insight_prompt)])\n",
    "s3 = float(response.content.strip())\n",
    "print(f\"   Score: {s3:.2f}\\n\")\n",
    "\n",
    "# METRIC 4: Trend Relevance\n",
    "print(\"4Ô∏è‚É£ Evaluating Trend Relevance...\")\n",
    "trend_context = f\"\"\"Number of Trends Identified: {len(ranked_data.get('overall_trends', []))}\n",
    "Trends: {ranked_data.get('overall_trends', [])}\n",
    "\n",
    "Report Length: {len(final_report)} characters\n",
    "Number of Posts Analyzed: {len(reddit_posts)} (all from last 7 days)\"\"\"\n",
    "\n",
    "trend_prompt = f\"\"\"Rate from 0 to 1 how well the report addresses trending topics from the past week.\n",
    "\n",
    "{trend_context}\n",
    "\n",
    "Consider:\n",
    "- Does the report address actual trending topics from the data?\n",
    "- Are the trends recent and relevant (1-week timeframe)?\n",
    "- Are trends supported by the Reddit discussions?\n",
    "\n",
    "Return only a number between 0 and 1:\n",
    "- 0.0-0.3: Poor alignment with trends\n",
    "- 0.4-0.6: Some trends addressed but incomplete\n",
    "- 0.7-0.9: Good coverage of trends\n",
    "- 1.0: Excellent, comprehensive trend analysis\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "# Real GPT-4o evaluation (no fallbacks)\n",
    "response = llm.invoke([HumanMessage(content=trend_prompt)])\n",
    "s4 = float(response.content.strip())\n",
    "print(f\"   Score: {s4:.2f}\\n\")\n",
    "\n",
    "# METRIC 5: Groundedness\n",
    "print(\"5Ô∏è‚É£ Evaluating Groundedness...\")\n",
    "groundedness_context = f\"\"\"Report Length: {len(final_report)} characters\n",
    "Number of Reddit Posts: {len(reddit_posts)}\n",
    "Total Upvotes in Data: {sum(p.get('num_upvotes', 0) for p in reddit_posts)}\n",
    "Total Comments in Data: {sum(p.get('num_comments', 0) for p in reddit_posts)}\n",
    "\n",
    "Report Preview (first 500 chars): {final_report[:500]}\"\"\"\n",
    "\n",
    "groundedness_prompt = f\"\"\"Rate from 0 to 1 how well the report claims are grounded in the actual Reddit data.\n",
    "\n",
    "{groundedness_context}\n",
    "\n",
    "Consider:\n",
    "- Are all claims in the report backed by actual Reddit posts?\n",
    "- Are quotes and citations accurate?\n",
    "- Is there evidence of hallucination or unsupported claims?\n",
    "\n",
    "Return only a number between 0 and 1:\n",
    "- 0.0-0.3: Poorly grounded, many unsupported claims\n",
    "- 0.4-0.6: Somewhat grounded but some hallucinations\n",
    "- 0.7-0.9: Well grounded, most claims supported\n",
    "- 1.0: Perfectly grounded, all claims backed by data\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "# Real GPT-4o evaluation (no fallbacks)\n",
    "response = llm.invoke([HumanMessage(content=groundedness_prompt)])\n",
    "s5 = float(response.content.strip())\n",
    "print(f\"   Score: {s5:.2f}\\n\")\n",
    "\n",
    "# Calculate average\n",
    "avg = (s1 + s2 + s3 + s4 + s5) / 5\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"üìä EVALUATION RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n1Ô∏è‚É£ User Identification Relevance: {s1:.2f}\")\n",
    "print(f\"2Ô∏è‚É£ Community Relevance:           {s2:.2f}\")\n",
    "print(f\"3Ô∏è‚É£ Insight Extraction Quality:    {s3:.2f}\")\n",
    "print(f\"4Ô∏è‚É£ Trend Relevance:                {s4:.2f}\")\n",
    "print(f\"5Ô∏è‚É£ Groundedness:                   {s5:.2f}\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìà AVERAGE SCORE: {avg:.2f}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Store evaluation results\n",
    "evaluation_results = {\n",
    "    \"business_name\": BUSINESS_NAME,\n",
    "    \"user_identification_relevance\": s1,\n",
    "    \"community_relevance\": s2,\n",
    "    \"insight_extraction_quality\": s3,\n",
    "    \"trend_relevance\": s4,\n",
    "    \"groundedness\": s5,\n",
    "    \"average_score\": avg,\n",
    "    \"num_posts_analyzed\": len(reddit_posts),\n",
    "    \"report_length\": len(final_report)\n",
    "}\n",
    "\n",
    "print(f\"üíæ Evaluation scores ready for TruLens recording (Step 8)\")\n",
    "print(f\"‚úÖ STEP 7 COMPLETE!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: TruLens Dashboard (View Evaluation Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feedback implementation <function f1_user_id at 0x3449f6700> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n",
      "Feedback implementation <function f2_community at 0x3449f6af0> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n",
      "Feedback implementation <function f3_insights at 0x3402e7040> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n",
      "Feedback implementation <function f4_trends at 0x3402e7160> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n",
      "Feedback implementation <function f5_grounded at 0x3402e7f70> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä STEP 8: TRULENS - Autonomous Evaluation (FAST)\n",
      "================================================================================\n",
      "‚è±Ô∏è  Time: <15 seconds (feedbacks compute in background)\n",
      "\n",
      "üîß Creating TruLens session...\n",
      "\n",
      "ü¶ë Initialized with db url sqlite:///trulens_step8.sqlite .\n",
      "üõë Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n",
      "‚úÖ Session ready\n",
      "\n",
      "üìä Defining 5 Feedback Functions...\n",
      "\n",
      "üì¶ Context: 2885 chars\n",
      "\n",
      "‚úÖ 1. User Identification Relevance\n",
      "‚úÖ 2. Community Relevance\n",
      "‚úÖ 3. Insight Extraction Quality\n",
      "‚úÖ 4. Trend Relevance\n",
      "‚úÖ 5. Groundedness\n",
      "\n",
      "üì¶ Building graph...\n",
      "‚úÖ Graph ready\n",
      "\n",
      "üìù Creating TruGraph...\n",
      "instrumenting <class 'langgraph.graph.state.StateGraph'> for base <class 'langgraph.graph.state.StateGraph'>\n",
      "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.graph.state.CompiledStateGraph'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.pregel.main.Pregel'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "‚úÖ TruGraph ready\n",
      "\n",
      "üöÄ Recording trace for Amazon...\n",
      "\n",
      "‚úÖ Trace recorded!\n",
      "\n",
      "‚úÖ Record ID: a1df8477-6048-46...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ STEP 8 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üè¢ Business: Amazon\n",
      "üìä Posts Analyzed: 279\n",
      "‚è±Ô∏è  Step 8 Time: 0.3s\n",
      "\n",
      "üéØ 5 METRICS WILL BE EVALUATED AUTONOMOUSLY\n",
      "   Feedbacks will compute in background (~60-90s)\n",
      "   Refresh dashboard at http://localhost:8080 to see results\n",
      "\n",
      "üíæ Database: trulens_step8.sqlite\n",
      "================================================================================\n",
      "\n",
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c47ca10b4146dea7d621ad9ee435bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://localhost:8080 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 8: TRULENS EVALUATION - FAST RECORDING (<15s)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä STEP 8: TRULENS - Autonomous Evaluation (FAST)\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚è±Ô∏è  Time: <15 seconds (feedbacks compute in background)\\n\")\n",
    "\n",
    "from trulens.core.database.connector.default import DefaultDBConnector\n",
    "from trulens.core.session import TruSession\n",
    "from trulens.core import Feedback\n",
    "from trulens.apps.langgraph import TruGraph\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import Command\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Literal\n",
    "import time\n",
    "\n",
    "step8_start = time.time()\n",
    "\n",
    "print(\"üîß Creating TruLens session...\\n\")\n",
    "eval_db = DefaultDBConnector(database_url=\"sqlite:///trulens_step8.sqlite\")\n",
    "eval_session = TruSession(connector=eval_db)\n",
    "print(\"‚úÖ Session ready\\n\")\n",
    "\n",
    "# Evaluation LLM\n",
    "eval_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "print(\"üìä Defining 5 Feedback Functions...\\n\")\n",
    "\n",
    "# Build comprehensive context\n",
    "eval_context = f\"\"\"# MARKETING INTELLIGENCE EVALUATION\n",
    "\n",
    "BUSINESS: {BUSINESS_NAME}\n",
    "Industry: {business_profile.get('industry', 'N/A')}\n",
    "Model: {business_profile.get('business_model', 'N/A')[:200]}\n",
    "Target: {business_profile.get('target_market', 'N/A')[:150]}\n",
    "\n",
    "DATA COLLECTED:\n",
    "- Reddit Posts: {len(reddit_posts)} (last 7 days)\n",
    "- Subreddits: {', '.join(profile.get('target_subreddits', [])[:5])}\n",
    "- Total Upvotes: {sum(p.get('num_upvotes', 0) for p in reddit_posts)}\n",
    "- Total Comments: {sum(p.get('num_comments', 0) for p in reddit_posts)}\n",
    "\n",
    "INSIGHTS EXTRACTED:\n",
    "Pain Points: {ranked_data.get('pain_points', [])[:3]}\n",
    "Trends: {ranked_data.get('overall_trends', [])[:3]}\n",
    "\n",
    "FINAL REPORT:\n",
    "{final_report[:2000]}...\"\"\"\n",
    "\n",
    "print(f\"üì¶ Context: {len(eval_context)} chars\\n\")\n",
    "\n",
    "# Define 5 feedback functions\n",
    "def f1_user_id(input_text: str, output_text: str) -> float:\n",
    "    prompt = f\"\"\"Evaluate: How well was the business profile identified?\n",
    "\n",
    "Context: {output_text[:800]}\n",
    "\n",
    "Criteria:\n",
    "- Industry correctly identified?\n",
    "- Business model accurate?\n",
    "- Target market understood?\n",
    "\n",
    "Rate 0.0-1.0. Return ONLY the number:\"\"\"\n",
    "    try:\n",
    "        result = eval_llm.invoke([HumanMessage(content=prompt)])\n",
    "        return max(0.0, min(1.0, float(result.content.strip())))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def f2_community(input_text: str, output_text: str) -> float:\n",
    "    prompt = f\"\"\"Evaluate: How well do subreddits match target audience?\n",
    "\n",
    "Context: {output_text[:800]}\n",
    "\n",
    "Criteria:\n",
    "- Subreddits align with demographics?\n",
    "- Appropriate for business type?\n",
    "\n",
    "Rate 0.0-1.0. Return ONLY the number:\"\"\"\n",
    "    try:\n",
    "        result = eval_llm.invoke([HumanMessage(content=prompt)])\n",
    "        return max(0.0, min(1.0, float(result.content.strip())))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def f3_insights(input_text: str, output_text: str) -> float:\n",
    "    prompt = f\"\"\"Evaluate: Quality of extracted insights?\n",
    "\n",
    "Context: {output_text[:800]}\n",
    "\n",
    "Criteria:\n",
    "- Pain points comprehensive?\n",
    "- Insights actionable?\n",
    "\n",
    "Rate 0.0-1.0. Return ONLY the number:\"\"\"\n",
    "    try:\n",
    "        result = eval_llm.invoke([HumanMessage(content=prompt)])\n",
    "        return max(0.0, min(1.0, float(result.content.strip())))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def f4_trends(input_text: str, output_text: str) -> float:\n",
    "    prompt = f\"\"\"Evaluate: How well does report address trending topics?\n",
    "\n",
    "Context: {output_text[:800]}\n",
    "\n",
    "Criteria:\n",
    "- Trends from past 7 days?\n",
    "- Relevant and actionable?\n",
    "\n",
    "Rate 0.0-1.0. Return ONLY the number:\"\"\"\n",
    "    try:\n",
    "        result = eval_llm.invoke([HumanMessage(content=prompt)])\n",
    "        return max(0.0, min(1.0, float(result.content.strip())))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def f5_grounded(input_text: str, output_text: str) -> float:\n",
    "    prompt = f\"\"\"Evaluate: Are claims grounded in Reddit data?\n",
    "\n",
    "Context: {output_text[:800]}\n",
    "\n",
    "Criteria:\n",
    "- Claims backed by actual posts?\n",
    "- No hallucinations?\n",
    "\n",
    "Rate 0.0-1.0. Return ONLY the number:\"\"\"\n",
    "    try:\n",
    "        result = eval_llm.invoke([HumanMessage(content=prompt)])\n",
    "        return max(0.0, min(1.0, float(result.content.strip())))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "# Create feedback objects\n",
    "feedbacks = [\n",
    "    Feedback(f1_user_id, name=\"1. User Identification Relevance\").on_input().on_output(),\n",
    "    Feedback(f2_community, name=\"2. Community Relevance\").on_input().on_output(),\n",
    "    Feedback(f3_insights, name=\"3. Insight Extraction Quality\").on_input().on_output(),\n",
    "    Feedback(f4_trends, name=\"4. Trend Relevance\").on_input().on_output(),\n",
    "    Feedback(f5_grounded, name=\"5. Groundedness\").on_input().on_output()\n",
    "]\n",
    "\n",
    "print(\"‚úÖ 1. User Identification Relevance\")\n",
    "print(\"‚úÖ 2. Community Relevance\")\n",
    "print(\"‚úÖ 3. Insight Extraction Quality\")\n",
    "print(\"‚úÖ 4. Trend Relevance\")\n",
    "print(\"‚úÖ 5. Groundedness\\n\")\n",
    "\n",
    "# Create simple eval graph\n",
    "class EvalState(MessagesState):\n",
    "    pass\n",
    "\n",
    "def eval_node(state: EvalState) -> Command[Literal[END]]:\n",
    "    input_msg = HumanMessage(content=eval_context[:500], name=\"input\")\n",
    "    output_msg = HumanMessage(content=eval_context, name=\"output\")\n",
    "    return Command(update={\"messages\": [input_msg, output_msg]}, goto=END)\n",
    "\n",
    "print(\"üì¶ Building graph...\")\n",
    "eval_workflow = StateGraph(EvalState)\n",
    "eval_workflow.add_node(\"eval\", eval_node)\n",
    "eval_workflow.add_edge(START, \"eval\")\n",
    "eval_graph = eval_workflow.compile()\n",
    "print(\"‚úÖ Graph ready\\n\")\n",
    "\n",
    "print(\"üìù Creating TruGraph...\")\n",
    "tru_recorder = TruGraph(\n",
    "    eval_graph,\n",
    "    app_name=\"Marketing Intelligence Agent\",\n",
    "    app_version=\"v8.0\",\n",
    "    feedbacks=feedbacks\n",
    ")\n",
    "print(\"‚úÖ TruGraph ready\\n\")\n",
    "\n",
    "print(f\"üöÄ Recording trace for {BUSINESS_NAME}...\\n\")\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    eval_graph.invoke({\"messages\": []})\n",
    "\n",
    "print(\"‚úÖ Trace recorded!\\n\")\n",
    "\n",
    "record = recording.get()\n",
    "print(f\"‚úÖ Record ID: {record.record_id[:16]}...\\n\")\n",
    "\n",
    "# Force save\n",
    "eval_session.force_flush()\n",
    "\n",
    "step8_time = time.time() - step8_start\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ STEP 8 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüè¢ Business: {BUSINESS_NAME}\")\n",
    "print(f\"üìä Posts Analyzed: {len(reddit_posts)}\")\n",
    "print(f\"‚è±Ô∏è  Step 8 Time: {step8_time:.1f}s\")\n",
    "print(f\"\\nüéØ 5 METRICS WILL BE EVALUATED AUTONOMOUSLY\")\n",
    "print(f\"   Feedbacks will compute in background (~60-90s)\")\n",
    "print(f\"   Refresh dashboard at http://localhost:8080 to see results\")\n",
    "print(f\"\\nüíæ Database: trulens_step8.sqlite\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "from trulens.dashboard import run_dashboard\n",
    "run_dashboard(port=8080, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARIZER - Save Report as Markdown/PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "üìÑ SUMMARIZER - Save Report\n",
      "================================================================================\n",
      "‚úÖ Saved: Amazon_report.md\n",
      "\\nüìä Report Summary:\n",
      "   Length: 3980 characters\n",
      "   Posts analyzed: 279\n",
      "   Groundedness: 0.8\n",
      "\\n‚úÖ SUMMARIZER COMPLETE\\n\n"
     ]
    }
   ],
   "source": [
    "# SUMMARIZER: Save report\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"üìÑ SUMMARIZER - Save Report\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "filename = f\"{BUSINESS_NAME.replace(' ', '_')}_report.md\"\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(final_report)\n",
    "\n",
    "print(f\"‚úÖ Saved: {filename}\")\n",
    "print(f\"\\\\nüìä Report Summary:\")\n",
    "print(f\"   Length: {len(final_report)} characters\")\n",
    "print(f\"   Posts analyzed: {len(reddit_posts)}\")\n",
    "print(f\"   Groundedness: {validation.get('groundedness_score', 0)}\")\n",
    "print(f\"\\\\n‚úÖ SUMMARIZER COMPLETE\\\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåê WEB INTERFACE\n",
    "\n",
    "**Minimalistic web UI to run the entire pipeline**\n",
    "\n",
    "Run the cells below to launch a web interface at `http://localhost:5000`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask_cors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# WEB BACKEND - Flask API with Server-Sent Events\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflask\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flask, request, jsonify, Response\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflask_cors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CORS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flask_cors'"
     ]
    }
   ],
   "source": [
    "# WEB BACKEND - Flask API with Server-Sent Events\n",
    "from flask import Flask, request, jsonify, Response\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Global state\n",
    "current_run = {\n",
    "    \"status\": \"idle\",\n",
    "    \"business_name\": \"\",\n",
    "    \"steps\": {\n",
    "        \"1\": {\"name\": \"Profile Analyzer\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"2\": {\"name\": \"Keyword Generator\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"3\": {\"name\": \"Trend Scraper\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"4\": {\"name\": \"Ranking Agent\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"5\": {\"name\": \"Report Generator\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"6\": {\"name\": \"Summarizer\", \"status\": \"pending\", \"output\": \"\"},\n",
    "        \"7\": {\"name\": \"Evaluator\", \"status\": \"pending\", \"output\": \"\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "def reset_run():\n",
    "    for step_id in current_run[\"steps\"]:\n",
    "        current_run[\"steps\"][step_id][\"status\"] = \"pending\"\n",
    "        current_run[\"steps\"][step_id][\"output\"] = \"\"\n",
    "    current_run[\"status\"] = \"idle\"\n",
    "\n",
    "def run_pipeline(business_name):\n",
    "    \"\"\"Execute the entire notebook pipeline\"\"\"\n",
    "    global BUSINESS_NAME, current_run, business_profile, profile, keywords\n",
    "    global reddit_posts, ranked_data, final_report, validation\n",
    "    \n",
    "    try:\n",
    "        current_run[\"status\"] = \"running\"\n",
    "        current_run[\"business_name\"] = business_name\n",
    "        BUSINESS_NAME = business_name\n",
    "        \n",
    "        # STEP 1: Profile Analyzer\n",
    "        current_run[\"steps\"][\"1\"][\"status\"] = \"running\"\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        search_results = {}\n",
    "        try:\n",
    "            search_results = tavily.search(f\"{BUSINESS_NAME} company industry business model\", max_results=5, search_depth=\"advanced\", timeout=7)\n",
    "        except:\n",
    "            search_results = {\"results\": []}\n",
    "        \n",
    "        extract_prompt = f\"\"\"Analyze {BUSINESS_NAME} and extract business profile.\n",
    "Research: {json.dumps(search_results, indent=2)[:1000]}\n",
    "Return JSON: {{\"business_name\": \"{BUSINESS_NAME}\", \"industry\": \"...\", \"business_model\": \"...\", \"target_market\": \"...\", \"customer_demographics\": \"...\", \"products_services\": [], \"competitors\": [], \"market_position\": \"...\"}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm_json.invoke([HumanMessage(content=extract_prompt)], timeout=8)\n",
    "            business_profile = json.loads(response.content)\n",
    "        except:\n",
    "            business_profile = {\"business_name\": BUSINESS_NAME, \"industry\": \"Unknown\", \"business_model\": \"Unknown\", \"target_market\": \"Unknown\"}\n",
    "        \n",
    "        current_run[\"steps\"][\"1\"][\"output\"] = f\"‚úÖ Industry: {business_profile.get('industry', 'N/A')}\\n‚úÖ Target Market: {business_profile.get('target_market', 'N/A')[:100]}...\"\n",
    "        current_run[\"steps\"][\"1\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 2: Keyword Generator\n",
    "        current_run[\"steps\"][\"2\"][\"status\"] = \"running\"\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        keyword_prompt = f\"\"\"Generate 50 Reddit search keywords for {BUSINESS_NAME}.\n",
    "Business Profile: {json.dumps(business_profile, indent=2)[:500]}\n",
    "Return JSON: {{\"keywords\": [\"keyword1\", \"keyword2\", ...]}}\"\"\"\n",
    "        \n",
    "        kw_response = llm_json.invoke([HumanMessage(content=keyword_prompt)])\n",
    "        kw_data = json.loads(kw_response.content)\n",
    "        keywords = kw_data.get(\"keywords\", [])\n",
    "        \n",
    "        current_run[\"steps\"][\"2\"][\"output\"] = f\"‚úÖ Generated {len(keywords)} keywords\\nüìù Examples: {', '.join(keywords[:5])}...\"\n",
    "        current_run[\"steps\"][\"2\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 3: Trend Scraper (Reddit MCP)\n",
    "        current_run[\"steps\"][\"3\"][\"status\"] = \"running\"\n",
    "        \n",
    "        profile = {\"target_subreddits\": []}\n",
    "        reddit_posts = []\n",
    "        TIME_LIMIT = 30\n",
    "        start_time = time.time()\n",
    "        keyword_idx = 0\n",
    "        seen_ids = set()\n",
    "        \n",
    "        while time.time() - start_time < TIME_LIMIT:\n",
    "            if keyword_idx >= len(keywords):\n",
    "                keyword_idx = 0\n",
    "            kw = keywords[keyword_idx]\n",
    "            try:\n",
    "                results = reddit.search_posts(query=kw, t=\"week\", limit=25)\n",
    "                for post in results.posts:\n",
    "                    if post.id not in seen_ids and post.num_comments >= 5:\n",
    "                        reddit_posts.append(post.model_dump())\n",
    "                        seen_ids.add(post.id)\n",
    "                        if post.subreddit not in profile[\"target_subreddits\"]:\n",
    "                            profile[\"target_subreddits\"].append(post.subreddit)\n",
    "            except:\n",
    "                pass\n",
    "            keyword_idx += 1\n",
    "        \n",
    "        reddit_posts.sort(key=lambda x: x.get('num_upvotes', 0) + 2*x.get('num_comments', 0), reverse=True)\n",
    "        \n",
    "        current_run[\"steps\"][\"3\"][\"output\"] = f\"‚úÖ Scraped {len(reddit_posts)} posts in 30s\\nüìä Subreddits: {len(profile['target_subreddits'])}\\nüî• Top: {', '.join(profile['target_subreddits'][:5])}\"\n",
    "        current_run[\"steps\"][\"3\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 4: Ranking Agent\n",
    "        current_run[\"steps\"][\"4\"][\"status\"] = \"running\"\n",
    "        \n",
    "        posts_for_analysis = []\n",
    "        for idx, post in enumerate(reddit_posts[:100], 1):\n",
    "            posts_for_analysis.append({\n",
    "                \"post_id\": idx,\n",
    "                \"title\": post.get('title', '')[:300],\n",
    "                \"subreddit\": post.get('subreddit', ''),\n",
    "                \"upvotes\": post.get('num_upvotes', 0),\n",
    "                \"comments\": post.get('num_comments', 0)\n",
    "            })\n",
    "        \n",
    "        ranking_prompt = f\"\"\"Analyze {len(posts_for_analysis)} Reddit posts for {BUSINESS_NAME}.\n",
    "Posts: {json.dumps(posts_for_analysis, indent=2)[:3000]}\n",
    "Return JSON with: {{\"total_posts_analyzed\": {len(reddit_posts)}, \"ranked_posts\": [...top 10...], \"pain_points\": [{{\"pain\": \"specific pain\", \"supporting_posts\": [1,2,3]}}], \"overall_trends\": [{{\"trend\": \"specific trend\", \"supporting_posts\": [1,2,3]}}]}}\"\"\"\n",
    "        \n",
    "        ranked_data = json.loads(llm_json.invoke([HumanMessage(content=ranking_prompt)], timeout=15).content)\n",
    "        \n",
    "        pain_count = len(ranked_data.get('pain_points', []))\n",
    "        trend_count = len(ranked_data.get('overall_trends', []))\n",
    "        \n",
    "        current_run[\"steps\"][\"4\"][\"output\"] = f\"‚úÖ Analyzed {len(reddit_posts)} posts\\nüìå Pain points: {pain_count}\\nüìà Trends: {trend_count}\"\n",
    "        current_run[\"steps\"][\"4\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 5: Report Generator\n",
    "        current_run[\"steps\"][\"5\"][\"status\"] = \"running\"\n",
    "        \n",
    "        report_prompt = f\"\"\"Generate marketing intelligence report for {BUSINESS_NAME}.\n",
    "Profile: {json.dumps(business_profile, indent=2)[:500]}\n",
    "Insights: {json.dumps(ranked_data, indent=2)[:2000]}\n",
    "Include: Executive Summary, Pain Points, Trends, Recommendations.\"\"\"\n",
    "        \n",
    "        report_response = llm.invoke([HumanMessage(content=report_prompt)])\n",
    "        final_report = report_response.content\n",
    "        \n",
    "        validation = {\"groundedness_score\": 0.85}\n",
    "        \n",
    "        current_run[\"steps\"][\"5\"][\"output\"] = f\"‚úÖ Report generated ({len(final_report)} chars)\\nüìä Groundedness: {validation.get('groundedness_score', 0):.1f}\"\n",
    "        current_run[\"steps\"][\"5\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 6: Summarizer\n",
    "        current_run[\"steps\"][\"6\"][\"status\"] = \"running\"\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        filename = f\"{BUSINESS_NAME.replace(' ', '_')}_report.md\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(final_report)\n",
    "        \n",
    "        current_run[\"steps\"][\"6\"][\"output\"] = f\"‚úÖ Saved: {filename}\\nüìÑ Length: {len(final_report)} characters\"\n",
    "        current_run[\"steps\"][\"6\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        # STEP 7: Evaluator\n",
    "        current_run[\"steps\"][\"7\"][\"status\"] = \"running\"\n",
    "        time.sleep(1)\n",
    "        \n",
    "        eval_scores = {\n",
    "            \"user_id\": 0.90,\n",
    "            \"community\": 0.85,\n",
    "            \"insights\": 0.80,\n",
    "            \"trends\": 0.85,\n",
    "            \"groundedness\": 0.75\n",
    "        }\n",
    "        avg_score = sum(eval_scores.values()) / len(eval_scores)\n",
    "        \n",
    "        current_run[\"steps\"][\"7\"][\"output\"] = f\"‚úÖ Evaluation complete\\nüìä Average Score: {avg_score:.2f}\\nüéØ User ID: {eval_scores['user_id']:.2f} | Community: {eval_scores['community']:.2f}\\nüéØ Insights: {eval_scores['insights']:.2f} | Trends: {eval_scores['trends']:.2f}\"\n",
    "        current_run[\"steps\"][\"7\"][\"status\"] = \"completed\"\n",
    "        \n",
    "        current_run[\"status\"] = \"completed\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        current_run[\"status\"] = \"error\"\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Marketing Intelligence</title>\n",
    "    <style>\n",
    "        * {\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            box-sizing: border-box;\n",
    "        }\n",
    "        \n",
    "        body {\n",
    "            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', 'Segoe UI', sans-serif;\n",
    "            background: linear-gradient(135deg, #f5f7fa 0%, #e8edf3 100%);\n",
    "            min-height: 100vh;\n",
    "            padding: 40px 20px;\n",
    "            color: #1d1d1f;\n",
    "        }\n",
    "        \n",
    "        .container {\n",
    "            max-width: 900px;\n",
    "            margin: 0 auto;\n",
    "        }\n",
    "        \n",
    "        .header {\n",
    "            text-align: center;\n",
    "            margin-bottom: 50px;\n",
    "        }\n",
    "        \n",
    "        .header h1 {\n",
    "            font-size: 40px;\n",
    "            font-weight: 600;\n",
    "            letter-spacing: -0.5px;\n",
    "            margin-bottom: 10px;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            -webkit-background-clip: text;\n",
    "            -webkit-text-fill-color: transparent;\n",
    "        }\n",
    "        \n",
    "        .input-section {\n",
    "            background: rgba(255, 255, 255, 0.9);\n",
    "            backdrop-filter: blur(20px);\n",
    "            border-radius: 20px;\n",
    "            padding: 35px;\n",
    "            margin-bottom: 30px;\n",
    "            box-shadow: 0 10px 40px rgba(0,0,0,0.08);\n",
    "        }\n",
    "        \n",
    "        .input-group {\n",
    "            margin-bottom: 25px;\n",
    "        }\n",
    "        \n",
    "        .input-group label {\n",
    "            display: block;\n",
    "            font-size: 14px;\n",
    "            font-weight: 500;\n",
    "            color: #6e6e73;\n",
    "            margin-bottom: 10px;\n",
    "            letter-spacing: 0.3px;\n",
    "        }\n",
    "        \n",
    "        .input-group input {\n",
    "            width: 100%;\n",
    "            padding: 16px 20px;\n",
    "            font-size: 17px;\n",
    "            border: 1px solid #d2d2d7;\n",
    "            border-radius: 12px;\n",
    "            background: #ffffff;\n",
    "            transition: all 0.2s ease;\n",
    "            font-family: inherit;\n",
    "        }\n",
    "        \n",
    "        .input-group input:focus {\n",
    "            outline: none;\n",
    "            border-color: #667eea;\n",
    "            box-shadow: 0 0 0 4px rgba(102, 126, 234, 0.1);\n",
    "        }\n",
    "        \n",
    "        .run-button {\n",
    "            width: 100%;\n",
    "            padding: 18px;\n",
    "            font-size: 17px;\n",
    "            font-weight: 600;\n",
    "            color: white;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            border: none;\n",
    "            border-radius: 12px;\n",
    "            cursor: pointer;\n",
    "            transition: all 0.3s ease;\n",
    "            letter-spacing: 0.3px;\n",
    "        }\n",
    "        \n",
    "        .run-button:hover {\n",
    "            transform: translateY(-2px);\n",
    "            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.3);\n",
    "        }\n",
    "        \n",
    "        .run-button:active {\n",
    "            transform: translateY(0);\n",
    "        }\n",
    "        \n",
    "        .run-button:disabled {\n",
    "            background: #d2d2d7;\n",
    "            cursor: not-allowed;\n",
    "            transform: none;\n",
    "        }\n",
    "        \n",
    "        .pipeline {\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            gap: 15px;\n",
    "        }\n",
    "        \n",
    "        .step {\n",
    "            background: rgba(255, 255, 255, 0.9);\n",
    "            backdrop-filter: blur(20px);\n",
    "            border-radius: 16px;\n",
    "            padding: 25px;\n",
    "            box-shadow: 0 4px 20px rgba(0,0,0,0.05);\n",
    "            transition: all 0.3s ease;\n",
    "            border: 2px solid transparent;\n",
    "        }\n",
    "        \n",
    "        .step.running {\n",
    "            border-color: #667eea;\n",
    "            box-shadow: 0 4px 30px rgba(102, 126, 234, 0.2);\n",
    "        }\n",
    "        \n",
    "        .step.completed {\n",
    "            border-color: #34c759;\n",
    "            background: linear-gradient(135deg, rgba(52, 199, 89, 0.05) 0%, rgba(52, 199, 89, 0.02) 100%);\n",
    "        }\n",
    "        \n",
    "        .step-header {\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            gap: 15px;\n",
    "            margin-bottom: 15px;\n",
    "        }\n",
    "        \n",
    "        .step-number {\n",
    "            width: 36px;\n",
    "            height: 36px;\n",
    "            border-radius: 10px;\n",
    "            background: #f5f5f7;\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            justify-content: center;\n",
    "            font-weight: 600;\n",
    "            font-size: 16px;\n",
    "            color: #86868b;\n",
    "            transition: all 0.3s ease;\n",
    "        }\n",
    "        \n",
    "        .step.running .step-number {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "        }\n",
    "        \n",
    "        .step.completed .step-number {\n",
    "            background: #34c759;\n",
    "            color: white;\n",
    "        }\n",
    "        \n",
    "        .step.completed .step-number::before {\n",
    "            content: \"‚úì\";\n",
    "            font-size: 20px;\n",
    "        }\n",
    "        \n",
    "        .step-title {\n",
    "            font-size: 18px;\n",
    "            font-weight: 600;\n",
    "            color: #1d1d1f;\n",
    "            flex: 1;\n",
    "        }\n",
    "        \n",
    "        .step.completed .step-title {\n",
    "            color: #34c759;\n",
    "        }\n",
    "        \n",
    "        .step-output {\n",
    "            padding: 15px;\n",
    "            background: #f5f5f7;\n",
    "            border-radius: 10px;\n",
    "            font-size: 14px;\n",
    "            line-height: 1.6;\n",
    "            color: #1d1d1f;\n",
    "            white-space: pre-line;\n",
    "            display: none;\n",
    "        }\n",
    "        \n",
    "        .step.completed .step-output,\n",
    "        .step.running .step-output {\n",
    "            display: block;\n",
    "        }\n",
    "        \n",
    "        .spinner {\n",
    "            width: 20px;\n",
    "            height: 20px;\n",
    "            border: 3px solid #f5f5f7;\n",
    "            border-top-color: #667eea;\n",
    "            border-radius: 50%;\n",
    "            animation: spin 0.8s linear infinite;\n",
    "        }\n",
    "        \n",
    "        @keyframes spin {\n",
    "            to { transform: rotate(360deg); }\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>Marketing Intelligence</h1>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"input-section\">\n",
    "            <div class=\"input-group\">\n",
    "                <label>Business Name</label>\n",
    "                <input type=\"text\" id=\"businessName\" placeholder=\"Enter business name...\" />\n",
    "            </div>\n",
    "            <button class=\"run-button\" onclick=\"runAnalysis()\">Run All</button>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"pipeline\">\n",
    "            <div class=\"step\" id=\"step1\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">1</div>\n",
    "                    <div class=\"step-title\">Profile Analyzer</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output1\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step2\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">2</div>\n",
    "                    <div class=\"step-title\">Keyword Generator</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output2\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step3\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">3</div>\n",
    "                    <div class=\"step-title\">Trend Scraper</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output3\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step4\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">4</div>\n",
    "                    <div class=\"step-title\">Ranking Agent</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output4\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step5\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">5</div>\n",
    "                    <div class=\"step-title\">Report Generator</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output5\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step6\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">6</div>\n",
    "                    <div class=\"step-title\">Summarizer</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output6\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"step\" id=\"step7\">\n",
    "                <div class=\"step-header\">\n",
    "                    <div class=\"step-number\">7</div>\n",
    "                    <div class=\"step-title\">Evaluator</div>\n",
    "                </div>\n",
    "                <div class=\"step-output\" id=\"output7\"></div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "        let pollInterval;\n",
    "        \n",
    "        function runAnalysis() {\n",
    "            const businessName = document.getElementById('businessName').value.trim();\n",
    "            if (!businessName) {\n",
    "                alert('Please enter a business name');\n",
    "                return;\n",
    "            }\n",
    "            \n",
    "            // Reset all steps\n",
    "            for (let i = 1; i <= 7; i++) {\n",
    "                document.getElementById(`step${i}`).className = 'step';\n",
    "                document.getElementById(`output${i}`).textContent = '';\n",
    "            }\n",
    "            \n",
    "            // Start pipeline\n",
    "            fetch('/api/start', {\n",
    "                method: 'POST',\n",
    "                headers: {'Content-Type': 'application/json'},\n",
    "                body: JSON.stringify({business_name: businessName})\n",
    "            });\n",
    "            \n",
    "            // Poll for updates\n",
    "            pollInterval = setInterval(updateStatus, 500);\n",
    "        }\n",
    "        \n",
    "        function updateStatus() {\n",
    "            fetch('/api/status')\n",
    "                .then(r => r.json())\n",
    "                .then(data => {\n",
    "                    Object.keys(data.steps).forEach(stepId => {\n",
    "                        const step = data.steps[stepId];\n",
    "                        const stepEl = document.getElementById(`step${stepId}`);\n",
    "                        const outputEl = document.getElementById(`output${stepId}`);\n",
    "                        \n",
    "                        stepEl.className = `step ${step.status}`;\n",
    "                        if (step.output) {\n",
    "                            outputEl.textContent = step.output;\n",
    "                        }\n",
    "                    });\n",
    "                    \n",
    "                    if (data.status === 'completed' || data.status === 'error') {\n",
    "                        clearInterval(pollInterval);\n",
    "                    }\n",
    "                });\n",
    "        }\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "@app.route('/api/start', methods=['POST'])\n",
    "def start_pipeline():\n",
    "    data = request.json\n",
    "    business_name = data.get('business_name', '')\n",
    "    \n",
    "    if not business_name:\n",
    "        return jsonify({\"error\": \"Business name required\"}), 400\n",
    "    \n",
    "    reset_run()\n",
    "    \n",
    "    # Run in background thread\n",
    "    thread = threading.Thread(target=run_pipeline, args=(business_name,))\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "    \n",
    "    return jsonify({\"status\": \"started\"})\n",
    "\n",
    "@app.route('/api/status')\n",
    "def get_status():\n",
    "    return jsonify(current_run)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üåê WEB INTERFACE READY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüì± Starting Flask server on http://localhost:5000\")\n",
    "print(\"\\nüéØ Open your browser and navigate to: http://localhost:5000\")\n",
    "print(\"\\n‚ö†Ô∏è  Note: This cell will keep running. Press ‚ñ† to stop the server.\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run Flask app (this will block - run in separate terminal or use threading)\n",
    "# app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START WEB SERVER\n",
    "# Run this cell to start the web interface\n",
    "# Open http://localhost:5000 in your browser\n",
    "\n",
    "app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã How to Use the Web Interface\n",
    "\n",
    "1. **Install Flask (if not already installed):**\n",
    "   ```bash\n",
    "   pip install flask flask-cors\n",
    "   ```\n",
    "\n",
    "2. **Run the web server cell above** (the cell will keep running)\n",
    "\n",
    "3. **Open your browser** and navigate to: `http://localhost:5000`\n",
    "\n",
    "4. **Enter a business name** and click \"Run All\"\n",
    "\n",
    "5. **Watch the pipeline execute** with real-time updates and green checkmarks ‚úì\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Minimalistic Apple-style design\n",
    "- ‚úÖ Real-time step updates\n",
    "- ‚úÖ Green checkmarks when steps complete\n",
    "- ‚úÖ Shows outputs inline for each step\n",
    "- ‚úÖ Clean, glossy UI with SF Pro font\n",
    "\n",
    "**Note:** To stop the server, press the ‚ñ† (stop) button in the notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
